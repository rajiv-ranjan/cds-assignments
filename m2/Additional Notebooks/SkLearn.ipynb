{"cells":[{"cell_type":"markdown","metadata":{"id":"Ps9llghv8jX1"},"source":["## Learning Notebook: SkLearn"]},{"cell_type":"markdown","metadata":{"id":"QeP1PAXf8jYD"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"AkwaW3k58jYG"},"source":["At the end of the experiment, you will be able to:\n","\n","* have an overview of the basics of Machine Learning\n","\n","* understand the SkLearn Machine Learning framework\n","\n","* understand the implementation of Train/Test Split\n","\n","* perform Linear Regression using SkLearn\n"]},{"cell_type":"markdown","metadata":{"id":"XC7_wvrxhv9o"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"HLVIgHBjeaXx"},"source":["**Machine learning** is a subfield of artificial intelligence (AI). The goal of machine learning is to understand the structure of data and model (fit) the data so that it can accurately predict the label or output for similar unseen data.\n","\n","**Machine Learning use cases:**\n","\n","Detecting tumors in brain scans, automatically classifying news articles, automatically flagging offensive comments on discussion forums,\n","summarizing long documents automatically,\n","creating a chatbot or a personal assistant,\n","detecting credit card fraud,\n","making your app react to voice commands,\n","building an intelligent bot for a game.\n","\n","**Machine Learning Workflow:**\n","\n","1. Frame the ML problem by looking at the business need\n","2. Gather the data and do Data Munging/Wrangling for each subproblem\n","3. Explore different models, perform V&V and shortlist promising candidates\n","4. Fine-tune shortlisted models and combine them together to form the final  solution\n","5. Present your solution  \n","6. Deploy\n","\n","\n","**Model training and testing**\n","\n","![wget](https://cdn.iisc.talentsprint.com/CDS/Images/model_train_test1.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uWDdGhE4miC9"},"source":["### Training, Validation, and Test Set"]},{"cell_type":"markdown","metadata":{"id":"QHbUne3bm5O5"},"source":["A machine learning algorithm splits the Dataset into two sets.\n","\n","Splitting your dataset is essential for an unbiased evaluation of prediction performance. In most cases, it’s enough to split your dataset randomly into two subsets:\n","\n","**Training Dataset:** The sample of data used to fit the model.\n","\n","**Test Dataset:** The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n","\n","We usually split the data as 80% for training stage and 20% for testing stage. 70% train and 30% test or 75% train and 25% test are also often used.\n","\n","**Validation Set:** This is a separate section of your dataset that you will use during training to get a sense of how well your model is doing on data that are not being used in training.\n","\n","In less complex cases, when you don’t have to tune hyperparameters, it’s okay to work with only the training and test sets.\n","\n","\n","<img src=\"https://miro.medium.com/max/700/1*aNPC1ifHN2WydKHyEZYENg.png\" alt=\"drawing\" width=\"500\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"DpQGMY8R3NFY"},"source":["#### Prerequisites for using train_test_split()"]},{"cell_type":"markdown","metadata":{"id":"KurBaHZL3PPI"},"source":["We will use scikit-learn, or sklearn library which has many packages for machine learning in Python.\n","\n","Refer the sklearn documentation [here](https://scikit-learn.org/stable/)"]},{"cell_type":"markdown","metadata":{"id":"e6y_C2m97dgv"},"source":["**Applying train_test_split()**"]},{"cell_type":"markdown","metadata":{"id":"2YKF0nWf7iEm"},"source":["You need to import:\n","\n","1.   train_test_split()\n","2.   NumPy\n","\n","We import NumPy because, in supervised machine learning applications, you’ll typically work with two such sequences:\n","\n","* A two-dimensional array with the inputs (x)\n","* A one-dimensional array with the outputs (y)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r9I5gDbRB1F8"},"source":["**sklearn.model_selection.train_test_split(arrays, options)**\n","\n","* **arrays** is the sequence of lists, NumPy arrays, pandas DataFrames, or similar array-like objects that hold the data you want to split. All these objects together make up the dataset and must be of the same length.\n","\n","* **options** are the optional keyword arguments that you can use to get desired behavior:\n","\n","  * **train_size** is the number that defines the size of the training set.\n","\n","  * **test_size** is the number that defines the size of the test set. You should provide either train_size or test_size.\n","      * If neither is given, then the default share of the dataset that will be used for testing is 0.25, or 25 percent.\n","      * If float (eg 0.25), it represents the proportion of the dataset to include in the test split and should be between 0.0 and 1.0.\n","      * If int (eg. 4), it represents the absolute number of test samples, eg. 4 samples of 12.\n","      * If None, the value is set to the complement of the train size.\n","      * If train_size is also None, it will be set to 0.25.\n","\n","  * **random_state** is the object that controls randomization during splitting. It can be either an int or an instance of RandomState. The default value is None.\n","\n","  * **shuffle** is the Boolean object (True by default) that determines whether to shuffle the dataset before applying the split.\n","\n","  * **stratify** is an array-like object that, if not None, determines how to use a stratified split.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YyV0gnaQV4GZ"},"source":["### Importing required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GorV3kJHV3gC"},"outputs":[],"source":["# Importing Standard Libraries\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Importing sklearn Libraries\n","from sklearn import datasets\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import train_test_split, learning_curve\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LinearRegression\n","from sklearn import linear_model\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"]},{"cell_type":"markdown","metadata":{"id":"K_28riB2i-z_"},"source":["### Let us use a small synthetically created dataset to understand how to implement a train and test split"]},{"cell_type":"markdown","metadata":{"id":"HDgLrsIHD8Q3"},"source":["#### Creating a simple dataset to work with"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VWTQUroD8ak"},"outputs":[],"source":["# inputs in the two-dimensional array X\n","X = np.arange(1, 25).reshape(12, 2)\n","\n","# outputs in the one-dimensional array y\n","y = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URRI4cnPD8rn"},"outputs":[],"source":["print(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYn2VwOBEkox"},"outputs":[],"source":["print(y)"]},{"cell_type":"markdown","metadata":{"id":"mcEUV1HkE-W0"},"source":["#### Splitting input and output datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juSLomf0E-lL"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=4, random_state=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-o4KrAxuFQOl"},"outputs":[],"source":["X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWGHP9sRFT4h"},"outputs":[],"source":["X_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZvPfClhFUDM"},"outputs":[],"source":["y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAdX8odIFQUf"},"outputs":[],"source":["y_test"]},{"cell_type":"markdown","metadata":{"id":"c01hKgX3UVe8"},"source":["### Develop an understanding of Least Squares"]},{"cell_type":"markdown","metadata":{"id":"KlO8nnlKYOk4"},"source":["**Least Squares** method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve.\n","\n","**Calculate Line Of Best Fit**\n","\n","A more accurate way of finding the line of best fit is the least square method.\n","\n","Use the following steps to find the equation of line of best fit for a set of ordered pairs $(x_1,y_1),(x_2,y_2),...(x_n,y_n)$.\n","\n","**Step 1:** Calculate the slope ‘m’ by using the following formula:\n","\n","$$m = \\frac{\\sum \\left ( x-\\bar{x} \\right )* \\left ( y-\\bar{y} \\right )}{\\sum \\left ( x-\\bar{x} \\right )^{2}}$$\n","\n","\n","**Step 2:** Compute the y -intercept of the line by using the formula:\n","\n","$$c = y - mx$$\n","\n","**Step 3:** Substitute the values in the final equation\n","\n","$$y = mx + c$$\n","\n","* y: dependent variable\n","* m: the slope of the line\n","* x: independent variable\n","* c: y-intercept\n"]},{"cell_type":"markdown","metadata":{"id":"8WfqM10H8Tqh"},"source":["As an example, we will try to find the least squares regression line for the below data set:\n","\n","\\begin{array} {|r|r|}\\hline Hours Spent & Grade \\\\\\hline 6 & 82 \\\\ \\hline 10 & 88 \\\\ \\hline 2 & 56 \\\\ \\hline 4 & 64 \\\\ \\hline 6 & 77 \\\\ \\hline 7 & 92 \\\\ \\hline 0 & 23 \\\\ \\hline 1 & 41 \\\\ \\hline 8 & 80 \\\\ \\hline 5 & 59 \\\\ \\hline 3 & 47 \\\\ \\hline  \\end{array}\n","\n","$x$ = HoursSpent\n","\n","$y$ = Grade\n","\n","$\\bar{x}$ = 4.72\n","\n","$\\bar{y}$ = 64.45\n","\n","\n","\\begin{array} {|r|r|}\\hline Hours Spent & Grade &  x - \\bar{x}  & y - \\bar{y} & (x - \\bar{x})*(y - \\bar{y}) \\\\ \\hline 6 & 82 & 1.27 & 17.55 & 22.33 \\\\ \\hline 10 & 88 & 5.27 & 23.55 & 124.15 \\\\ \\hline 2 & 56 & -2.73 & -8.45 & 23.06 \\\\ \\hline 4 & 64 & -0.73 & -0.45 & 0.33 \\\\ \\hline 6 & 77 & 1.27 & 12.55 & 15.97 \\\\ \\hline 7 & 92 & 2.27 & 27.55 & 62.60 \\\\ \\hline 0 & 23 & -4.73 & -41.45 & 195.97 \\\\ \\hline 1 & 41 & -3.73 & -23.42 & 87.42 \\\\ \\hline 8 & 80 & 3.27 & 15.55 & 50.88 \\\\ \\hline 5 & 59 & 0.27 & -5.45 & -1.49 \\\\ \\hline 3 & 47 & -1.73 & -17.45 & 30.15 \\\\ \\hline  \\end{array}\n","\n","\n","$$\\sum \\left ( x-\\bar{x} \\right )* \\left ( y-\\bar{y} \\right ) = 611.36$$\n","\n","$$\\sum \\left ( x-\\bar{x} \\right )^{2} = 94.18$$\n","\n","$$m = \\frac{611.36}{94.18}$$\n","\n","$$m = 6.49$$\n","\n","**Calculate the intercept:**\n","\n","$$c = y - mx$$\n","\n","$$c = 64.45-(6.49*4.72)$$\n","\n","$$c = 64.45 – 30.63$$\n","\n","$$c = 30.18$$\n","\n","Now that we have all the values to fit into the equation. If we want to know the predicted grade of someone who spends 2.35 hours on their essay, all we need to do is substitute that in for X.\n","\n","$$y =  (6.49 * X) + 30.18 $$\n","\n","$$y = (6.49 * 2.35) + 30.18$$\n","\n","$$y = 45.43$$\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_J5c2tcxCv3y"},"source":["### Example: Ordinary least squares Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"GwArexvF29YG"},"source":["Ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function of the independent variable.\n","\n","Geometrically, this is seen as the sum of the squared distances, parallel to the axis of the dependent variable, between each data point in the set and the corresponding point on the regression surface—the smaller the differences, the better the model fits the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUYjc5cSI0gv"},"outputs":[],"source":["# Generating Sample data\n","\n","rng = np.random.RandomState(1)              # instantiate random number generator\n","x = 10 * rng.rand(50)                       # generate 50 random numbers from uniform distribution\n","y = 2 * x - 5 + rng.randn(50)               # use 50 random numbers from normal distribution as noise\n","plt.scatter(x, y, c='b');"]},{"cell_type":"markdown","metadata":{"id":"fWW8fCnnNq3V"},"source":["**Using Scikit-Learn's Linear Regression estimator to fit the above data and construct the best-fit line**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XVil2eQRLkk"},"outputs":[],"source":["model = LinearRegression(fit_intercept=True)                   # instantiate LinearRegression\n","\n","model.fit(x[:, np.newaxis], y)                                 # fit the model on data using 'x' as column vector\n","\n","xfit = np.linspace(0, 10, 1000)                                # create 1000 points between 0 and 10\n","yfit = model.predict(xfit[:, np.newaxis])                      # predict the values for dependent variable\n","\n","plt.scatter(x, y, c='b')\n","plt.plot(xfit, yfit, 'k');"]},{"cell_type":"markdown","metadata":{"id":"b_3M43EWQSvU"},"source":["### Example: Machine Learning Workflow using Linear-Regression with Auto-MPG Dataset"]},{"cell_type":"markdown","metadata":{"id":"ZZPszz97QSvV"},"source":["#### Dataset"]},{"cell_type":"markdown","metadata":{"id":"SeZW_SX7QSvV"},"source":["In this example, we will be using the “Auto-MPG” dataset.\n","\n","The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\n","\n","Attribute Information:\n","\n","1. mpg: continuous\n","2. cylinders: multi-valued discrete\n","3. displacement: continuous\n","4. horsepower: continuous\n","5. weight: continuous\n","6. acceleration: continuous\n","7. model year: multi-valued discrete\n","8. origin: multi-valued discrete\n","9. car name: string (unique for each instance)\n","\n","Number of instances: 398\n","\n","**Problem statement:** Predict the fuel consumption in miles per gallon."]},{"cell_type":"markdown","metadata":{"id":"LuKp5UhPQSvW"},"source":["#### Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcsxx5wNQSvX"},"outputs":[],"source":["# Load and read the data\n","!wget https://cdn.iisc.talentsprint.com/CDS/Datasets/auto_mpg.csv\n","auto = pd.read_csv(\"auto_mpg.csv\")"]},{"cell_type":"markdown","metadata":{"id":"1i5RLbUbQSvZ"},"source":["Displaying Dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNVSv6BPQSvZ"},"outputs":[],"source":["auto.head()"]},{"cell_type":"markdown","metadata":{"id":"_o-DwjSjQSvb"},"source":["#### Exploring the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jq-F3CjPQSve"},"outputs":[],"source":["# print names of the features\n","print(auto.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmqH-b6fQSvh"},"outputs":[],"source":["# generating descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values.\n","auto.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOamk3laQSvh"},"outputs":[],"source":["# summary of the DataFrame\n","auto.info()"]},{"cell_type":"markdown","metadata":{"id":"OC18UgPdeStw"},"source":["#### Checking for Missing values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02MV1r9LeVw1"},"outputs":[],"source":["auto.isna().sum()"]},{"cell_type":"markdown","metadata":{"id":"M-A5Vuq1QSvi"},"source":["### Visualization of Auto-MPG Dataset\n","\n","#### Creating a pairplot and a heatmap to check which features seems to be more correlated\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHaYKdHqQSvi"},"outputs":[],"source":["# Pairplot\n","plt.style.use('ggplot')\n","sns.pairplot(auto)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qujS8LanYCBc"},"outputs":[],"source":["# Heatmap\n","\n","auto = auto.apply(pd.to_numeric, errors='coerce')\n","\n","plt.figure(figsize=(8, 8))\n","sns.heatmap(auto.corr(), annot=True, linewidth=0.5, center=0)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"hFElFkXBfQJP"},"source":["From the above plots, we can see that the features cylinders, displacement, and weight are highly correlated. We can use anyone of them for modeling."]},{"cell_type":"markdown","metadata":{"id":"FJb4eQ7GQSvi"},"source":["### Modeling and Prediction (Linear Regression)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNJdiDH8fmQf"},"outputs":[],"source":["auto.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FyPp6kllKvL"},"outputs":[],"source":["# Datatypes of all features\n","auto.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7pkqc8tkC3n"},"outputs":[],"source":["# Unique values in horsepower column\n","auto['horsepower'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHBY7FaGkJDd"},"outputs":[],"source":["# Removing '?' from horsepower column\n","auto = auto[auto['horsepower'] != '?']\n","auto['horsepower'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4MisrwlhrwA"},"outputs":[],"source":["# Converting horsepower column datatype from string to float\n","auto['horsepower'] = auto['horsepower'].astype(float)\n","auto.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13Nm0o8BgLLR"},"outputs":[],"source":["# Pridiction features\n","X = auto[['displacement', 'horsepower', 'acceleration', 'model year', 'origin']]\n","# Imputing missing values in X\n","imputer = SimpleImputer(strategy='mean')\n","X = imputer.fit_transform(X)\n","X = pd.DataFrame(X, columns=['displacement', 'horsepower', 'acceleration', 'model year', 'origin'])\n","# Target feature\n","y = auto['mpg']\n","X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29L3dCX7QSvj"},"outputs":[],"source":["# Splitting the Dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state= 101)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqF0AZqNQSvk"},"outputs":[],"source":["X_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4-H4CTIQSvk"},"outputs":[],"source":["# Instantiating LinearRegression() Model\n","lr = LinearRegression()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3lAZv-gQSvk"},"outputs":[],"source":["# Training/Fitting the Model\n","lr.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"S36ubhlcQSvl"},"source":["Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZBmXH7rQSvl"},"outputs":[],"source":["# Making Predictions\n","pred = lr.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5is2hESiQSvl"},"outputs":[],"source":["# Evaluating Model's Performance\n","print('Mean Absolute Error:', mean_absolute_error(y_test, pred))\n","print('Mean Squared Error:', mean_squared_error(y_test, pred))\n","print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))\n","print('Coefficient of Determination:', r2_score(y_test, pred))"]},{"cell_type":"markdown","metadata":{"id":"ipEod9TGQSvm"},"source":["Predicting the value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PgpTVfjpQSvm"},"outputs":[],"source":["pred = lr.predict(X_test)\n","print('Predicted fuel consumption(mpg):', pred[2])\n","print('Actual fuel consumption(mpg):', y_test.values[2])"]},{"cell_type":"markdown","metadata":{"id":"W5auVs4yrWK2"},"source":["### Let us now apply the above learnings to perform a linear regression based price prediction, using a 'Real estate' dataset (Practice section)"]},{"cell_type":"markdown","metadata":{"id":"F12UbnoDlbG1"},"source":["Linear regression model implementation\n","\n","  * Fit the model\n","  * Do the prediction\n","  * Plot the straight line for the predicted data using linear regression model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7-Tfzn8hoYcL"},"source":["#### Dataset"]},{"cell_type":"markdown","metadata":{"id":"yokzc2fPoh9P"},"source":["In this example, we will be using the “Real estate price prediction” dataset\n","\n","- Transaction date (purchase)\n","- House age\n","- Distance to the nearest MRT station (metric not defined)\n","- Amount of convenience stores\n","- Location (latitude and longitude)\n","- House price of unit area\n","\n","Problem statement: Predict the house price of unit area based on various features provided such as house age, location, etc."]},{"cell_type":"markdown","metadata":{"id":"dA9pKMxUproB"},"source":["#### Importing all the required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAzxkjLKrEFC"},"outputs":[],"source":["# Your Code Here"]},{"cell_type":"markdown","metadata":{"id":"KtnD9bjTrEQv"},"source":["#### Load and importing the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FawXXiHk29Yl"},"outputs":[],"source":["# Download the dataset\n","\n","!wget https://cdn.iisc.talentsprint.com/CDS/Datasets/Real_estate.csv\n","\n","# Convert it into a pandas dataframe:\n","\n","df = pd.read_csv('Real_estate.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d9XFuPyps4Xa"},"outputs":[],"source":["# View the data\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"UIYaWysFsFR6"},"source":["#### Dropping non-useful columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6m69YJtsFbt"},"outputs":[],"source":["#dropping columns\n","\n","# YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"XtsNthCMx1HY"},"source":["#### Finding if there are any null values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZRZ5EjZx1SH"},"outputs":[],"source":["# YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"CFpgaTDlsZEV"},"source":["#### Exploring the data using a scatter plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7o4RARCsZOa"},"outputs":[],"source":["# YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"8zHP3i7UwIW7"},"source":["#### Training our model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTtVP-7KwIgj"},"outputs":[],"source":["# Separating the data into independent and dependent variables\n","\n","# YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"870vu2t0xObO"},"source":["Splitting the data into training and testing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8dAkWC6ixO7s"},"outputs":[],"source":["# YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"cba93Mk3xPRe"},"source":["#### Training the Linear Regression model on the Training set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUHU3Z53xP2W"},"outputs":[],"source":["# Instantiate a LinearRegression() model"]},{"cell_type":"markdown","metadata":{"id":"q8Tc6Dffp1fV"},"source":["Training/Fitting the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-1hJnFfp1xC"},"outputs":[],"source":["# YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"7K4re2blxi17"},"source":["#### Exploring the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoIWz8E8xjDg"},"outputs":[],"source":["# Scatter plot of predicted values\n","\n","# YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"QKM4-sAGtzko"},"source":["## Q&A"]},{"cell_type":"markdown","metadata":{"id":"HVp0pNJoLkHY"},"source":["\n","1. What is the difference between the training set and the test set?\n","\n","    The training set is a subset of your data on which your model will learn how to predict the dependent variable with the independent variables.\n","\n","    The test set is the complimentary subset from the training set, on which you will evaluate your model to see if it manages to predict correctly the dependent variable with the independent variables.\n"]},{"cell_type":"markdown","metadata":{"id":"O3CmVNSiLibG"},"source":["\n","2. Why do we split on the dependent variable?\n","\n","    We want to have well-distributed values of the dependent variable in the training and test set. For example, if we only had the same value of the dependent variable in the training set, our model wouldn't be able to learn any correlation between the independent and dependent variables.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xpQ2WyEBLiot"},"source":["3. What is the purpose of a validation set?\n","\n","    The Validation Set is a separate section of your dataset that you will use during training to get a sense of how well your model is doing on data that are not being used in training.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"K481mDD4LjBl"},"source":["4. If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions?\n","\n","   If the model performs poorly to new instances, then it has overfitted on the training data. To solve this, we can do any of the following three: get more data, implement a simpler model, or eliminate outliers or noise from the existing data set."]},{"cell_type":"markdown","metadata":{"id":"Q8dLBPfy1Tcp"},"source":["## Reference Reading:\n","\n","1. https://livebook.manning.com/book/real-world-machine-learning/chapter-3/173\n","(Section 3.3 and 3.3.1)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}