{"cells":[{"cell_type":"markdown","metadata":{"id":"zZbO1TP3GWLl"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Additional Notebook (ungraded): CatBoost parameter tuning"]},{"cell_type":"markdown","metadata":{"id":"ssvaS0BoGWLp"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"zC3yBYSkGWLp"},"source":["At the end of the experiment, you will be able to\n","\n","- perform model training, cross-validation and prediction using CatBoost\n","- explore some useful features of CatBoost like early stopping,  snapshot support, feature importances, etc\n","- implement hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"3wDGpB4AGWLq"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"35VIuXX_MGq-"},"source":["CatBoost or Categorical Boosting is an algorithm for gradient boosting on decision trees. It is developed by Yandex researchers and engineers, and is used for search, recommendation systems, personal assistant, self-driving cars, weather prediction and many other tasks at Yandex and in other companies. It is a high-performance open source library for gradient boosting on decision trees and can be used by anyone.\n","\n","<center>\n","<image src=https://affine.ai/wp-content/uploads/2022/01/120.png width=350px>\n","</center>\n","\n","Some of the features of CatBoost are:\n","\n","- **Great quality without parameter tuning**: Reduce time spent on parameter tuning, as it provides great results with default parameters\n","- **Categorical features support**: Improve the training results that allows us to use non-numeric factors, instead of having to pre-process the data or spend time and effort turning it to numbers\n","- **Fast and scalable GPU version**: Train the model on a fast implementation of gradient-boosting algorithm for GPU\n","- **Improved accuracy**: Reduce overfitting when constructing the models with a novel gradient-boosting scheme\n","- **Fast prediction**: Apply the trained model quickly and efficiently even to latency-critical tasks using it's model applier\n","\n","The CatBoost library can be used to solve both classification and regression tasks. For classification, we can use **`CatBoostClassifier`** and for regression, **`CatBoostRegressor`**.\n","\n","To know more about CatBoost, refer [here](https://catboost.ai/en/docs/)."]},{"cell_type":"markdown","metadata":{"id":"Yon7buaRRFz9"},"source":["### Dataset Description"]},{"cell_type":"markdown","metadata":{"id":"dKAynoGwr-au"},"source":["[Titanic dataset](https://www.kaggle.com/c/titanic/data) contains passenger data, i.e., name, age, gender, socio-economic class, etc. The brief description of features is given below.\n","\n","<br>\n","\n","| Variable | Definition | Key |\n","|:--------|:--------|:--------|\n","|survival | Survival | 0 = No, 1 = Yes |\n","| pclass | Ticket class (a proxy for socio-economic status (SES)) | 1 = 1st (Upper), 2 = 2nd (Middle), 3 = 3rd (Lower)|\n","| sex| Sex | |\n","| Age | Age in years | Age is fractional if less than 1. If the age is estimated, it is in the form of xx.5 |\n","| sibsp | # of siblings / spouses aboard the Titanic | |\n","| parch | # of parents / children aboard the Titanic | Some children travelled only with a nanny, therefore parch=0 for them |\n","| ticket | Ticket number | |\n","| fare | Passenger fare | |\n","| cabin | Cabin number | |\n","| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |\n","\n","<br>\n","\n","Here, we will be predicting survival on the Titanic using passengers' data."]},{"cell_type":"markdown","metadata":{"id":"zV9JjuNrGWLs"},"source":["### Import required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cb4pvvIDvDh4"},"outputs":[],"source":["!pip -qq install catboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gL4hjwyJGWLt"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from catboost.datasets import titanic\n","from catboost import CatBoostClassifier, Pool, metrics, cv"]},{"cell_type":"markdown","metadata":{"id":"hO2yjVh572PI"},"source":["### Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_b4l5jGUr-av"},"outputs":[],"source":["# Create training and testing dataset\n","train_df, test_df = titanic()\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fjxxusd4I2W"},"outputs":[],"source":["# Shape\n","train_df.shape, test_df.shape"]},{"cell_type":"markdown","metadata":{"id":"QA6CErIOr-a0"},"source":["### Check for missing values in training set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FpL4vnUShSn"},"outputs":[],"source":["# Check for missing values\n","train_df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"r_rfZcjvr-a4"},"source":["From the above results, we can see that **`Age`**, **`Cabin`** and **`Embarked`** have some missing values.\n","\n","Now let's fill the missing values with some number way out of their distributions, so the model would be able to easily distinguish between them and take it into account."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0PuMwzmQr-a5"},"outputs":[],"source":["# Fill the missing values with -999\n","train_df.fillna(-999, inplace=True)\n","test_df.fillna(-999, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqJn8wfUT-3h"},"outputs":[],"source":["train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"KI1qUv-0r-a6"},"source":["### Separate features and label variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dxd2qBJzr-a7"},"outputs":[],"source":["# Features\n","X = train_df.drop('Survived', axis=1)\n","# Label\n","y = train_df['Survived']\n","\n","X.shape, y.shape"]},{"cell_type":"markdown","metadata":{"id":"1aE02zCTr-a7"},"source":["**Note that** our features are of different types - some of them are numeric, some are categorical, and some are even just strings, which normally should be handled in some specific way (for example encoded with bag-of-words representation). But in our case we could treat these string features just as categorical features and all the heavy lifting is done inside CatBoost."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uoi_v5f-r-a8"},"outputs":[],"source":["# Get categorical features indices by selecting the features which are of type other than float\n","print(X.dtypes)\n","categorical_features_indices = np.where(X.dtypes != float)[0]            # np.where will give indices where the condition is True\n","categorical_features_indices"]},{"cell_type":"markdown","metadata":{"id":"v3fuqE2tr-a9"},"source":["### Split the train data into training and validation sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFsR2HOkr-a-"},"outputs":[],"source":["# Create training and validation sets\n","X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.75, random_state=42)\n","X_test = test_df"]},{"cell_type":"markdown","metadata":{"id":"VCCLz_bzr-bB"},"source":["### Model Training\n","\n","To create the model we will use **`CatBoostClassifier`** class with default parameters, as they provide a good baseline almost all the time.\n","\n","Some of its parameters are:\n","\n","- ***custom_loss***: This would give us an ability to see what's going on in terms of accuracy or any other metric\n","- ***logging_level***: The logging level to show in output. Possible values:\n","    - Silent — Do not output any logging information\n","    - Verbose — Output the following data:\n","        - optimized metric\n","        - elapsed time of training\n","        - remaining time of training\n","    - Info — Output additional information and the number of trees\n","    - Debug — Output debugging information\n","\n","To know more about **`CatBoostClassifier`**, refer [here](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier).\n","\n","To know more about **`CatBoostClassifier's`** training parameters, refer [here](https://catboost.ai/en/docs/references/training-parameters/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o22BqsGUr-bB"},"outputs":[],"source":["# Create CatBoost model\n","model = CatBoostClassifier(custom_loss=[metrics.Accuracy()],\n","                           random_seed=42,\n","                           logging_level='Silent')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMKZFBZ_r-bC"},"outputs":[],"source":["# Model training\n","model.fit(X_train, y_train,\n","          cat_features=categorical_features_indices,\n","          eval_set=(X_validation, y_validation),\n","          verbose=False);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESlsnfSZ_p2-"},"outputs":[],"source":["# Model performance on validation set\n","model.score(X_validation, y_validation)"]},{"cell_type":"markdown","metadata":{"id":"BpXSc8mkr-bD"},"source":["From above, we can see that the accuracy value of **~0.82** (on validation set) was acheived."]},{"cell_type":"markdown","metadata":{"id":"UC4QyscVr-bE"},"source":["### Model Cross-Validation"]},{"cell_type":"markdown","metadata":{"id":"ZU0lYPJ23Rvb"},"source":["To perform cross-validation, we will be using CatBoost's **`cv`** package. It splits the dataset into N folds. N–1 folds are used for training, and one fold is used for model performance estimation. N models are updated on each iteration K. Each model is evaluated on its' own validation dataset on each iteration. This produces N metric values on each iteration K.\n","\n","The **`cv`** function calculates the average of these N values and the standard deviation. Thus, these two values are returned on each iteration.\n","\n","Some of **`cv`**'s parameters are:\n","\n","- ***pool***: The input dataset to cross-validate\n","- ***params***: The list of parameters to start training with\n","- ***iterations***: The maximum number of trees that can be built when solving machine learning problems, default=1000\n","- ***fold_count***: The number of folds to split the dataset into, default=3\n","\n","To know more about **`cv`** package, refer [here](https://catboost.ai/en/docs/concepts/python-reference_cv)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFtAdWUiAksC"},"outputs":[],"source":["# Get Model parameters\n","cv_params = model.get_params()\n","cv_params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6RJgJL0eA79Z"},"outputs":[],"source":["# Add Logloss as loss_function\n","cv_params.update({\n","                  'loss_function': metrics.Logloss()\n","                  })\n","cv_params"]},{"cell_type":"markdown","metadata":{"id":"1FaWtJHNvuk-"},"source":["**Pool constructor**: In order to train and optimize the model, we can utilize CatBoost library integrated tool called **`Pool`** for combining features and target variables into a dataset. This pooling allows us to pinpoint target variables, predictors, and the list of categorical features, while the pool constructor will combine those inputs and pass them to the model.\n","\n","To know more about **`Pool`** constructor, refer [here](https://catboost.ai/en/docs/concepts/python-reference_pool)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aw-R9-HIxZUZ"},"outputs":[],"source":["# Pool the dataset\n","pool_data = Pool(X, y, cat_features= categorical_features_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YzHrptsbB5_J"},"outputs":[],"source":["# Perform cross-validation\n","cv_data = cv(\n","             pool= pool_data,\n","             params= cv_params,\n","             fold_count= 3);\n","cv_data.head()"]},{"cell_type":"markdown","metadata":{"id":"oIonDV5Cr-bG"},"source":["Now we have values of our loss functions at each boosting step (iteration) averaged by 3 folds, which should provide us with a more accurate estimation of our model performance:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zb8yjJpmr-bG"},"outputs":[],"source":["# Maximum mean test accuracy\n","max_mean_test_acc = np.max(cv_data['test-Accuracy-mean'])\n","\n","# Index of Maximum mean test accuracy\n","idx = np.argmax(cv_data['test-Accuracy-mean'])\n","\n","# Standard deviation of maximum mean test accuracy\n","std_max_mean_test_acc = cv_data['test-Accuracy-std'][idx]\n","\n","# Best validation accuracy\n","print('Best validation accuracy score: {:.2f}±{:.2f} on step {}'.format(max_mean_test_acc, std_max_mean_test_acc, idx))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nEOoVs_mr-bI"},"outputs":[],"source":["# Precise validation accuracy\n","print('Precise validation accuracy score: {}'.format(max_mean_test_acc))"]},{"cell_type":"markdown","metadata":{"id":"mPUlZvg3r-bK"},"source":["### Applying model for prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vM4-GDjtr-bL"},"outputs":[],"source":["# Make predictions for X_test\n","predictions = model.predict(X_test)\n","predictions_probs = model.predict_proba(X_test)\n","print(f\"Predictions: {predictions[:10]}\")\n","print(f\"Prediction probabilities:\\n {predictions_probs[:10]}\")"]},{"cell_type":"markdown","metadata":{"id":"ihQWWKker-bL"},"source":["Let's look at some characteristics of CatBoost to get better predictions."]},{"cell_type":"markdown","metadata":{"id":"2yn0Rm0Vr-bL"},"source":["## Characteristics of CatBoost"]},{"cell_type":"markdown","metadata":{"id":"PlJi1kEGr-bN"},"source":["Some of the characteristics of CatBoost includes:\n","\n","- Using the best model\n","- Early Stopping\n","- Using Baseline\n","- Snapshot Support\n","- User Defined Objective Function\n","- User Defined Objective Function\n","- Staged Predict\n","- Feature Importances\n","- Eval Metrics\n","- Model Saving\n","\n","Let's define some parameters and create `Pool` for more convenience in exploring the above mentioned points."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9xi0VGCr-bO"},"outputs":[],"source":["# Create params dictionary\n","params = {\n","    'iterations': 500,\n","    'learning_rate': 0.1,\n","    'eval_metric': metrics.Accuracy(),\n","    'random_seed': 42,\n","    'logging_level': 'Silent',\n","    'use_best_model': False\n","}\n","# Create Pool for training set\n","train_pool = Pool(X_train, y_train, cat_features=categorical_features_indices)\n","\n","# Create Pool for validation set\n","validate_pool = Pool(X_validation, y_validation, cat_features=categorical_features_indices)"]},{"cell_type":"markdown","metadata":{"id":"7s_qsjnZr-bO"},"source":["### Using the best model\n","\n","If we essentially have a validation set, it's always better to use the `use_best_model` parameter during training. By default, this parameter is enabled. If it is enabled, the resulting trees ensemble is shrinking to the best iteration."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4mA08mrY4rsj"},"outputs":[],"source":["# Create a Simple Model having 'use_best_model' = False\n","model = CatBoostClassifier(**params)\n","model.fit(train_pool, eval_set=validate_pool);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJtgIW-I5FxN"},"outputs":[],"source":["# Create a best_model_params by updating 'use_best_model' to True\n","best_model_params = params.copy()\n","best_model_params.update({'use_best_model': True})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vye3s9Z5fbP"},"outputs":[],"source":["# Create a Best Model having 'use_best_model' = True\n","best_model = CatBoostClassifier(**best_model_params)\n","best_model.fit(train_pool, eval_set=validate_pool);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjkoxOTpr-bO"},"outputs":[],"source":["# Accuracy score for Simple model\n","pred = model.predict(X_validation)\n","acc = accuracy_score(y_validation, pred)\n","print('Simple model validation accuracy: {:.4}'.format(acc))\n","\n","# Accuracy score for Best model\n","best_pred = best_model.predict(X_validation)\n","best_acc = accuracy_score(y_validation, best_pred)\n","print('\\nBest model validation accuracy: {:.4}'.format(best_acc))"]},{"cell_type":"markdown","metadata":{"id":"UIZld216r-bP"},"source":["### Early Stopping\n","\n","If we essentially have a validation set, it's always easier and better to use early stopping. This feature is similar to the previous one, but only in addition to improving the quality it still saves time.\n","\n","Few parameters for early stopping are:\n","\n","- ***od_type***: The type of the overfitting detector to use. Possible values:\n","    - IncToDec (default)\n","    - Iter\n","- ***od_wait***: The number of iterations to continue the training after the iteration with the optimal metric value; default=20\n","\n","The purpose of ***od_wait*** parameter differs depending on the selected overfitting detector type, ***od_type***:\n","\n","- IncToDec — Ignore the overfitting detector when the threshold is reached and continue learning for the specified number of iterations after the iteration with the optimal metric value.\n","- Iter — Consider the model overfitted and stop training after the specified number of iterations since the iteration with the optimal metric value.\n"]},{"cell_type":"markdown","metadata":{"id":"3OyIAcwh-KIc"},"source":["Create a simple model using `params` and show the computation time for its training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWRcF4V3r-bP"},"outputs":[],"source":["%%time\n","model = CatBoostClassifier(**params)\n","model.fit(train_pool, eval_set=validate_pool)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OJ0Fx0E7zCq"},"outputs":[],"source":["# Create earlystop_params variable by updating 'od_type' and 'od_wait' in params\n","earlystop_params = params.copy()\n","earlystop_params.update({\n","                         'od_type': 'Iter',\n","                         'od_wait': 40\n","                         })"]},{"cell_type":"markdown","metadata":{"id":"X73Gabf7-d6B"},"source":["Create an early stopped model using `earlystop_params` and show the computation time for its training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8ib75zdr-bP"},"outputs":[],"source":["%%time\n","earlystop_model = CatBoostClassifier(**earlystop_params)\n","earlystop_model.fit(train_pool, eval_set=validate_pool);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2nLjYC6r-bQ"},"outputs":[],"source":["# Number of Tree count and Accuracy score for Simple model\n","print('Simple model tree count: {}'.format(model.tree_count_))\n","print('Simple model validation accuracy: {:.4}'.format(\n","    accuracy_score(y_validation, model.predict(X_validation))\n","))\n","\n","# Number of Tree count and Accuracy score for Early-stopped model\n","print('\\nEarly-stopped model tree count: {}'.format(earlystop_model.tree_count_))\n","print('Early-stopped model validation accuracy: {:.4}'.format(\n","    accuracy_score(y_validation, earlystop_model.predict(X_validation))\n","))"]},{"cell_type":"markdown","metadata":{"id":"t3NcrdOyr-bR"},"source":["From the above, we can see that we get similar or better quality in a shorter time.\n","\n","Though simple validation scheme does not precisely describes model out-of-train score (validation set may be biased because of dataset split) it is still nice to track model improvement dynamics - and thereby it is really good to stop boosting process earlier (before the overfitting kicks in)."]},{"cell_type":"markdown","metadata":{"id":"99ahBw-8r-bR"},"source":["### Using Baseline\n","\n","It is also posible to use pre-training results (baseline) for training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScCtDp1_Ov7H"},"outputs":[],"source":["# Create current_params variable by updating 'iterations' to 10 in params\n","current_params = params.copy()\n","current_params.update({\n","                       'iterations': 10\n","                       })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0VQH8f-ur-bS"},"outputs":[],"source":["# Create model with current_params and fit on training set\n","model = CatBoostClassifier(**current_params)\n","model.fit(X_train, y_train, categorical_features_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgZ1PtbzPeiy"},"outputs":[],"source":["# Get baseline (only with prediction_type='RawFormulaVal')\n","baseline = model.predict(X_train, prediction_type='RawFormulaVal')\n","X_train.shape, baseline.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPJotyDSPWXk"},"outputs":[],"source":["# Fit new model\n","model.fit(X_train, y_train, categorical_features_indices, baseline=baseline);"]},{"cell_type":"markdown","metadata":{"id":"fL2PjrASr-bT"},"source":["### Snapshot Support\n","\n","Catboost supports snapshots. We can use it for recovering training after an interruption or for starting training with previous results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GW-DbjmdQsRZ"},"outputs":[],"source":["# Create params_with_snapshot variable using params\n","params_with_snapshot = params.copy()\n","params_with_snapshot.update({\n","                             'iterations': 5,\n","                             'learning_rate': 0.5,\n","                             'logging_level': 'Verbose'\n","                             })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUjxsOV2Q_zM"},"outputs":[],"source":["# Create model with params_with_snapshot and fit on training set\n","model = CatBoostClassifier(**params_with_snapshot)\n","model.fit(train_pool, eval_set=validate_pool, save_snapshot=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ev9jluc2TCW5"},"outputs":[],"source":["# Update 'iterations' and 'learning_rate' in params_with_snapshot\n","params_with_snapshot.update({\n","                             'iterations': 10,\n","                             'learning_rate': 0.1,\n","                             })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAK9ivE5r-bT"},"outputs":[],"source":["# Create model with params_with_snapshot and fit on training set\n","model = CatBoostClassifier(**params_with_snapshot)\n","model.fit(train_pool, eval_set=validate_pool, save_snapshot=True)"]},{"cell_type":"markdown","metadata":{"id":"f17ulAucTnjC"},"source":["From above we can see that the training started with previous results."]},{"cell_type":"markdown","metadata":{"id":"JaKtPrhIr-bU"},"source":["### User Defined Objective Function\n","\n","It is possible to create our own objective function. Let's create logloss objective function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PsjqS4Uhr-bV"},"outputs":[],"source":["# for performance reasons it is better to install `numba` package for working with user defined functions\n","!pip install numba"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pf3fv8Ner-bW"},"outputs":[],"source":["class LoglossObjective(object):\n","    def calc_ders_range(self, approxes, targets, weights):\n","        # approxes, targets, weights are indexed containers of floats\n","        # (containers which have only __len__ and __getitem__ defined).\n","        # weights parameter can be None.\n","        #\n","        # To understand what these parameters mean, assume that there is\n","        # a subset of our dataset that is currently being processed.\n","        # approxes contains current predictions for this subset,\n","        # targets contains target values we provided with the dataset.\n","        #\n","        # This function should return a list of pairs (der1, der2), where\n","        # der1 is the first derivative of the loss function with respect\n","        # to the predicted value, and der2 is the second derivative.\n","        #\n","        # In our case, logloss is defined by the following formula:\n","        # target * log(sigmoid(approx)) + (1 - target) * (1 - sigmoid(approx))\n","        # where sigmoid(x) = 1 / (1 + e^(-x)).\n","\n","        assert len(approxes) == len(targets)\n","        if weights is not None:\n","            assert len(weights) == len(approxes)\n","\n","        result = []\n","        for index in range(len(targets)):\n","            e = np.exp(approxes[index])\n","            p = e / (1 + e)\n","            der1 = (1 - p) if targets[index] > 0.0 else -p\n","            der2 = -p * (1 - p)\n","\n","            if weights is not None:\n","                der1 *= weights[index]\n","                der2 *= weights[index]\n","\n","            result.append((der1, der2))\n","        return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSzee6shr-bX"},"outputs":[],"source":["# Create model\n","model = CatBoostClassifier(\n","                           iterations=10,\n","                           random_seed=42,\n","                           loss_function=LoglossObjective(),\n","                           eval_metric=metrics.Logloss()\n","                           )\n","# Fit model\n","model.fit(train_pool)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAuRMDEsUl3W"},"outputs":[],"source":["# Only prediction_type='RawFormulaVal' is allowed with custom `loss_function`\n","preds_raw = model.predict(X_test, prediction_type='RawFormulaVal')"]},{"cell_type":"markdown","metadata":{"id":"GvXvLik9r-bY"},"source":["### User Defined Metric Function\n","\n","Also it is possible to create our own metric function. Let's create logloss metric function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-3knCAsr-bY"},"outputs":[],"source":["class LoglossMetric(object):\n","    def get_final_error(self, error, weight):\n","        return error / (weight + 1e-38)\n","\n","    def is_max_optimal(self):\n","        return False\n","\n","    def evaluate(self, approxes, target, weight):\n","        # approxes is a list of indexed containers\n","        # (containers with only __len__ and __getitem__ defined),\n","        # one container per approx dimension.\n","        # Each container contains floats.\n","        # weight is a one dimensional indexed container.\n","        # target is float.\n","\n","        # weight parameter can be None.\n","        # Returns pair (error, weights sum)\n","\n","        assert len(approxes) == 1\n","        assert len(target) == len(approxes[0])\n","\n","        approx = approxes[0]\n","\n","        error_sum = 0.0\n","        weight_sum = 0.0\n","\n","        for i in range(len(approx)):\n","            w = 1.0 if weight is None else weight[i]\n","            weight_sum += w\n","            error_sum += -w * (target[i] * approx[i] - np.log(1 + np.exp(approx[i])))\n","\n","        return error_sum, weight_sum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdrCEtVAr-bZ"},"outputs":[],"source":["# Create model\n","model = CatBoostClassifier(\n","                           iterations=10,\n","                           random_seed=42,\n","                           loss_function=metrics.Logloss(),\n","                           eval_metric=LoglossMetric()\n","                           )\n","# Fit model\n","model.fit(train_pool)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w75eTV5oVO37"},"outputs":[],"source":["# Only prediction_type='RawFormulaVal' is allowed with custom `loss_function`\n","preds_raw = model.predict(X_test, prediction_type='RawFormulaVal')"]},{"cell_type":"markdown","metadata":{"id":"e0la5j-hr-bZ"},"source":["### Staged Predict\n","\n","CatBoost model has `staged_predict` method. It allows us to iteratively get predictions for a given range of trees.\n","\n","Some of `staged_predict` method's paramets are:\n","\n","- ***data***: Feature values data\n","- ***prediction_type***: The required prediction type. Supported prediction types are Probability, Class, Exponent, LogProbability, etc.\n","- ***ntree_start***\n","- ***ntree_end***\n","- ***eval_period***\n","\n","To reduce the number of trees to use when the model is applied or the metrics are calculated, set the range of the tree indices to [***ntree_start; ntree_end***) and the ***eval_period*** parameter to $k$ to calculate metrics on every $k$-th iteration.\n","\n","To know more about `staged_predict` method, refer [here](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier_staged_predict)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnT9x5e7Vm6k"},"outputs":[],"source":["# Create model\n","model = CatBoostClassifier(iterations=10, random_seed=42, logging_level='Silent')\n","model.fit(train_pool)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0pkaKChr-ba"},"outputs":[],"source":["# Specify start, end, and evaluation period for staged predict\n","ntree_start = 3\n","ntree_end = 9\n","eval_period = 2\n","# Stage predictions\n","predictions_iterator = model.staged_predict(\n","                                            data = validate_pool,\n","                                            prediction_type='Probability',\n","                                            ntree_start = ntree_start,\n","                                            ntree_end = ntree_end,\n","                                            eval_period = eval_period\n","                                            )\n","# Show stage predictions\n","for preds, tree_count in zip(predictions_iterator, range(ntree_start, ntree_end, eval_period)):\n","    print('First class probabilities using the first {} trees: {}'.format(tree_count, preds[:5, 1]))"]},{"cell_type":"markdown","metadata":{"id":"mYg_CGqTr-ba"},"source":["### Feature Importances\n","\n","Sometimes it is very important to understand which feature made the greatest contribution to the final result. To do this, the CatBoost model has a `get_feature_importance` method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TM4OmXHti8fz"},"outputs":[],"source":["# Create model\n","model = CatBoostClassifier(iterations=50, random_seed=42, logging_level='Silent')\n","model.fit(train_pool)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snozjGfar-ba"},"outputs":[],"source":["# Get feature importances\n","feature_importances = model.get_feature_importance(train_pool)\n","# Feature names\n","feature_names = X_train.columns\n","\n","# Show features in decreasing order based on their importances\n","for score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n","    print('{}: {}'.format(name, score))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0JfJe4_jdPy"},"outputs":[],"source":["# Show bar plot for feature importance\n","plt.bar(feature_names, feature_importances)\n","plt.xticks(rotation=80)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dtbzujdvr-bb"},"source":["From the above results, we can see that features **`Sex`** and **`Pclass`** had the biggest influence on the result."]},{"cell_type":"markdown","metadata":{"id":"p-50XFJPr-bb"},"source":["### Eval Metrics\n","\n","The CatBoost has a `eval_metrics` method that allows to calculate a given metrics on a given dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MSqM0wsJr-bb"},"outputs":[],"source":["# Create model and fit on training set\n","model = CatBoostClassifier(iterations=50, random_seed=42, logging_level='Silent')\n","model.fit(train_pool)\n","# Eval metrics\n","eval_metrics = model.eval_metrics(data = validate_pool,\n","                                  metrics = [metrics.AUC()],               # the list of metrics to be calculated\n","                                  )\n","eval_metrics.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hw7j51Z3r-bc"},"outputs":[],"source":["# Eval metric for first 6 iterations\n","print(eval_metrics['AUC'][:6])"]},{"cell_type":"markdown","metadata":{"id":"YAf64lN7r-bd"},"source":["### Model Saving\n","\n","It is always really handy to be able to save the model to disk (especially if training took some time). CatBoost have a method named `save_model` just for this purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CzeXwC03DJX2"},"outputs":[],"source":["# Create a model and train on training set\n","model = CatBoostClassifier(iterations=10, random_seed=42, logging_level='Silent')\n","model.fit(train_pool)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1ZO90PiDQT_"},"outputs":[],"source":["# Save model\n","model.save_model('catboost_model.dump')"]},{"cell_type":"markdown","metadata":{"id":"k7YlMGnoU_LK"},"source":["Once the model is saved we can load it again to use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sxw7hupYr-be"},"outputs":[],"source":["# Use the saved model\n","model = CatBoostClassifier()\n","model.load_model('catboost_model.dump');"]},{"cell_type":"markdown","metadata":{"id":"p2dPv6yBr-be"},"source":["## Parameters Tuning\n","\n","While we could always select optimal number of iterations (boosting steps) by cross-validation and learning curve plots, it is also important to play with some of model parameters, and we would like to pay some special attention to `l2_leaf_reg` and `learning_rate`.\n","\n","Here, we'll select these parameters using the **`hyperopt`** package.\n","\n","**`hyperopt`**'s job is to find the best value of a scalar-valued, possibly-stochastic function over a set of possible arguments to that function. Whereas many optimization packages will assume that these inputs are drawn from a vector space, hyperopt is different in that as it encourages us to describe our search space in more detail. By providing more information about where our function is defined, and where we think the best values are, we allow algorithms in hyperopt to search more efficiently.\n","\n","To know more about **`hyperopt`**, refer [here](https://hyperopt.github.io/hyperopt/)."]},{"cell_type":"markdown","metadata":{"id":"z4-Qo_n9lIj9"},"source":["To perform hyperparameter optimization, `hyperopt.fmin()` function is used. It iterates on different sets of algorithms and their hyperparameters and then minimizes the objective function.\n","\n","Some of `hyperopt.fmin()`'s parameters are:\n","\n","- ***fn***: the objective function to minimize\n","- ***space***: the space over which to search\n","- ***trials***: the database in which to store all the point evaluations of the search\n","- ***algo***: the search algorithm to use such as Random search, TPE (Tree Parzen Estimators), and Adaptive TPE.\n","    - `hyperopt.rand.suggest` and `hyperopt.tpe.suggest` provides logic for a sequential search of the hyperparameter space\n","\n","To know more about **`hyperopt.fmin()`**, refer [here](https://github.com/hyperopt/hyperopt/wiki/FMin)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgXJS5OHr-be"},"outputs":[],"source":["!pip -qq install hyperopt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DS3JIOyrr-bf"},"outputs":[],"source":["# Create an objective function which hyperopt needs to minimize\n","import hyperopt\n","\n","def hyperopt_objective(params):\n","    # Create model\n","    model = CatBoostClassifier(\n","                               l2_leaf_reg = int(params['l2_leaf_reg']),\n","                               learning_rate = params['learning_rate'],\n","                               iterations=500,\n","                               eval_metric=metrics.Accuracy(),\n","                               random_seed=42,\n","                               verbose=False,\n","                               loss_function=metrics.Logloss()\n","                               )\n","    # Perform cross-validation\n","    cv_data = cv(\n","                 Pool(X, y, cat_features=categorical_features_indices),\n","                 model.get_params(),\n","                 logging_level='Silent'\n","                 )\n","    # Best test accuracy\n","    best_accuracy = np.max(cv_data['test-Accuracy-mean'])\n","\n","    return 1 - best_accuracy          # as hyperopt minimizes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mb1nha4zmaTe"},"outputs":[],"source":["# Create space over which hyperopt needs to search\n","'''\n","   hp.qloguniform(label, low, high, q): returns a value like round(exp(uniform(low, high)) / q) * q\n","   hp.uniform(label, low, high): returns a value uniformly between low and high\n","'''\n","params_space = {\n","                'l2_leaf_reg': hyperopt.hp.qloguniform('l2_leaf_reg', 0, 2, 1),\n","                'learning_rate': hyperopt.hp.uniform('learning_rate', 1e-3, 5e-1),\n","                }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1vntDAymnjS"},"outputs":[],"source":["# Create trials database to store all the point evaluations of the search\n","trials = hyperopt.Trials()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4w8MVEoYr-bf"},"outputs":[],"source":["# Perform hyperparameter optimization\n","best = hyperopt.fmin(\n","                     fn= hyperopt_objective,\n","                     space= params_space,\n","                     algo= hyperopt.tpe.suggest,\n","                     max_evals= 50,\n","                     trials= trials,\n","                     )\n","\n","print(f\"Best params values: {best}\")"]},{"cell_type":"markdown","metadata":{"id":"OtuuDZZYr-bf"},"source":["Now let's get all cross-validation data with best parameters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFnwGfWrr-bg"},"outputs":[],"source":["# Create model with best params\n","model = CatBoostClassifier(\n","                           l2_leaf_reg=int(best['l2_leaf_reg']),\n","                           learning_rate=best['learning_rate'],\n","                           iterations=500,\n","                           eval_metric=metrics.Accuracy(),\n","                           random_seed=42,\n","                           verbose=False,\n","                           loss_function=metrics.Logloss(),\n","                           )\n","# Cross-validation\n","cv_data = cv(Pool(X, y, cat_features=categorical_features_indices), model.get_params())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yv1Z52DEr-bg"},"outputs":[],"source":["# Accuracy score\n","print('Precise validation accuracy score: {}'.format(np.max(cv_data['test-Accuracy-mean'])))"]},{"cell_type":"markdown","metadata":{"id":"PF3lI4Rqr-bh"},"source":["Recall that with default parameters the cv score was ~0.82, and thereby some improvement can be seen above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgx7ShSNfOOd"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}