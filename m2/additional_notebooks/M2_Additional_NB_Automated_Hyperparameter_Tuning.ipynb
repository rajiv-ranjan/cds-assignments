{"cells":[{"cell_type":"markdown","metadata":{"id":"I1xuuUQdpC0r"},"source":["# Advanced Certification Program in Computational Data Science\n","## A Program by IISc and TalentSprint\n","### Additional Notebook (Ungraded): Automated Hyperparameter Tuning"]},{"metadata":{"id":"8Aa79N9PHXzE"},"cell_type":"markdown","source":["## Introduction <a id =\"1\"></a>\n","While studying the basic machine learning algorithms, you must have noticed that in case of Support Vector Machines we have to choose gamma and Regularization parameter(C), in case of K-Means Clustering we have to provide the value of 'K'. Such parameters which needs to be specified before fitting the model are called as Hyperparameters. Since parameters cannot be estimated from data, getting their value correct has a big impact on the model performance. The only way to find the best possible value of hyperparameters is to try them and choose the best performing one and this process is knows as hyperparameter tuning.\n","\n","## Parameter vs Hyperparameter <a id =\"2\"></a>\n","\n","### Parameter <a id =\"3\"></a>\n","\n","* Parameter or model parameter is a configuration variable which is internal to the model and whose value can be estimated from the data\n","* They are required by the model when making predictions\n","* They are estimated or learned from data\n","* They are often not set manually by the user\n","* They are often saved as part of the learned model\n","* Some examples of model parameters include:\n","  * The weights in an artificial neural network\n","  * The support vectors in a support vector machine\n","  * The coefficients in a linear regression or logistic regression\n","\n","### Hyperparameter <a id =\"4\"></a>\n","\n","* Hyperparameters are external to the model and whose values cannot be estimated based on the data\n","* They are often specified by the user\n","* They are often tuned for a given predictive modeling problem\n","* They can often be set using heuristics\n","* Some examples of model hyperparameters include:\n","  * The learning rate for training a neural network\n","  * The gamma and Regularization parameter(C) hyperparameters for support vector machines\n","  * The K in K-nearest neighbors\n","  * No of trees (n_estimators) in Random Forest Algorithm\n","  \n","\n","## Hypertuning Steps <a id =\"5\"></a>\n","\n","* List the different hyperparameters based on the problem in hand. If there are more than one hyperparameter then make grid with different combinations of parameters\n","* Fit all of them separately to the model. If you have a large number of hyperparameters then training time and computational cost will be very high. To reduce it you may try few random combination of hyperparameters, instead of going for every possible permutation.\n","* Note down the model performance\n","* Choose the best performing one\n","\n","Always use cross validation technique for hyperparameter tuning to avoid the model overfitting on test data.\n","\n","## Tuning Strategy <a id =\"6\"></a>\n","\n","Models can have many hyperparameters and to try every permutation of it can be treated as a search problem. Below are the two most common ways to perform the hyperparameter tuning.\n","\n","### Grid Search <a id =\"7\"></a>\n","\n","* Grid search exhaustively considers all parameter combinations for an estimator.\n","* GridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n","* The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n","* Since grid search goes through every possible combination of hyperparameters, it is computationally expensive and takes longer time to complete.\n","\n","### Random Search <a id =\"8\"></a>\n","\n","* Unlike grid search, random search perform randomized search on hyper parameters and selects few parameter combinations for an estimator.\n","* RandomizedSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n","* The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.\n","* Since random search goes through selective combination of hyperparameters, it is computationally less expensive and takes less time to complete."]},{"metadata":{"id":"3Mi5Z5oBHXzG"},"cell_type":"markdown","source":["## Grid Search Example <a id =\"9\"></a>\n","\n","We are going to use [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/overview) competition data from Kaggle. We will to convert this dataset into a toy dataset so that we can straightaway jump into hyperparameter tuning\n","\n","\n","\n","### Import Libraries <a id =\"10\"></a>\n","\n","* pandas: Used for data manipulation and analysis\n","* train_test_split: Sklearn library to split arrays or matrices into random train and test subsets\n","* GridSearchCV: Sklearn library to perform exhaustive search over specified parameter values for an estimator\n","* RandomizedSearchCV: Sklearn library to perform randomized search on hyper parameters\n","* svm: Sklearn Support Vector Machines library"]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"LH1VMzamHXzH"},"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n","from sklearn import svm"],"execution_count":null,"outputs":[]},{"metadata":{"id":"rbFyEJolHXzI"},"cell_type":"markdown","source":["### Load Dataset <a id =\"11\"></a>\n","We will load the dataset into pandas dataframe and convert it into a toy dataset by removing categorical columns and rows and columns with null values."]},{"metadata":{"trusted":true,"id":"BtXp2Fx8HXzJ"},"cell_type":"code","source":["train_data = pd.read_csv('https://cdn.iisc.talentsprint.com/CDS/Datasets/titanic_train_data.csv')\n","\n","# Remove rows with missing target values\n","train_data.dropna(axis=0, subset=['Survived'], inplace=True)\n","y = train_data.Survived # Target variable\n","train_data.drop(['Survived'], axis=1, inplace=True) # Removing target variable from training data\n","\n","train_data.drop(['Age'], axis=1, inplace=True) # Remove columns with null values\n","\n","# Select numeric columns only\n","numeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n","X = train_data[numeric_cols].copy()\n","\n","print(\"Shape of input data: {} and shape of target variable: {}\".format(X.shape, y.shape))\n","\n","# X.head()\n","pd.concat([X,y], axis=1).head()# Show first 5 training examples"],"execution_count":null,"outputs":[]},{"metadata":{"id":"kJTRn9mAHXzK"},"cell_type":"markdown","source":["### Understanding the Data <a id =\"12\"></a>\n","The final dataset contains 5 features and 891 training examples. We have to predict which passengers survived the Titanic shipwreck based on available training data. Features that we are going to use in this example are passenger id, ticket class, sibling/spouse aboard, parent/children aboard and ticket fare."]},{"metadata":{"id":"e00_7ZEVHXzK"},"cell_type":"markdown","source":["### Model Score Without Hyperparameter Tuning <a id =\"13\"></a>\n","\n","We will split the dataset using **train_test_split()** method and use training set for model training and test set for model testing. Later we will use hyperparameter tuning to improve the model performance."]},{"metadata":{"trusted":true,"id":"M5IOxF6GHXzL"},"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42)\n","print('X_train dimension= ', X_train.shape)\n","print('X_test dimension= ', X_test.shape)\n","print('y_train dimension= ', y_train.shape)\n","print('y_train dimension= ', y_test.shape)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"x9LiNQSgHXzL"},"cell_type":"markdown","source":["Now lets train using SVM classifier. Note that we are using default parameters."]},{"metadata":{"trusted":true,"id":"W2D-msHVHXzL"},"cell_type":"code","source":["clf= svm.SVC()\n","clf.fit(X_train, y_train)\n","print('Model score using default parameters is = ', clf.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"eOjPIrz-HXzM"},"cell_type":"markdown","source":["### Model Score Using Hyperparameter Tuning <a id =\"14\"></a>\n","So without hyperparameter tuning we get only 60% accuracy, lets see the model performance using hyperparameter tuning"]},{"metadata":{"trusted":true,"id":"cf07CzVVHXzM"},"cell_type":"code","source":["# Let create parameter grid for GridSearchCV\n","parameters = {  'C':[0.01, 1, 5],\n","                'kernel':('linear', 'rbf'),\n","                'gamma' :('scale', 'auto')\n","             }\n","gsc = GridSearchCV(estimator = svm.SVC(), param_grid= parameters,cv= 5,verbose =1)\n","\n","# Fitting the model for grid search. It will first find the best parameter combination using cross validation.\n","# Once it has the best combination, it runs fit again on all data passed to fit (without cross-validation),\n","# to built a single new model using the best parameter setting.\n","gsc.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"HE5nV5iHHXzM"},"cell_type":"markdown","source":["That would have taken a long time to complete!!\n","Now lets review the results from GridSearchCV."]},{"metadata":{"trusted":true,"id":"ZbNComoDHXzM"},"cell_type":"code","source":["print(f'Best hyperparameters: {gsc.best_params_}')\n","print(f'Best score: {gsc.best_score_}')\n","print('Detailed GridSearchCV result is as below')\n","gsc_result = pd.DataFrame(gsc.cv_results_).sort_values('mean_test_score',ascending= False)\n","gsc_result[['param_C','param_kernel','param_gamma','mean_test_score']]"],"execution_count":null,"outputs":[]},{"metadata":{"id":"-P0QPwJCHXzM"},"cell_type":"markdown","source":["## Random Search Example <a id =\"15\"></a>\n","We will use same dataset and same parameter grid for hyperparameter tuning."]},{"metadata":{"trusted":true,"id":"pxlsWsGQHXzN"},"cell_type":"code","source":["# n_iter=5 > Number of parameter settings that are sampled.\n","# So instaed of 12 it will randomly search for only 5 combinations for each fold\n","rsc = RandomizedSearchCV(estimator = svm.SVC(), param_distributions= parameters,cv=5,n_iter = 5,verbose =1)\n","rsc.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"g3dVHSdEHXzN"},"cell_type":"code","source":["print(f'Best hyperparameters: {rsc.best_params_}')\n","print(f'Best score: {rsc.best_score_}')\n","print('Detailed RandomizedSearchCV result is as below')\n","rsc_result = pd.DataFrame(rsc.cv_results_).sort_values('mean_test_score',ascending= False)\n","rsc_result[['param_C','param_kernel','param_gamma','mean_test_score']]"],"execution_count":null,"outputs":[]},{"metadata":{"id":"OLa71AoTHXzN"},"cell_type":"markdown","source":["## Conclusion <a id =\"16\"></a>\n","Hyperparameter tuning helps to find the best parameters which can improve the model performance. Since grid search takes more time and is computationally more heavy, it is not suitable for big datasets. Random search is the obvious choice for big datasets, but it does not guarantee to find the best parameters as it uses only selected samples of the parameters."]},{"metadata":{"id":"UyMDuuJwHXzN"},"cell_type":"markdown","source":["## References:\n","* [What is the Difference Between a Parameter and a Hyperparameter?](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module2/M2_Additional_Notebook_Difference%20Between%20a%20Parameter%20and%20a%20Hyperparameter.pdf)\n","* [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n","* [sklearn.model_selection.RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n","* [Hyperparameter Tuning, Kaggle: Satish Gunjal](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module2/M2_Additional-hyperparameter-tuning.pdf)"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}