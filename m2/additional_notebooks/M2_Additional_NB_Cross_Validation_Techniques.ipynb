{"cells":[{"cell_type":"markdown","source":["# Advanced Certification Program in Computational Data Science\n","## A Program by IISc and TalentSprint\n","### Additional Notebook (ungraded) on Cross Validation Techniques\n"],"metadata":{"id":"u3weJ6s61hPf"},"id":"u3weJ6s61hPf"},{"cell_type":"markdown","source":["## Learning Objectives"],"metadata":{"id":"KcGQ2CQc1sFK"},"id":"KcGQ2CQc1sFK"},{"cell_type":"markdown","source":["At the end of the experiment, you will be able to\n","\n","* Train the Decision Tree model on the diabetes dataset.\n","* Explore HoldOut Validation Approach- Train And Test Split\n","* Explore K Fold Cross Validation\n","* Explore Stratified K-fold Cross Validation\n","* Explore Leave One Out Cross Validation(LOOCV)\n","* Explore Repeated Random Test-Train Splits"],"metadata":{"id":"oET7DFCr1vLU"},"id":"oET7DFCr1vLU"},{"cell_type":"markdown","source":["## Introduction"],"metadata":{"id":"YBpSMy_SMQSz"},"id":"YBpSMy_SMQSz"},{"cell_type":"markdown","source":["Cross-validation is a statistical method used to estimate the skill of machine learning models.\n","\n","It is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.\n","\n"],"metadata":{"id":"m9CNUwP9Mzlt"},"id":"m9CNUwP9Mzlt"},{"cell_type":"markdown","source":["## Dataset Description"],"metadata":{"id":"mmWWmQ3_LP45"},"id":"mmWWmQ3_LP45"},{"cell_type":"markdown","source":["Attribute Information for Breast Cancer Wisconsin (Diagnostic) Data Set:\n","\n","1. ID number\n","2. Diagnosis (M = malignant, B = benign)\n","\n","\n","3-32.\n","\n","Ten real-valued features are computed for each cell nucleus:\n","\n","    a. radius (mean of distances from center to points on the perimeter)\n","    b. texture (standard deviation of gray-scale values)\n","    c. perimeter\n","    d. area\n","    e. smoothness (local variation in radius lengths)\n","    f. compactness (perimeter^2 / area - 1.0)\n","    g. concavity (severity of concave portions of the contour)\n","    h. concave points (number of concave portions of the contour)\n","    i. symmetry\n","    j. fractal dimension (\"coastline approximation\" - 1)\n","\n","\n","The mean, standard error and \"worst\" or largest (mean of the three\n","largest values) of these features were computed for each image,\n","resulting in 30 features. For instance, field 3 is Mean Radius, field\n","13 is Radius SE, field 23 is Worst Radius.\n","\n","* All feature values are recoded with four significant digits.\n","\n","* Missing attribute values: none\n","\n","* Class distribution: 357 benign, 212 malignant\n","\n","For more information click [here](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))"],"metadata":{"id":"-20QyjJfLqnf"},"id":"-20QyjJfLqnf"},{"cell_type":"code","source":["# @title Download the Dataset\n","! wget https://cdn.iisc.talentsprint.com/CDS/Datasets/cancer_dataset.csv\n","print(\"The datset was downloaded\")\n"],"metadata":{"cellView":"form","id":"EoFtHC0wDtNV"},"id":"EoFtHC0wDtNV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load data"],"metadata":{"id":"dQ8zaJlc6rME"},"id":"dQ8zaJlc6rME"},{"cell_type":"code","execution_count":null,"id":"3845bf69","metadata":{"id":"3845bf69"},"outputs":[],"source":["import pandas as pd\n","df=pd.read_csv('cancer_dataset.csv')\n","df.head()"]},{"cell_type":"code","execution_count":null,"id":"ab19efdb","metadata":{"id":"ab19efdb"},"outputs":[],"source":["###  Independent And dependent features\n","X=df.iloc[:,2:]\n","y=df.iloc[:,1]"]},{"cell_type":"code","execution_count":null,"id":"5583d706","metadata":{"id":"5583d706"},"outputs":[],"source":["X.head()"]},{"cell_type":"code","execution_count":null,"id":"c1a26f91","metadata":{"id":"c1a26f91"},"outputs":[],"source":["X=X.dropna(axis=1)"]},{"cell_type":"code","execution_count":null,"id":"ecbb65c9","metadata":{"id":"ecbb65c9"},"outputs":[],"source":["# Independent Feature Set\n","X.head()"]},{"cell_type":"code","execution_count":null,"id":"f6f5ea09","metadata":{"id":"f6f5ea09"},"outputs":[],"source":["# Dependent Feature\n","y.value_counts()"]},{"cell_type":"markdown","id":"403c6b52","metadata":{"id":"403c6b52"},"source":["### HoldOut Validation Approach- Train And Test Split"]},{"cell_type":"markdown","source":["[Here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) is the official documentation of DecisionTreeClassifier"],"metadata":{"id":"0N56XQ1mIBl-"},"id":"0N56XQ1mIBl-"},{"cell_type":"code","execution_count":null,"id":"6c6313e1","metadata":{"id":"6c6313e1"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","# Splitting the data and fixin the random state.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=4)\n","model = DecisionTreeClassifier()\n","# Training the model\n","model.fit(X_train, y_train)\n","# Looking at the scores\n","result = model.score(X_test, y_test)\n","print(result)"]},{"cell_type":"markdown","id":"be48e090","metadata":{"id":"be48e090"},"source":["### K Fold Cross Validation"]},{"cell_type":"markdown","source":["[Here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) is the official documentation of KFold"],"metadata":{"id":"w633MiIgINDJ"},"id":"w633MiIgINDJ"},{"cell_type":"code","execution_count":null,"id":"82eecd5a","metadata":{"id":"82eecd5a"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","model=DecisionTreeClassifier()\n","kfold_validation=KFold(10)"]},{"cell_type":"code","execution_count":null,"id":"91f94014","metadata":{"id":"91f94014"},"outputs":[],"source":["import numpy as np\n","from sklearn.model_selection import cross_val_score\n","# Displaying the cross validation score\n","results=cross_val_score(model,X,y,cv=kfold_validation)\n","print(results)\n","print(np.mean(results))"]},{"cell_type":"markdown","id":"c54fcbbd","metadata":{"id":"c54fcbbd"},"source":["### Stratified K-fold Cross Validation"]},{"cell_type":"markdown","source":["[Here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) is the official documentation of StratifiedKFold"],"metadata":{"id":"70FsLMQyKL53"},"id":"70FsLMQyKL53"},{"cell_type":"code","execution_count":null,"id":"ccd42d3c","metadata":{"id":"ccd42d3c"},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","skfold=StratifiedKFold(n_splits=5)\n","model=DecisionTreeClassifier()\n","scores=cross_val_score(model,X,y,cv=skfold)\n","print(np.mean(scores))"]},{"cell_type":"code","execution_count":null,"id":"484be06e","metadata":{"id":"484be06e"},"outputs":[],"source":["scores"]},{"cell_type":"markdown","id":"fcb97b00","metadata":{"id":"fcb97b00"},"source":["### Leave One Out Cross Validation(LOOCV)"]},{"cell_type":"markdown","source":["[Here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut) is the official documentation of LeaveOneOut"],"metadata":{"id":"pgwV5DWsKTei"},"id":"pgwV5DWsKTei"},{"cell_type":"code","execution_count":null,"id":"33cc252e","metadata":{"id":"33cc252e"},"outputs":[],"source":["from sklearn.model_selection import LeaveOneOut\n","model=DecisionTreeClassifier()\n","leave_validation=LeaveOneOut()\n","results=cross_val_score(model,X,y,cv=leave_validation)"]},{"cell_type":"code","execution_count":null,"id":"73a4e15e","metadata":{"id":"73a4e15e"},"outputs":[],"source":["results"]},{"cell_type":"code","execution_count":null,"id":"d5a1c9e0","metadata":{"id":"d5a1c9e0"},"outputs":[],"source":["print(np.mean(results))"]},{"cell_type":"markdown","id":"18d2f281","metadata":{"id":"18d2f281"},"source":["### Repeated Random Test-Train Splits\n","This technique is a hybrid of traditional train-test splitting and the k-fold cross-validation method. In this technique, we create random splits of the data in the training-test set manner and then repeat the process of splitting and evaluating the algorithm multiple times, just like the cross-validation method."]},{"cell_type":"markdown","source":["[Here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit) is the official documentation of ShuffleSplit"],"metadata":{"id":"mgLlokXkKe2J"},"id":"mgLlokXkKe2J"},{"cell_type":"code","execution_count":null,"id":"98b7f1d4","metadata":{"id":"98b7f1d4"},"outputs":[],"source":["from sklearn.model_selection import ShuffleSplit\n","model=DecisionTreeClassifier()\n","ssplit=ShuffleSplit(n_splits=10,test_size=0.30)\n","results=cross_val_score(model,X,y,cv=ssplit)"]},{"cell_type":"code","execution_count":null,"id":"462ea9f5","metadata":{"id":"462ea9f5"},"outputs":[],"source":["# view the results\n","results"]},{"cell_type":"code","execution_count":null,"id":"4def7fd7","metadata":{"id":"4def7fd7"},"outputs":[],"source":["# Taking the mean of results\n","np.mean(results)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}