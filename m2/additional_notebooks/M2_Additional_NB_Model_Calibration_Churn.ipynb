{"cells":[{"cell_type":"markdown","source":["# Advanced Certification Program in Computational Data Science\n","## A Program by IISc and TalentSprint\n","### Additional Notebook (ungraded) on Model Calibration Churn\n"],"metadata":{"id":"8R9Uj-T5HUZu"}},{"cell_type":"markdown","source":["## Learning Objectives"],"metadata":{"id":"VZhZn_6XHe_r"}},{"cell_type":"markdown","source":["In this notebook, we will learn about calibrating machine learning models to obtain sensible and interpretable probabilities as outputs\n","\n","- What is model calibration and why it is important\n","\n","- When to and When NOT to calibrate models\n","\n","- How to assess whether a model is calibrated (reliability curves)\n","\n","- Different techniques to calibrate a Machine Learning model"],"metadata":{"id":"AaaKuWQ3Hzny"}},{"cell_type":"markdown","source":["## Introduction\n","\n","**What is model calibration?**\n","\n","Model calibration adjusts a model's outputs to ensure they reflect true probabilities. In binary classification, an uncalibrated model's output, such as 0.4 for predicting a dog, cannot be interpreted directly as a probability. Calibration ensures that if the model outputs 0.4, then ideally, 4 out of 10 such predictions would indeed be dogs. This process aligns the model's predicted probabilities with actual outcomes, making them more interpretable and reliable.\n","\n","**When to calibrate models and why it is critical?**\n","\n","Calibration is vital for applications where accurate probability estimates are crucial, such as predicting fire alarms. For an uncalibrated model, a prediction score of 0.9 does not reliably indicate that a fire is three times more likely than a score of 0.3. Calibration ensures these scores can be interpreted correctly, allowing for informed decision-making.\n","\n","For example, if improving a fire detection system costs $200,000, a calibrated model helps determine whether a change in prediction scores (from 0.3 to 0.35 and 0.9 to 0.7) justifies the investment. Without calibration, these scores lack meaningful interpretation, making it difficult to assess potential gains accurately.\n","\n","Model calibration is particularly important in production environments where models are continually improved through learning and feedback. Accurate probability estimates enable better risk assessment, resource allocation, and overall decision-making, ensuring the model's outputs are reliable and actionable."],"metadata":{"id":"_JxgBdy2YjDJ"}},{"cell_type":"markdown","source":["## Dataset Description\n","\n","**The Customer Churn** table contains information on all 7,043 customers from a Telecommunications company in California for Q2 2022.\n","\n","This dataset contains 2 tables, in CSV format:\n","\n","Each record represents one customer, and contains details about their demographics, location, tenure, subscription services, status for the quarter (joined, stayed, or churned), and more!\n","The Zip Code Population table contains complimentary information on the estimated populations for the California zip codes in the Customer Churn table"],"metadata":{"id":"Jr7kgJupX9fx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYMip47REuce"},"outputs":[],"source":["!mkdir ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","source":["# @title Download the Dataset\n","!wget https://cdn.exec.talentsprint.com/static/cds/content/telecom_customer_churn_by_maven_analytics.zip\n","!unzip /content/telecom_customer_churn_by_maven_analytics.zip\n","print(\"The datset was downloaded\")"],"metadata":{"cellView":"form","id":"Xn1KSlGiI9E9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2b_WCBjYpfJ"},"outputs":[],"source":["!pip install ml_insights -q"]},{"cell_type":"markdown","source":["## Import required packages"],"metadata":{"id":"ifpRkR8SKEWq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"27McswWDYe-J"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import ml_insights as mli\n","%matplotlib inline\n","import lightgbm as lgbm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import log_loss, roc_auc_score, f1_score\n","from sklearn.isotonic import IsotonicRegression\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"markdown","source":["## Load data"],"metadata":{"id":"Fp-5hVplJ09M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qe39q7TY-Uz"},"outputs":[],"source":["data = pd.read_csv('/content/telecom_customer_churn.csv')\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ga0u9QlZCpt"},"outputs":[],"source":["data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4uOhBb1ZLBZ"},"outputs":[],"source":["data = data.iloc[:,[1,2,3,4,9,10,11,12,15,26,27,28,29,30,31,32,33,34,35]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eO8D8Vy6wMLk"},"outputs":[],"source":["data = data[data['Customer Status']!='Joined'].reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bD-keaGoyACy"},"outputs":[],"source":["data['Customer Status'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugBWvIVsxsy4"},"outputs":[],"source":["data['Customer Status'] = data['Customer Status'].apply(lambda x: 1 if x=='Churned' else 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CH_2j1jiyArb"},"outputs":[],"source":["data['Customer Status'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wHLa1nRuxNa"},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ho_NPaU7xEX8"},"outputs":[],"source":["# Extract categoricals and their indices\n","cat_features = data.select_dtypes(exclude=np.number).columns.to_list()\n","cat_idx = [data.columns.get_loc(col) for col in cat_features]\n","\n","# Convert cat_features to pd.Categorical dtype\n","for col in cat_features:\n","    data[col] = pd.Categorical(data[col])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iskCiU4vv8Ll"},"outputs":[],"source":["data.info()"]},{"cell_type":"markdown","metadata":{"id":"UmzqSwpGNBNq"},"source":["## Splitting the data"]},{"cell_type":"code","source":["# Set the percentage splits for the training, validation, and test datasets\n","train_perc = 0.4\n","val_perc = 0.3\n","test_perc = 0.3\n","\n","# Set the random seed for reproducibility\n","rs = 1234\n","\n","# Split the data into a combined training/validation set and a test set\n","# 'test_size=test_perc' means 30% of the data will be assigned to the test set\n","# 'random_state=rs' ensures the split is reproducible\n","# 'stratify=data.iloc[:,-1]' ensures the split maintains the same proportion of classes as in the original data\n","X_train_val, X_test, y_train_val, y_test = train_test_split(\n","    data.iloc[:, :-1],  # Features\n","    data.iloc[:, -1],   # Labels\n","    test_size=test_perc,\n","    random_state=rs,\n","    stratify=data.iloc[:, -1]\n",")\n","\n","# Split the combined training/validation set into separate training and validation sets\n","# 'test_size=val_perc' means 30% of the remaining data (after removing the test set) will be assigned to the validation set\n","# 'random_state=rs' ensures the split is reproducible\n","# 'stratify=y_train_val' ensures the split maintains the same proportion of classes as in the original data\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train_val,  # Combined training/validation features\n","    y_train_val,  # Combined training/validation labels\n","    test_size=val_perc,\n","    random_state=rs,\n","    stratify=y_train_val\n",")\n"],"metadata":{"id":"5WiSJ9ANfHEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCwNIMfwbLBR"},"outputs":[],"source":["# Check the shape of the training set features (X_train) and labels (y_train)\n","X_train.shape, y_train.shape\n","\n","# Check the shape of the validation set features (X_val) and labels (y_val)\n","X_val.shape, y_val.shape\n","\n","# Check the shape of the test set features (X_test) and labels (y_test)\n","X_test.shape, y_test.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YWSZ2PTbisM"},"outputs":[],"source":["y_train.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mo-35ZbXb303"},"outputs":[],"source":["y_val.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDycANGAb4WK"},"outputs":[],"source":["y_test.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"ynWDOUysMRWb"},"source":["## Uncallibrated Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lms1hjW_cLO6"},"outputs":[],"source":["# Measure the time taken to execute the following code block\n","%%time\n","\n","# Initialize the LightGBM classifier with specified hyperparameters\n","lgbm_clf = lgbm.LGBMClassifier(\n","    objective=\"binary\",       # Set the objective to binary classification\n","    random_state=rs,          # Ensure reproducibility with a fixed random seed\n","    n_estimators=10,          # Set the number of boosting iterations (trees)\n","    boosting=\"gbdt\",          # Use Gradient Boosting Decision Trees (default method)\n","    class_weight=\"balanced\",  # Adjust weights to handle class imbalance\n",")\n","\n","# Train the LightGBM classifier on the training data\n","# 'categorical_feature=cat_idx' specifies which features are categorical\n","lgbm_clf.fit(\n","    X_train,      # Training features\n","    y_train,      # Training labels\n","    categorical_feature=cat_idx  # List of indices of categorical features\n",")\n","\n","# Predict probabilities for the validation set\n","# 'predict_proba' returns the probability of each class, [:,1] extracts the probability of the positive class\n","preds_uncalibrated_val = lgbm_clf.predict_proba(X_val)[:, 1]\n","\n","# Calculate and print the log loss on the validation set\n","# Log loss measures the accuracy of the probabilistic predictions\n","print(f\"LightGBM logloss on the evaluation set: {log_loss(y_val, preds_uncalibrated_val):.5f}\")\n","\n","# Calculate and print the ROC-AUC score on the validation set\n","# ROC-AUC measures the ability of the classifier to distinguish between classes\n","print(f\"LightGBM ROC-AUC on the evaluation set: {roc_auc_score(y_val, preds_uncalibrated_val):.5f}\")\n","\n","# Calculate and print the F1 score on the validation set\n","# F1 score is the harmonic mean of precision and recall, using binary predictions\n","print(f\"LightGBM F1 on the evaluation set: {f1_score(y_val, lgbm_clf.predict(X_val)):.5f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wW-DmsYgfn_"},"outputs":[],"source":["# Predict probabilities for the test set\n","# 'predict_proba' returns the probability of each class, [:,1] extracts the probability of the positive class\n","preds_uncalibrated_test = lgbm_clf.predict_proba(X_test)[:, 1]\n","\n","# Calculate and print the log loss on the test set\n","# Log loss measures the accuracy of the probabilistic predictions\n","print(f\"LightGBM logloss on the test set: {log_loss(y_test, preds_uncalibrated_test):.5f}\")\n","\n","# Calculate and print the ROC-AUC score on the test set\n","# ROC-AUC measures the ability of the classifier to distinguish between classes\n","print(f\"LightGBM ROC-AUC on the test set: {roc_auc_score(y_test, preds_uncalibrated_test):.5f}\")\n","\n","# Calculate and print the F1 score on the test set\n","# F1 score is the harmonic mean of precision and recall, using binary predictions\n","print(f\"LightGBM F1 on the test set: {f1_score(y_test, lgbm_clf.predict(X_test)):.5f}\")\n","\n","# Print a blank line for better readability of output\n","print()\n","\n","# Plot the reliability diagram for the test set predictions\n","# Set the figure size\n","plt.figure(figsize=(15, 5))\n","\n","# Create and display the reliability diagram with histogram\n","# 'mli.plot_reliability_diagram' is assumed to be a function from an external library to plot the reliability diagram\n","# 'show_histogram=True' adds a histogram of predicted probabilities\n","rd = mli.plot_reliability_diagram(y_test, preds_uncalibrated_test, show_histogram=True)\n"]},{"cell_type":"markdown","source":["We can see our log-loss has definitely reduced here. Since we have many data points with model predictions close to 0, we can see the benefit of using the Ml-insights package (and its logit scaling feature) here."],"metadata":{"id":"Ciy8YFy9i5eN"}},{"cell_type":"markdown","metadata":{"id":"coVIJSC5M0yQ"},"source":["## Sklearn Plots\n","\n","**The reliability curve** visually identifies whether a model is calibrated. Here's how it's created:\n","\n","- Bin the data: Divide predicted outputs into bins from 0 to 1. For example, using 0.1 intervals, create 10 bins.\n","\n","- Assign data points: Place data points into these bins based on their predicted values. For instance, if five data points (0.05, 0.05, 0.02, 0.01, 0.02) fall into the 0-0.1 bin, the average prediction is 0.03.\n","\n","- Plot points: On the X-axis, plot the average prediction (0.03). On the Y-axis, plot the empirical probabilityâ€”the fraction of data points with ground truth equal to 1. If 1 out of 5 points has a ground truth of 1, the Y value is 0.2. Thus, the first point is [0.03, 0.2].\n","\n","- Form the line: Repeat for all bins, connect the points to form a line, and compare it to the y = x line.\n","If points are above y = x, the model under-predicts the true probability; if below, it over-predicts. This plot can be constructed using Sklearn.\n","\n","Reference - https://towardsdatascience.com/classifier-calibration-7d0be1e05452"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfoHank4z3df"},"outputs":[],"source":["from sklearn.calibration import calibration_curve\n","\n","\n","def plot_calibration_curve(name, fig_index, probs):\n","    \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n","\n","    fig = plt.figure(fig_index, figsize=(10, 10))\n","    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n","    ax2 = plt.subplot2grid((3, 1), (2, 0))\n","\n","    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n","\n","    frac_of_pos, mean_pred_value = calibration_curve(y_test, probs, n_bins=20)\n","\n","    ax1.plot(mean_pred_value, frac_of_pos, \"s-\", label=f'{name}')\n","    ax1.set_ylabel(\"Fraction of positives\")\n","    ax1.set_ylim([-0.05, 1.05])\n","    ax1.legend(loc=\"lower right\")\n","    ax1.set_title(f'Calibration plot ({name})')\n","\n","    ax2.hist(probs, range=(0, 1), bins=20, label=name, histtype=\"step\", lw=2)\n","    ax2.set_xlabel(\"Mean predicted value\")\n","    ax2.set_ylabel(\"Count\")\n","\n","plot_calibration_curve(\"LightGBM Uncalibrated\", 1, preds_uncalibrated_test)\n"]},{"cell_type":"markdown","metadata":{"id":"k-sRu50SpDJq"},"source":["## Platt Scaling\n","\n","Platt Scaling assumes that there is a logistic relationship between the model predictions and the true probabilities.\n","\n","We simply use a logistic regressor to fit on the model predictions for the validation set and the true probabilities of this validation set as the outputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgxbBZaTnt8t"},"outputs":[],"source":["# Fit Platt scaling (logistic calibration)\n","lr = LogisticRegression(C=99999999999, solver='lbfgs')\n","lr.fit(preds_uncalibrated_val.reshape(-1,1), y_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDrL7nIHoUga"},"outputs":[],"source":["testset_platt_probs = lr.predict_proba(preds_uncalibrated_test.reshape(-1,1))[:,1]\n","print(f\"LightGBM logloss on the test set: {log_loss(y_test, testset_platt_probs):.5f}\")\n","print(f\"LightGBM ROC-AUC on the test set: {roc_auc_score(y_test, testset_platt_probs):.5f}\")\n","print()\n","plt.figure(figsize=(15,5))\n","mli.plot_reliability_diagram(y_test, testset_platt_probs,show_histogram=True,scaling='logit');\n","plt.title('Reliability Diagram on Test Data\\n after Platt Calibration');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvqx6PsMfpG2"},"outputs":[],"source":["mli.plot_reliability_diagram(y_test, testset_platt_probs,error_bars=False);\n","tvec = np.linspace(.01, .99, 99)\n","plt.plot(tvec, lr.predict_proba(tvec.reshape(-1,1))[:,1]);\n","plt.title('Platt Calibration Curve on Calibration Data');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2FKqyvBpL3H"},"outputs":[],"source":["print('Platt calibrated log_loss = {}'.format(log_loss(y_test, testset_platt_probs)))\n","print('Uncalibrated log_loss = {}'.format(log_loss(y_test, preds_uncalibrated_test)))\n","print('Platt calibrated ROC_AUC = {}'.format(roc_auc_score(y_test, testset_platt_probs)))\n","print('Uncalibrated ROC_AUC = {}'.format(roc_auc_score(y_test, preds_uncalibrated_test)))"]},{"cell_type":"markdown","metadata":{"id":"EFYJ3Tw-pFcg"},"source":["## Isotonic Regression\n","\n","This method combines Bayesian classifiers and Decision trees to calibrate models and works better than Platt scaling when we have enough data for it to fit. The detailed algorithm can be found here: https://cseweb.ucsd.edu/~elkan/calibrated.pdf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DI4RAYyXpHQR"},"outputs":[],"source":["iso = IsotonicRegression(out_of_bounds = 'clip')\n","iso.fit(preds_uncalibrated_val, y_val)\n","testset_iso_probs = iso.predict(preds_uncalibrated_test)\n","plt.figure(figsize=(15,5))\n","mli.plot_reliability_diagram(y_test, testset_iso_probs,show_histogram=True,scaling='logit');\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eE3Ed3DAfeAE"},"outputs":[],"source":["mli.plot_reliability_diagram(y_test, testset_iso_probs,error_bars=False);\n","tvec = np.linspace(0, 1, 99)\n","plt.plot(tvec, iso.predict(tvec));\n","plt.title('Isotonic Calibration Curve on Calibration Data');"]},{"cell_type":"markdown","source":["This seems to work better than Platt scaling for our data. Although it would be much wiser to come to such conclusions after averaging the results of these experiments over different data splits and random seeds or using cross-validation"],"metadata":{"id":"H4JghN0PjPwe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LpRuULs1qIL0"},"outputs":[],"source":["print('Isotonic calibrated log_loss = {}'.format(log_loss(y_test, testset_iso_probs)))\n","print('Uncalibrated log_loss = {}'.format(log_loss(y_test, preds_uncalibrated_test)))\n","print('Isotonic calibrated ROC_AUC = {}'.format(roc_auc_score(y_test, testset_iso_probs)))\n","print('Uncalibrated ROC_AUC = {}'.format(roc_auc_score(y_test, preds_uncalibrated_test)))"]},{"cell_type":"markdown","metadata":{"id":"M0z6ry2Yqg6y"},"source":["## Spline Calib\n","\n","The Spline Calib algorithm uses a smooth cubic polynomial (which is chosen to minimize a certain loss) and is fit on the model predictions on the validation set and its true probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2H7o5M9xqizC"},"outputs":[],"source":["splinecalib = mli.SplineCalib()\n","splinecalib.fit(preds_uncalibrated_val, y_val)\n","testset_splinecalib_probs = splinecalib.predict(preds_uncalibrated_test)\n","plt.figure(figsize=(15,5))\n","mli.plot_reliability_diagram(y_test, testset_splinecalib_probs,show_histogram=True,scaling='logit');\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJkykh6WePLi"},"outputs":[],"source":["mli.plot_reliability_diagram(y_test, testset_splinecalib_probs,error_bars=False);\n","tvec = np.linspace(0, 1, 999)\n","plt.plot(tvec, splinecalib.predict(tvec));\n","plt.title('Spline Calibration Curve on Calibration Data');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0ioClXQq7eB"},"outputs":[],"source":["print('Spline calibrated log_loss = {}'.format(log_loss(y_test, testset_splinecalib_probs)))\n","print('Uncalibrated log_loss = {}'.format(log_loss(y_test, preds_uncalibrated_test)))\n","print('Spline calibrated ROC_AUC = {}'.format(roc_auc_score(y_test, testset_splinecalib_probs)))\n","print('Uncalibrated ROC_AUC = {}'.format(roc_auc_score(y_test, preds_uncalibrated_test)))"]},{"cell_type":"markdown","metadata":{"id":"wnTiyq0G0cPx"},"source":["## Sklearn Comparision"]},{"cell_type":"code","source":["# Create a figure and two subplots: one for the reliability curve and one for the histograms\n","fig = plt.figure(0, figsize=(10, 10))\n","\n","# First subplot: reliability curve\n","ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n","\n","# Second subplot: histograms of predicted probabilities\n","ax2 = plt.subplot2grid((3, 1), (2, 0))\n","\n","# Plot the perfectly calibrated line (y=x) for reference\n","ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n","\n","# Calculate the calibration curve for the uncalibrated predictions\n","frac_of_pos_0, mean_pred_value_0 = calibration_curve(y_test, preds_uncalibrated_test, n_bins=20)\n","# Plot the calibration curve for the uncalibrated predictions\n","ax1.plot(mean_pred_value_0, frac_of_pos_0, label='Uncalibrated')\n","\n","# Calculate the calibration curve for the Platt scaled predictions\n","frac_of_pos_1, mean_pred_value_1 = calibration_curve(y_test, testset_platt_probs, n_bins=20)\n","# Plot the calibration curve for the Platt scaled predictions\n","ax1.plot(mean_pred_value_1, frac_of_pos_1, label='Platt Scaled')\n","\n","# Calculate the calibration curve for the isotonic regression predictions\n","frac_of_pos_2, mean_pred_value_2 = calibration_curve(y_test, testset_iso_probs, n_bins=20)\n","# Plot the calibration curve for the isotonic regression predictions\n","ax1.plot(mean_pred_value_2, frac_of_pos_2, label='Isotonic Regression')\n","\n","# Calculate the calibration curve for the SplineCalib predictions\n","frac_of_pos_3, mean_pred_value_3 = calibration_curve(y_test, testset_splinecalib_probs, n_bins=20)\n","# Plot the calibration curve for the SplineCalib predictions\n","ax1.plot(mean_pred_value_3, frac_of_pos_3, label='SplineCalib')\n","\n","# Set the y-axis label for the reliability curve\n","ax1.set_ylabel(\"Fraction of positives\")\n","# Set the y-axis limits for the reliability curve\n","ax1.set_ylim([-0.05, 1.05])\n","# Add a legend to the reliability curve plot\n","ax1.legend(loc=\"lower right\")\n","# Set the title for the reliability curve plot\n","ax1.set_title(f'Comparison of Reliability Curves')\n","\n","# Plot histograms of the predicted probabilities for each method\n","ax2.hist(preds_uncalibrated_test, range=(0, 1), bins=20, histtype=\"step\", lw=2, label='Uncalibrated')\n","ax2.hist(testset_platt_probs, range=(0, 1), bins=20, histtype=\"step\", lw=2, label='Platt Scaled')\n","ax2.hist(testset_iso_probs, range=(0, 1), bins=20, histtype=\"step\", lw=2, label='Isotonic Regression')\n","ax2.hist(testset_splinecalib_probs, range=(0, 1), bins=20, histtype=\"step\", lw=2, label='SplineCalib')\n","\n","# Set the x-axis label for the histogram plot\n","ax2.set_xlabel(\"Mean predicted value\")\n","# Set the y-axis label for the histogram plot\n","ax2.set_ylabel(\"Count\")\n","# Add a legend to the histogram plot\n","ax2.legend(loc=\"upper left\")\n","\n","# Show the plot\n","plt.show()\n"],"metadata":{"id":"AONeoRFpePAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8k6twlFN76LC"},"outputs":[],"source":["(iso.predict(preds_uncalibrated_test)>=0.5).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLZiqIlH338Y"},"outputs":[],"source":["print('Uncalibrated log_loss = {}'.format(log_loss(y_test, preds_uncalibrated_test)))\n","print('Uncalibrated ROC_AUC = {}'.format(roc_auc_score(y_test, preds_uncalibrated_test)))\n","print(f\"Uncalibrated F1 on the test set: {f1_score(y_test, lgbm_clf.predict(X_test)):.5f}\")\n","print()\n","print('Platt calibrated log_loss = {}'.format(log_loss(y_test, testset_platt_probs)))\n","print('Platt calibrated ROC_AUC = {}'.format(roc_auc_score(y_test, testset_platt_probs)))\n","print(f\"Platt calibrated F1 on the test set: {f1_score(y_test, lr.predict(preds_uncalibrated_test.reshape(-1,1))):.5f}\")\n","\n","print()\n","print('Isotonic calibrated log_loss = {}'.format(log_loss(y_test, testset_iso_probs)))\n","print('Isotonic calibrated ROC_AUC = {}'.format(roc_auc_score(y_test, testset_iso_probs)))\n","print(f\"Isotonic calibrated F1 on the test set: {f1_score(y_test,(iso.predict(preds_uncalibrated_test)>=0.5).astype(int)):.5f}\")\n","\n","print()\n","print('Spline calibrated log_loss = {}'.format(log_loss(y_test, testset_splinecalib_probs)))\n","print('Spline calibrated ROC_AUC = {}'.format(roc_auc_score(y_test, testset_splinecalib_probs)))\n","print(f\"Uncalibrated F1 on the test set: {f1_score(y_test, (splinecalib.predict(preds_uncalibrated_test)>=0.5)):.5f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"bqFhBlafMv0y"},"source":["## ECE and OE\n","\n","Reference - https://medium.com/@wolframalphav1.0/evaluate-the-performance-of-a-model-in-high-risk-applications-using-expected-calibration-error-and-dbc392c68318\n","\n","Expected Calibration Error and its Flaws:\n","\n","ECE is often used as a metric to measure how well a model is calibrated.\n","\n","Here is how ECE is formally calculated.\n","\n","1. Choose n the number of bins as we did earlier\n","For each bin calculate the average of the model predictions of the data points belonging to that bin and normalize it by the number of data points in that bin.\n","For each bin also calculate the fraction of true positives.\n","2. Now for each bin calculate the absolute difference between the values calculated in step 3 and step 2and multiply this absolute difference by the number of data points in that bin.\n","3. Add the results for all bins calculated in step 4 and normalize this added sum by the total number of samples in all the bins.\n","\n","The ECE quantifies the overall calibration error by averaging the absolute differences between predicted probabilities and actual outcomes, weighted by the number of samples in each bin. The OE focuses on the specific error due to overconfidence by only considering cases where the predicted probability exceeds the actual accuracy.\n","\n","### Expected Calibration Error (ECE)\n","\n","The Expected Calibration Error (ECE) measures the discrepancy between predicted probabilities and actual outcomes. It is calculated as:\n","\n","$$\n","\\text{ECE} = \\sum_{m=1}^M \\frac{|B_m|}{n} \\left| \\text{acc}(B_m) - \\text{conf}(B_m) \\right|\n","$$\n","\n","### Overconfidence Error (OE)\n","\n","The Overconfidence Error (OE) measures the degree to which the model's predicted probabilities are overconfident compared to actual outcomes. It is calculated as:\n","\n","$$\n","\\text{OE} = \\sum_{m=1}^M \\frac{|B_m|}{n} \\cdot \\text{conf}(B_m) \\cdot \\max(\\text{conf}(B_m) - \\text{acc}(B_m), 0)\n","$$\n","\n","\n","However, in our case above, the distribution of the data points across the bins was not very uniform (since most data points belonged to the first bin) and thus it is imperative to select the bins for ECE accordingly. We can see how the number of bins is directly affecting ECE in the algorithm.\n","\n","**For instance for only 5 bins, the uncalibrated model seemed to have lesser calibration error than all the other methods. However when we increase the number of bins, we can see that model calibration has actually helped in our case.**\n","\n","In the code snippets below, this effect can be verified.\n"]},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","\n","# Define a function to calculate calibration error\n","def calibration_error(y_true, y_prob, n_bins=5, strategy='uniform', return_expected_calibration_error=True):\n","    # Determine the bin edges based on the chosen strategy\n","    if strategy == 'quantile':  # Determine bin edges by distribution of data\n","        quantiles = np.linspace(0, 1, n_bins + 1)  # Create quantiles\n","        bins = np.percentile(y_prob, quantiles * 100)  # Determine bin edges based on percentiles\n","        bins[-1] = bins[-1] + 1e-8  # Ensure the last bin edge is slightly larger to include the maximum value\n","    elif strategy == 'uniform':  # Use uniform binning\n","        bins = np.linspace(0., 1. + 1e-8, n_bins + 1)  # Create uniform bins\n","    else:\n","        raise ValueError(\"Invalid entry to 'strategy' input. Strategy must be either 'quantile' or 'uniform'.\")\n","\n","    # Get the maximum predicted probability for each sample\n","    y_prob_max = np.max(y_prob, axis=-1)\n","    # Assign each probability to a bin\n","    binids = np.digitize(y_prob_max, bins) - 1\n","\n","    # Determine if the predictions are correct\n","    y_correct_classified = (np.argmax(y_true, axis=-1) == np.argmax(y_prob, axis=-1)).astype(int)\n","\n","    # Sum of predicted probabilities in each bin\n","    bin_sums = np.bincount(binids, weights=y_prob_max, minlength=len(bins))\n","    # Sum of correctly classified samples in each bin\n","    bin_true = np.bincount(binids, weights=y_correct_classified, minlength=len(bins))\n","    # Total number of samples in each bin\n","    bin_total = np.bincount(binids, minlength=len(bins))\n","\n","    # Identify non-empty bins\n","    nonzero = bin_total != 0\n","\n","    # Calculate the true probability for each bin (accuracy)\n","    prob_true = bin_true[nonzero] / bin_total[nonzero]\n","\n","    # Calculate the predicted probability for each bin (confidence)\n","    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n","\n","    # Calculate the Expected Calibration Error (ECE)\n","    expected_calibration_error = np.sum(bin_total[nonzero] * np.abs(prob_true - prob_pred)) / bin_total[nonzero].sum()\n","\n","    # Calculate the Overconfidence Error (OE)\n","    overconfidence_error = np.sum(\n","        bin_total[nonzero] * prob_pred * np.max(\n","            np.concatenate(((prob_pred - prob_true).reshape(-1, 1), np.zeros((1, len(prob_pred))).T), axis=1),\n","            axis=-1\n","        )\n","    ) / bin_total[nonzero].sum()\n","\n","    # Return the true probabilities, predicted probabilities, ECE, and OE\n","    return prob_true, prob_pred, expected_calibration_error, overconfidence_error\n"],"metadata":{"id":"OddaZEFAfu8M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These metrics provide insight into how well a model's predicted probabilities reflect the true likelihood of outcomes, which is crucial for applications where probabilistic predictions are used for decision-making."],"metadata":{"id":"-Rf-atJGgE42"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxYPULqa_Sj4"},"outputs":[],"source":["lgbm_clf.predict_proba(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJWVLqY46m5N"},"outputs":[],"source":["_,_,ECE,OE = calibration_error(y_test,lgbm_clf.predict_proba(X_test))\n","print(f'Uncalibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,lr.predict_proba(preds_uncalibrated_test.reshape(-1,1)))\n","print(f'Platt Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,np.concatenate((1-iso.predict(preds_uncalibrated_test).reshape(-1,1),iso.predict(preds_uncalibrated_test).reshape(-1,1)),axis=1))\n","print(f'Isotonic Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,np.concatenate((1-splinecalib.predict(preds_uncalibrated_test).reshape(-1,1),splinecalib.predict(preds_uncalibrated_test).reshape(-1,1)),axis=1))\n","print(f'Spline Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fV8Xux7CBb61"},"outputs":[],"source":["#5 bins\n","_,_,ECE,OE = calibration_error(y_test,lgbm_clf.predict_proba(X_test))\n","print(f'Uncalibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,lr.predict_proba(preds_uncalibrated_test.reshape(-1,1)))\n","print(f'Platt Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,np.concatenate((1-iso.predict(preds_uncalibrated_test).reshape(-1,1),iso.predict(preds_uncalibrated_test).reshape(-1,1)),axis=1))\n","print(f'Isotonic Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,np.concatenate((1-splinecalib.predict(preds_uncalibrated_test).reshape(-1,1),splinecalib.predict(preds_uncalibrated_test).reshape(-1,1)),axis=1))\n","print(f'Spline Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPRLBm4oBjIO"},"outputs":[],"source":["#50 bins\n","_,_,ECE,OE = calibration_error(y_test,lgbm_clf.predict_proba(X_test),n_bins=50)\n","print(f'Uncalibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,lr.predict_proba(preds_uncalibrated_test.reshape(-1,1)),n_bins=50)\n","print(f'Platt Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,np.concatenate((1-iso.predict(preds_uncalibrated_test).reshape(-1,1),iso.predict(preds_uncalibrated_test).reshape(-1,1)),axis=1),n_bins=50)\n","print(f'Isotonic Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,np.concatenate((1-splinecalib.predict(preds_uncalibrated_test).reshape(-1,1),splinecalib.predict(preds_uncalibrated_test).reshape(-1,1)),axis=1),n_bins=50)\n","print(f'Spline Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCBElbmDBvZC"},"outputs":[],"source":["#500 bins\n","_,_,ECE,OE = calibration_error(y_test,lgbm_clf.predict_proba(X_test),n_bins=500)\n","print(f'Uncalibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,lr.predict_proba(preds_uncalibrated_test.reshape(-1,1)),n_bins=500)\n","print(f'Platt Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,np.concatenate((1-iso.predict(preds_uncalibrated_test).reshape(-1,1),iso.predict(preds_uncalibrated_test).reshape(-1,1)),axis=1),n_bins=500)\n","print(f'Isotonic Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,np.concatenate((1-splinecalib.predict(preds_uncalibrated_test).reshape(-1,1),splinecalib.predict(preds_uncalibrated_test).reshape(-1,1)),axis=1),n_bins=500)\n","print(f'Spline Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WncadFNB1ah"},"outputs":[],"source":["#5000 bins\n","_,_,ECE,OE = calibration_error(y_test,lgbm_clf.predict_proba(X_test),n_bins=5000)\n","print(f'Uncalibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,lr.predict_proba(preds_uncalibrated_test.reshape(-1,1)),n_bins=5000)\n","print(f'Platt Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,np.concatenate((1-iso.predict(preds_uncalibrated_test).reshape(-1,1),iso.predict(preds_uncalibrated_test).reshape(-1,1)),axis=1),n_bins=5000)\n","print(f'Isotonic Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n","_,_,ECE,OE = calibration_error(y_test,np.concatenate((1-splinecalib.predict(preds_uncalibrated_test).reshape(-1,1),splinecalib.predict(preds_uncalibrated_test).reshape(-1,1)),axis=1),n_bins=5000)\n","print(f'Spline Calibrated LGBM ECE = {ECE} OE = {OE}')\n","print()\n"]},{"cell_type":"markdown","source":["## Reading References:\n","\n","\n","\n","- Probability Calibration workshop Youtube Video Series:\n","\n"," https://www.youtube.com/playlist?list=PLeVfk5xTWHYBw22D52etymvcpxey4QFIk\n","\n","- Conformal prediction:\n","\n","  https://www.youtube.com/playlist?list=PLBa0oe-LYIHa68NOJbMxDTMMjT8Is4WkI\n","\n","- TorchCP:\n","  \n","  https://github.com/ml-stat-Sustech/TorchCP\n","\n","- AWS Fortuna:\n","\n","  https://aws-fortuna.readthedocs.io/en/latest/\n","\n"],"metadata":{"id":"flKCadq9ZWkh"}}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}