{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zOk-I5WV60d3"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the function\n","def f(t):\n","    return 3 * t**2 - 2 * t + 1\n","\n","# Find the vertex of the parabola\n","a, b, c = 3, -2, 1  # Coefficients of the quadratic function\n","t_vertex = -b / (2 * a)\n","f_vertex = f(t_vertex)\n","\n","# Display domain and range\n","print(f\"Domain: All real numbers (-∞, ∞)\")\n","print(f\"Vertex: t = {t_vertex}, f(t) = {f_vertex}\")\n","print(f\"Range: [{f_vertex}, ∞)\")\n","\n","# Generate values for plotting\n","t_values = np.linspace(-5, 5, 500)  # Generate 500 points from -5 to 5\n","f_values = f(t_values)\n","\n","# Plot the function\n","plt.figure(figsize=(8, 6))\n","plt.plot(t_values, f_values, label=r\"$f(t) = 3t^2 - 2t + 1$\", color=\"blue\")\n","plt.scatter([t_vertex], [f_vertex], color=\"red\", label=\"Vertex (Minimum Point)\")\n","plt.axhline(y=f_vertex, color='green', linestyle='--', label=f\"Range starts at {f_vertex}\")\n","plt.axvline(x=t_vertex, color='orange', linestyle='--', label=f\"t = {t_vertex}\")\n","\n","# Adding labels and legend\n","plt.title(\"Graph of f(t) = 3t² - 2t + 1\")\n","plt.xlabel(\"t (Domain)\")\n","plt.ylabel(\"f(t) (Range)\")\n","plt.axhline(y=0, color='black', linewidth=0.5)\n","plt.axvline(x=0, color='black', linewidth=0.5)\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the functions\n","def f(x):\n","    return x**2 + 1\n","\n","def g(x):\n","    return np.sqrt(x)\n","\n","# Define the compositions\n","def f_comp_g(x):\n","    return f(g(x))  # f(g(x)) = g(x)**2 + 1 = x + 1\n","\n","def g_comp_f(x):\n","    return g(f(x))  # g(f(x)) = sqrt(f(x)) = sqrt(x**2 + 1)\n","\n","# Generate x-values for the domain\n","x_values = np.linspace(0, 10, 500)  # For f ∘ g(x), x >= 0\n","x_values_all = np.linspace(-10, 10, 500)  # For g ∘ f(x), x ∈ ℝ\n","\n","# Calculate function values\n","f_values = f(x_values_all)\n","g_values = g(x_values)\n","f_comp_g_values = f_comp_g(x_values)\n","g_comp_f_values = g_comp_f(x_values_all)\n","\n","# Plot the functions\n","plt.figure(figsize=(10, 8))\n","\n","# Plot f ∘ g(x)\n","plt.subplot(2, 1, 1)\n","plt.plot(x_values, f_comp_g_values, label=r\"$f(g(x))$\", color=\"blue\")\n","plt.title(\"Composition: $f(g(x))$\")\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$f(g(x))$\")\n","plt.grid(True)\n","plt.legend()\n","\n","# Plot g ∘ f(x)\n","plt.subplot(2, 1, 2)\n","plt.plot(x_values_all, g_comp_f_values, label=r\"$g(f(x))$\", color=\"orange\")\n","plt.title(\"Composition: $g(f(x))$\")\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$g(f(x))$\")\n","plt.grid(True)\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"MLX_HepD8-Y9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Define normalization function g(x)\n","def normalize(x, mean, std):\n","    if std == 0:\n","        raise ValueError(\"Standard deviation cannot be zero.\")\n","    return (x - mean) / std\n","\n","# Define sigmoid function f(x)\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Input data\n","data = np.array([50, 60, 70])  # Raw data\n","\n","# Calculate mean and standard deviation\n","mean = np.mean(data)\n","std = np.std(data)\n","\n","# Calculate normalized values (g(x))\n","normalized_data = normalize(data, mean, std)\n","\n","# Calculate sigmoid of normalized values (f(g(x)))\n","sigmoid_output = sigmoid(normalized_data)\n","\n","# Display results\n","print(\"Original Data:\", data)\n","print(\"Normalized Data (g(x)):\", normalized_data)\n","print(\"Sigmoid of Normalized Data (f(g(x))):\", sigmoid_output)\n","\n","# Output the compositions as a table\n","print(\"\\nResults for Composition:\")\n","print(\"Raw Data | Normalized (g(x)) | Sigmoid of Normalized (f(g(x)))\")\n","for i in range(len(data)):\n","    print(f\"{data[i]:8} | {normalized_data[i]:16.2f} | {sigmoid_output[i]:30.4f}\")\n"],"metadata":{"id":"gLfsskbs_wzk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the function and the derivative\n","def f(x):\n","    return x**2\n","\n","def derivative_at_point(x):\n","    return 2*x\n","\n","# Define the point of tangency\n","x_tangent = 1\n","y_tangent = f(x_tangent)\n","\n","# Define a range of x values for plotting\n","x = np.linspace(0, 2, 100)\n","y = f(x)\n","\n","# Secant lines for different h values\n","h_values = [1, 0.5, 0.1]\n","colors = ['red', 'green', 'blue']\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(x, y, label=\"f(x) = x^2\", color=\"black\")\n","\n","for h, color in zip(h_values, colors):\n","    # Calculate secant line\n","    slope_secant = (f(x_tangent + h) - f(x_tangent)) / h\n","    secant_line = slope_secant * (x - x_tangent) + y_tangent\n","    plt.plot(x, secant_line, linestyle='--', label=f\"Secant (h = {h})\", color=color)\n","\n","# Tangent line\n","slope_tangent = derivative_at_point(x_tangent)\n","tangent_line = slope_tangent * (x - x_tangent) + y_tangent\n","plt.plot(x, tangent_line, linestyle='-', label=\"Tangent Line (h → 0)\", color=\"orange\")\n","\n","# Highlight the point of tangency\n","plt.scatter(x_tangent, y_tangent, color='black', label=\"Point of Tangency\")\n","\n","# Add labels and legend\n","plt.title(\"Secant and Tangent Lines\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"f(x)\")\n","plt.axhline(0, color='black', linewidth=0.5)\n","plt.axvline(0, color='black', linewidth=0.5)\n","plt.legend()\n","plt.grid()\n","plt.show()\n"],"metadata":{"id":"E2vg07a-B8Gd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the functions and their derivatives\n","def f1(x):\n","    return x**2\n","\n","def f1_prime(x):\n","    return 2*x\n","\n","def f2(x):\n","    return np.sin(x)\n","\n","def f2_prime(x):\n","    return np.cos(x)\n","\n","def f3(x):\n","    return np.exp(x)\n","\n","def f3_prime(x):\n","    return np.exp(x)\n","\n","# Plot function and tangent line\n","def plot_tangent(func, derivative, x_point, x_range, label):\n","    x = np.linspace(x_range[0], x_range[1], 500)\n","    y = func(x)\n","\n","    # Calculate the tangent line\n","    slope = derivative(x_point)\n","    tangent_line = slope * (x - x_point) + func(x_point)\n","\n","    # Plot the function\n","    plt.plot(x, y, label=f\"y = {label}\")\n","\n","    # Plot the tangent line\n","    plt.plot(x, tangent_line, '--', label=f\"Tangent at x = {x_point}\")\n","\n","    # Highlight the point of tangency\n","    plt.scatter(x_point, func(x_point), color='red', label=f\"Point of Tangency ({x_point}, {func(x_point):.2f})\")\n","    plt.title(f\"Tangent Line to {label} at x = {x_point}\")\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"y\")\n","    plt.axhline(0, color='black', linewidth=0.5)\n","    plt.axvline(0, color='black', linewidth=0.5)\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","# Plot for y = x^2\n","plot_tangent(f1, f1_prime, x_point=1, x_range=(-2, 2), label=\"x^2\")\n","\n","# Plot for y = sin(x)\n","plot_tangent(f2, f2_prime, x_point=np.pi/4, x_range=(-np.pi, np.pi), label=\"sin(x)\")\n","\n","# Plot for y = e^x\n","plot_tangent(f3, f3_prime, x_point=0, x_range=(-2, 2), label=\"e^x\")\n"],"metadata":{"id":"DqaVv41rDWkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the function and its derivative\n","def f(x):\n","    return np.sqrt(x)\n","\n","def f_prime(x):\n","    return 1 / (2 * np.sqrt(x))\n","\n","# Tangent line equation\n","def tangent_line(x, x_point):\n","    slope = f_prime(x_point)\n","    y_point = f(x_point)\n","    return slope * (x - x_point) + y_point\n","\n","# Plot range\n","x = np.linspace(0.1, 6, 500)\n","y = f(x)\n","\n","# Tangent line at x = 4\n","x_point = 4\n","tangent_y = tangent_line(x, x_point)\n","\n","# Plot the function and tangent line\n","plt.figure(figsize=(8, 6))\n","plt.plot(x, y, label=\"f(x) = sqrt(x)\", color=\"blue\")\n","plt.plot(x, tangent_y, '--', label=\"Tangent Line at x = 4\", color=\"orange\")\n","plt.scatter(x_point, f(x_point), color='red', label=f\"Point of Tangency ({x_point}, {f(x_point):.2f})\")\n","plt.axhline(0, color='black', linewidth=0.5)\n","plt.axvline(0, color='black', linewidth=0.5)\n","plt.title(\"Tangent Line Approximation\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n"],"metadata":{"id":"yvAQ1jMHFyDW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the loss function and its derivative\n","def loss(w):\n","    return (w - 3)**2  # Loss function L(w)\n","\n","def gradient(w):\n","    return 2 * (w - 3)  # Derivative L'(w)\n","\n","# Gradient descent function\n","def gradient_descent(start, learning_rate, steps):\n","    w = start\n","    history = [w]  # Store parameter values at each step\n","    for _ in range(steps):\n","        w = w - learning_rate * gradient(w)  # Update w\n","        history.append(w)\n","    return history\n","\n","# Visualization parameters\n","start_point = 0  # Initial parameter value\n","learning_rate = 0.1  # Step size\n","iterations = 10  # Number of gradient descent steps\n","\n","# Perform gradient descent\n","trajectory = gradient_descent(start=start_point, learning_rate=learning_rate, steps=iterations)\n","\n","# Generate data for plotting\n","w_values = np.linspace(-1, 5, 500)  # Range of parameter values\n","loss_values = loss(w_values)  # Loss function values\n","\n","# Plot loss function and gradient descent steps\n","plt.figure(figsize=(10, 6))\n","plt.plot(w_values, loss_values, label=\"Loss Function: $L(w) = (w - 3)^2$\", color=\"blue\")\n","plt.scatter(trajectory, [loss(w) for w in trajectory], color=\"red\", label=\"Gradient Descent Steps\", zorder=5)\n","\n","# Annotate gradient descent steps\n","for i, w in enumerate(trajectory):\n","    plt.text(w, loss(w), f\"Step {i}\", fontsize=8, ha='right')\n","\n","# Highlight the minimum point\n","plt.scatter([3], [loss(3)], color=\"green\", label=\"Minimum at w = 3\", zorder=6)\n","\n","# Labels and legend\n","plt.title(\"Gradient Descent Optimization\")\n","plt.xlabel(\"Parameter (w)\")\n","plt.ylabel(\"Loss (L(w))\")\n","plt.axhline(0, color='black', linewidth=0.5, linestyle='--')\n","plt.axvline(3, color='green', linewidth=0.5, linestyle='--')\n","plt.legend()\n","plt.grid()\n","plt.show()\n"],"metadata":{"id":"V0j98x9QIy_O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define the loss function and its derivative\n","def loss(w):\n","    return (w - 2)**2  # Loss function L(w)\n","\n","def gradient(w):\n","    return 2 * (w - 2)  # Derivative L'(w)\n","\n","# Define the tangent line\n","def tangent_line(w, w0):\n","    slope = gradient(w0)\n","    intercept = loss(w0) - slope * w0\n","    return slope * w + intercept\n","\n","# Perform gradient descent\n","def gradient_descent(start, learning_rate, steps):\n","    w = start\n","    history = [w]  # Store parameter values at each step\n","    for _ in range(steps):\n","        w = w - learning_rate * gradient(w)  # Update w\n","        history.append(w)\n","    return history\n","\n","# Visualization parameters\n","start_point = 0  # Initial guess for w\n","learning_rate = 0.2  # Learning rate\n","iterations = 5  # Number of gradient descent steps\n","\n","# Perform gradient descent\n","trajectory = gradient_descent(start=start_point, learning_rate=learning_rate, steps=iterations)\n","\n","# Generate data for plotting\n","w_values = np.linspace(-1, 5, 500)  # Range of w values\n","loss_values = loss(w_values)  # Loss function values\n","\n","# Plot loss function and tangent lines\n","plt.figure(figsize=(10, 6))\n","plt.plot(w_values, loss_values, label=\"Loss Function: $L(w) = (w - 2)^2$\", color=\"blue\")\n","\n","# Plot tangent lines at each gradient descent step\n","for i, w in enumerate(trajectory[:-1]):\n","    tangent_y = tangent_line(w_values, w)\n","    plt.plot(w_values, tangent_y, '--', label=f\"Tangent Line at Step {i+1}\")\n","\n","# Plot gradient descent steps\n","plt.scatter(trajectory, [loss(w) for w in trajectory], color=\"red\", label=\"Gradient Descent Steps\", zorder=5)\n","\n","# Highlight the minimum point\n","plt.scatter([2], [loss(2)], color=\"green\", label=\"Minimum at w = 2\", zorder=6)\n","\n","# Labels and legend\n","plt.title(\"Tangent Lines and Gradient Descent\")\n","plt.xlabel(\"Parameter (w)\")\n","plt.ylabel(\"Loss (L(w))\")\n","plt.axhline(0, color='black', linewidth=0.5, linestyle='--')\n","plt.axvline(2, color='green', linewidth=0.5, linestyle='--')\n","plt.legend()\n","plt.grid()\n","plt.show()\n"],"metadata":{"id":"bOW0mMXCLLhU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Define a vector\n","x1 = np.array([1, -2, 3])\n","x2 = np.array([1, -2, 367])\n","\n","# Compute L1, L2, and L_infinity norms\n","l1_norm = np.sum(np.abs(x1))  # L1 norm\n","l2_norm = np.sqrt(np.sum(x1**2))  # L2 norm\n","linf_norm = np.max(np.abs(x1))  # L_infinity norm\n","# Print the results\n","print(f\"L1 Norm: {l1_norm}\")\n","print(f\"L2 Norm: {l2_norm}\")\n","print(f\"L_infinity Norm: {linf_norm}\")\n","\n","# Compute L1, L2, and L_infinity norms\n","l1_norm = np.sum(np.abs(x2))  # L1 norm\n","l2_norm = np.sqrt(np.sum(x2**2))  # L2 norm\n","linf_norm = np.max(np.abs(x2))  # L_infinity norm\n","\n","\n","# Print the results\n","print(f\"L1 Norm: {l1_norm}\")\n","print(f\"L2 Norm: {l2_norm}\")\n","print(f\"L_infinity Norm: {linf_norm}\")\n"],"metadata":{"id":"5iV-BwLRQjfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# Step 1: Generate a correlated dataset\n","np.random.seed(42)\n","x1 = np.random.rand(100)  # Feature 1\n","x2 = 2 * x1 + np.random.rand(100) * 0.1  # Feature 2 (correlated with x1)\n","X = np.column_stack((x1, x2))\n","\n","# Step 2: Visualize the original dataset\n","plt.scatter(X[:, 0], X[:, 1], alpha=0.7, label='Original Data')\n","plt.title(\"Original Dataset\")\n","plt.xlabel(\"Feature 1 (x1)\")\n","plt.ylabel(\"Feature 2 (x2)\")\n","plt.grid()\n","plt.legend()\n","plt.show()\n","\n","# Step 3: Standardize the dataset\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Step 4: Apply PCA\n","pca = PCA(n_components=2)  # Find two principal components\n","X_pca = pca.fit_transform(X_scaled)  # Transform the dataset\n","\n","# Step 5: Visualize the PCA components\n","components = pca.components_  # Principal components (orthogonal vectors)\n","explained_variance = pca.explained_variance_ratio_\n","\n","print(\"Principal Components (Orthogonal Vectors):\\n\", components)\n","print(\"Explained Variance Ratio:\", explained_variance)\n","\n","# Visualize original data and PCA directions\n","plt.scatter(X_scaled[:, 0], X_scaled[:, 1], alpha=0.7, label='Standardized Data')\n","origin = [0, 0]\n","plt.quiver(*origin, components[0, 0], components[0, 1], color='r', scale=3, label='PC1 (Max Variance)')\n","plt.quiver(*origin, components[1, 0], components[1, 1], color='g', scale=3, label='PC2 (Orthogonal)')\n","plt.title(\"Principal Components on Standardized Data\")\n","plt.xlabel(\"Standardized Feature 1\")\n","plt.ylabel(\"Standardized Feature 2\")\n","plt.grid()\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"lO6TdSgYUYRz"},"execution_count":null,"outputs":[]}]}