{"cells":[{"cell_type":"markdown","metadata":{"id":"CcvwSgBcnqyi"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","\n","### Additional Notebook (Ungraded) : Variational Autoencoder"]},{"cell_type":"markdown","metadata":{"id":"cVUFNqoknqyj"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"0je9rD0ynqyk"},"source":["At the end of the experiment, you will be able to\n","\n","* perform unsupervised pretraining using autoencoder\n","* know what is convolutional autoencoder\n","* know what are Variational autoencoders\n","* generate Fashion MNIST images using Variational autoencoder"]},{"cell_type":"markdown","metadata":{"id":"Sp5hONPtcrId"},"source":["### Unsupervised Pretraining Using Stacked Autoencoders"]},{"cell_type":"markdown","metadata":{"id":"AsjmD8DTcrId"},"source":["If we have a large dataset but most of it is unlabeled, we can first train a stacked autoencoder using all the data, then reuse the lower layers to create a neural network for the actual task and train it using the labeled data.\n","\n","For example, the below figure shows how to use a stacked autoencoder to perform **unsupervised pretraining** for a classification neural network.\n","<br><br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Unsupervised%20pre-training%20Autoencoders.png\" width=550px/>\n","</center>\n","\n","$\\hspace{8.6cm} \\text{Unsupervised pretraining using autoencoders}$\n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"nm4NPmuecrId"},"source":["For the implementation: just train an autoencoder using all the training data (labeled plus unlabeled), then reuse its encoder layers to create a new neural network.\n","\n","Let’s look at a few techniques for training stacked autoencoders:\n","\n","* Tying weights\n","* Training one autoencoder at a time"]},{"cell_type":"markdown","metadata":{"id":"6DwBVo0Enqyo"},"source":["### Import required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4q3t8b9cnqyp"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.datasets import fashion_mnist\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Flatten, Dense, Reshape, Conv2D, MaxPool2D, Conv2DTranspose\n","\n","from tensorflow.keras import Model, Input\n","from tensorflow.keras.layers import Flatten, Dense\n","import tensorflow.keras.backend as K\n","\n","from tensorflow.keras.layers import Layer"]},{"cell_type":"markdown","metadata":{"id":"Cnpx-DvDcrIe"},"source":["#### Tying weights"]},{"cell_type":"markdown","metadata":{"id":"RqO2hUXfcrIe"},"source":["When an autoencoder is neatly symmetrical, a common technique is to tie the weights of the decoder layers to the weights of the encoder layers. This halves the number of weights in the model, speeding up training and limiting the risk of overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9i6LnLlOnqyq"},"outputs":[],"source":["# Load fasion mnist dataset\n","(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n","\n","# Scale dataset\n","X_train_full = X_train_full.astype(np.float32) / 255\n","X_test = X_test.astype(np.float32) / 255\n","\n","# Training and validation set\n","X_train = X_train_full[:-5000]\n","X_valid = X_train_full[-5000:]\n","y_train = y_train_full[:-5000]\n","y_valid = y_train_full[-5000:]"]},{"cell_type":"markdown","metadata":{"id":"Phv21qHYfe0f"},"source":["To tie weights between layers using Keras, let’s define a custom layer:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o50AyJyRcrIe"},"outputs":[],"source":["class DenseTranspose(keras.layers.Layer):\n","    def __init__(self, dense, activation=None, **kwargs):\n","        self.dense = dense\n","        self.activation = keras.activations.get(activation)\n","        super().__init__(**kwargs)\n","    def build(self, batch_input_shape):\n","        # Access input_shape from dense layer's weights\n","        self.biases = self.add_weight(name=\"bias\",\n","                                      shape=[self.dense.weights[0].shape[0]], # get the input shape from weights\n","                                      initializer=\"zeros\")\n","        super().build(batch_input_shape)\n","    def call(self, inputs):\n","        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n","        return self.activation(z + self.biases)"]},{"cell_type":"markdown","metadata":{"id":"m28jpnn5crIe"},"source":["This custom layer acts like a regular Dense layer, but it uses another Dense layer’s weights, transposed. However, it uses its own bias vector. Next, we can build a new stacked autoencoder, with the decoder’s Dense layers tied to the encoder’s Dense layers:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgwDqaRhcrIf"},"outputs":[],"source":["# Create tied stacked autoencoder\n","dense_1 = Dense(100, activation=\"selu\")\n","dense_2 = Dense(30, activation=\"selu\")\n","\n","tied_encoder = Sequential([\n","                           Flatten(input_shape=[28, 28]),\n","                           dense_1,\n","                           dense_2\n","                           ])\n","\n","tied_decoder = Sequential([\n","                           DenseTranspose(dense_2, activation=\"selu\"),\n","                           DenseTranspose(dense_1, activation=\"sigmoid\"),\n","                           Reshape([28, 28])\n","                           ])\n","\n","tied_ae = Sequential([tied_encoder, tied_decoder])\n","\n","# Compile model\n","def rounded_accuracy(y_true, y_pred):\n","    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n","\n","tied_ae.compile(loss = \"binary_crossentropy\", optimizer = keras.optimizers.SGD(learning_rate=1.5), metrics = [rounded_accuracy])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84rRx3hucrIf"},"outputs":[],"source":["# Train model on training set\n","history = tied_ae.fit(X_train, X_train, epochs=10, validation_data=(X_valid, X_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"voaEr9ndcrIf"},"outputs":[],"source":["# Visualize reconstructions\n","\n","def show_reconstructions(model, images=X_valid, n_images=5):\n","    ''' Compare inputs and outputs of model using n_images from X_valid dataset '''\n","\n","    reconstructions = model.predict(images[:n_images])\n","\n","    fig = plt.figure(figsize=(n_images * 1.5, 3))\n","    for img_idx in range(n_images):\n","        plt.subplot(2, n_images, 1 + img_idx)\n","        plt.imshow(images[img_idx], cmap='binary')\n","        plt.axis(\"off\")\n","\n","        plt.subplot(2, n_images, 1 + n_images + img_idx)\n","        plt.imshow(reconstructions[img_idx], cmap='binary')\n","        plt.axis(\"off\")\n","\n","show_reconstructions(model = tied_ae)"]},{"cell_type":"markdown","metadata":{"id":"PWLQj1z_crIf"},"source":["From the above results, we can see that this model achieves a very slightly lower reconstruction error than the previous model, with almost half the number of parameters."]},{"cell_type":"markdown","metadata":{"id":"_LKPZpcacrIg"},"source":["#### Training One Autoencoder at a Time"]},{"cell_type":"markdown","metadata":{"id":"mIuLCUvgcrIh"},"source":["Rather than training the whole stacked autoencoder in one go, it is possible to train one shallow autoencoder at a time, then stack all of them into a single stacked autoencoder, as shown in the figure below. This technique is known as **greedy layerwise training**.\n","\n","<br><br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/training%20one%20auto-encoder.png\" width=500px/>\n","</center>\n","\n","$\\hspace{9.4cm} \\text{Training one autoencoder at a time}$\n","<br><br>\n","\n","Steps including in this method are:\n","\n","* During the first phase of training, the first autoencoder learns to reconstruct the inputs.\n","\n","* Then we encode the whole training set using this first autoencoder, and this gives us a new (compressed) training set.\n","\n","* We then train a second autoencoder on this new dataset. This is the second phase of training.\n","\n","* Finally, we first stack the hidden layers of each autoencoder, then the output layers in reverse order. This gives us the final stacked autoencoder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Avdcv1IacrIh"},"outputs":[],"source":["def train_autoencoder(n_neurons, X_train, X_valid, loss, optimizer, n_epochs=10, output_activation=None, metrics=None):\n","    ''' Return encoder and decoder submodels trained on X_train with specified parameters'''\n","\n","    n_inputs = X_train.shape[-1]\n","    encoder = Sequential([Dense(n_neurons, activation=\"selu\", input_shape=[n_inputs])])\n","    decoder = Sequential([Dense(n_inputs, activation=output_activation)])\n","\n","    autoencoder = Sequential([encoder, decoder])\n","    autoencoder.compile(optimizer, loss, metrics=metrics)\n","    autoencoder.fit(X_train, X_train, epochs=n_epochs, validation_data=(X_valid, X_valid))\n","\n","    return encoder, decoder, encoder(X_train), encoder(X_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHIpJeu2crIh"},"outputs":[],"source":["# Reshape training and validation set\n","X_train_flat = X_train.reshape(-1, 28*28)\n","X_valid_flat = X_valid.reshape(-1, 28*28)\n","\n","# First phase of training\n","enc1, dec1, X_train_enc1, X_valid_enc1 = train_autoencoder(100, X_train_flat, X_valid_flat, loss=\"binary_crossentropy\",\n","                                                           optimizer=keras.optimizers.SGD(learning_rate=1.5),\n","                                                           output_activation=\"sigmoid\", metrics=[rounded_accuracy])\n","# Second phase of training\n","enc2, dec2, _, _ = train_autoencoder(30, X_train_enc1, X_valid_enc1, \"mse\",\n","                                     keras.optimizers.SGD(learning_rate=0.05), output_activation=\"selu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjvM-k_4crIh"},"outputs":[],"source":["# Final stacked autoencoder\n","stacked_ae_1_by_1 = Sequential([\n","                                Flatten(input_shape=[28, 28]),\n","                                enc1, enc2, dec2, dec1,\n","                                Reshape([28, 28])\n","                                ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSpNg8a4crIh"},"outputs":[],"source":["# Visualize reconstructions\n","show_reconstructions(model = stacked_ae_1_by_1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYJ_y3o7crIi"},"outputs":[],"source":["# Training final stacked autoencoder\n","stacked_ae_1_by_1.compile(loss = \"binary_crossentropy\",\n","                          optimizer = keras.optimizers.SGD(learning_rate=0.1), metrics = [rounded_accuracy])\n","\n","history = stacked_ae_1_by_1.fit(X_train, X_train, epochs=10, validation_data=(X_valid, X_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjqTFQ6bcrIi"},"outputs":[],"source":["# Visualize reconstructions\n","show_reconstructions(model = stacked_ae_1_by_1)"]},{"cell_type":"markdown","metadata":{"id":"ltn94tSQcrIi"},"source":["Autoencoders are not limited to dense networks: we can also build convolutional\n","autoencoders."]},{"cell_type":"markdown","metadata":{"id":"jVn8xV2FcrIi"},"source":["### Convolutional Autoencoders"]},{"cell_type":"markdown","metadata":{"id":"WcC35Cm0crIi"},"source":["When we are dealing with images, the autoencoders we have seen so far will not work well: we will need to build a **convolutional autoencoder**.\n","\n","For convolutional autoencoder:\n","\n","* The **encoder** is a regular CNN composed of convolutional layers and pooling layers. It typically reduces the spatial dimensionality of the inputs (i.e., height and width) while increasing the depth (i.e., the number of feature maps).\n","\n","* The **decoder** must do the reverse (upscale the image and reduce its depth back to the original dimensions), and for this we can use transpose convolutional layers (alternatively, we could combine upsampling layers with convolutional layers).\n","\n","Let's see a simple convolutional autoencoder for Fashion MNIST:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccDPdBfscrIi"},"outputs":[],"source":["# Create convolutional encoder\n","conv_encoder = Sequential([\n","                           Reshape([28, 28, 1], input_shape=[28, 28]),\n","                           Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n","                           MaxPool2D(pool_size=2),\n","                           Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n","                           MaxPool2D(pool_size=2),\n","                           Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n","                           MaxPool2D(pool_size=2)\n","                           ])\n","\n","# Create convolutional decoder\n","conv_decoder = Sequential([\n","                           Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"VALID\", activation=\"selu\", input_shape=[3, 3, 64]),\n","                           Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\"),\n","                           Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\"),\n","                           Reshape([28, 28])\n","                           ])\n","\n","conv_ae = Sequential([conv_encoder, conv_decoder])\n","\n","# Compile model\n","conv_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0), metrics=[rounded_accuracy])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEbgopUqcrIj"},"outputs":[],"source":["# Model summary for convolutional encoder and decoder\n","conv_encoder.summary()\n","conv_decoder.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZun-AlQcrIj"},"outputs":[],"source":["# Train convolutional encoder on training set\n","history = conv_ae.fit(X_train, X_train, epochs=5, validation_data=(X_valid, X_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4laIZiOqcrIj"},"outputs":[],"source":["# Visualize reconstructions\n","show_reconstructions(conv_ae)"]},{"cell_type":"markdown","metadata":{"id":"YZwMsdevnqyl"},"source":["### Variational Autoencoders"]},{"cell_type":"markdown","metadata":{"id":"Qjcn_o7fnqym"},"source":["Another important category of autoencoders was introduced in 2013 by Diederik\n","Kingma and Max Welling and quickly became one of the most popular types of\n","autoencoders: variational autoencoders.\n","\n","They are quite different from all other autoencoders, in these particular ways:\n","\n","* They are probabilistic autoencoders, i.e, that their outputs are partly determined by chance, even after training.\n","\n","* They are generative autoencoders, i.e, that they can generate new instances that look like they were sampled from the training set.\n","\n","**Working of Variational autoencoders:**\n","\n","The below figure (left) shows a variational autoencoder. The basic structure is similar to all autoencoders, with an encoder followed by a decoder, but instead of directly producing a coding for a given input, the encoder produces a **mean coding μ** and a **standard deviation σ**. The actual coding is then sampled randomly from a Gaussian distribution with mean $μ$ and standard deviation $σ$. After that the decoder decodes the sampled coding normally.\n","\n","The right part of the diagram shows a training instance going through this autoencoder. First, the encoder produces $μ$ and $σ$, then a coding is sampled randomly, and finally this coding is decoded; the final output resembles the training instance."]},{"cell_type":"markdown","metadata":{"id":"M0oawbZnnqyn"},"source":["<br><br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Variational_AE.png\" width=500px/>\n","</center>\n","\n","$\\hspace{6cm} \\text{Variational autoencoder (left) and an instance going through it (right)}$\n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"x7QDxxfMnqyn"},"source":["After training a variational autoencoder, we can easily generate a new instance: just sample a random coding from the Gaussian distribution and decode it.\n","\n","The cost function is composed of two parts.\n","\n","* The first is the usual reconstruction loss that pushes the autoencoder to reproduce its inputs.\n","\n","* The second is the latent loss that pushes the autoencoder to have codings that look as though they were sampled from a simple Gaussian distribution.\n","\n","The latent loss can be computed using Equation:\n","\n","Variational autoencoder’s latent loss,\n","\n","$$L = -\\frac{1}{2}\\Sigma_{i=1}^{n}[1 + log(\\sigma_i^2) - \\sigma_i^2 - \\mu_i^2]$$\n","\n","where, $L$ is the latent loss, $n$ is the codings’ dimensionality, and $μ_i$ and $σ_i$ are the mean and standard deviation of the $i^{th}$ component of the codings. The vectors $μ$ and $σ$ are output by the encoder, as shown in\n","the above figure (left).\n","\n","A common tweak to the variational autoencoder’s architecture is to make the encoder output $γ = log(σ^2)$ rather than $σ$. The latent loss can then be computed as:\n","\n","$$L = -\\frac{1}{2}\\Sigma_{i=1}^{n}[1 + \\gamma_i - exp(\\gamma_i) - \\mu_i^2]$$\n","\n","This approach is more numerically stable and speeds up training.\n","\n","Let’s start building a variational autoencoder for Fashion MNIST using the $γ$ tweak."]},{"cell_type":"markdown","metadata":{"id":"3Z7bdZFknqyq"},"source":["First, we will need a custom layer to sample the codings, given $μ$ and $γ$:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XtiuCDOqnqyr"},"outputs":[],"source":["# Custom Sampling Layer\n","class Sampling(tf.keras.layers.Layer):\n","    def call(self, inputs):\n","        mean, log_var = inputs\n","        epsilon = tf.random.normal(shape=tf.shape(log_var))\n","        return mean + epsilon * tf.exp(0.5 * log_var)"]},{"cell_type":"markdown","metadata":{"id":"ihlcws-nnqyr"},"source":["This Sampling layer takes two inputs: mean ($μ$) and log_var ($γ$). It uses the function `tf.random.normal()` to sample a random vector (of the same shape as γ) from the Normal distribution, with mean 0 and standard deviation 1. Then it multiplies it by $exp(γ / 2)$ (which is equal to $σ$), and finally it adds $μ$ and returns the result. This samples a codings vector from the Normal distribution with mean $μ$ and standard deviation $σ$.\n","\n","Next, we can create the encoder, using the Functional API:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzr2vdEDnqys"},"outputs":[],"source":["codings_size = 10\n","# Variational Encoder\n","inputs = Input(shape=[28, 28])\n","z = Flatten()(inputs)\n","z = Dense(150, activation=\"selu\")(z)\n","z = Dense(100, activation=\"selu\")(z)"]},{"cell_type":"code","source":["# Custom Latent Loss Layer\n","class LatentLossLayer(Layer):\n","    def call(self, inputs):\n","        codings_mean, codings_log_var = inputs\n","        latent_loss = -0.5 * K.sum(\n","            1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),\n","            axis=-1\n","        )\n","        self.add_loss(K.mean(latent_loss) / 784.0)  # Add to the model's loss\n","        return codings_mean  # Forward pass"],"metadata":{"id":"ih1kdYt58Stu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Variational Encoder with Latent Loss\n","codings_mean = Dense(codings_size)(z) # without Latent Loss\n","codings_log_var = Dense(codings_size)(z)\n","codings_mean = LatentLossLayer()([codings_mean, codings_log_var])  # Attach Latent Loss\n","codings = Sampling()([codings_mean, codings_log_var])\n","\n","variational_encoder = Model(inputs, [codings_mean, codings_log_var, codings])"],"metadata":{"id":"RLfSa5Tx8dcH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mXhMYrg8nqyt"},"source":["Note that the Dense layers that output codings_mean ($μ$) and codings_log_var ($γ$) have the same inputs. We then pass both codings_mean and codings_log_var to the Sampling layer. Finally, the variational_encoder model has three outputs. The only output we will use is the last one (codings).\n","\n","Now let’s build the decoder:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtLDGkyHnqyt"},"outputs":[],"source":["# Variational Decoder\n","decoder_inputs = Input(shape=[codings_size])\n","x = Dense(100, activation=\"selu\")(decoder_inputs)\n","x = Dense(150, activation=\"selu\")(x)\n","x = Dense(28 * 28, activation=\"sigmoid\")(x)\n","outputs = tf.keras.layers.Reshape([28, 28])(x)\n","\n","variational_decoder = Model(decoder_inputs, outputs)"]},{"cell_type":"markdown","metadata":{"id":"DHMiiKGRnqyu"},"source":["Finally, let’s build the variational autoencoder model.\n","\n","Also, we must add the latent loss and the reconstruction loss. First apply the latent loss Equation for each instance in the batch. Then we compute the mean loss over all the instances in the batch, and scale it by divide the result by 784 to ensure it has the appropriate scale compared to the reconstruction loss."]},{"cell_type":"code","source":["# Create variational autoencoder using Functional API\n","_, _, codings = variational_encoder(inputs)\n","reconstructions = variational_decoder(codings)\n","variational_ae = Model(inputs=[inputs], outputs=[reconstructions])"],"metadata":{"id":"IfK87Ut85GEv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile model\n","variational_ae.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[rounded_accuracy])"],"metadata":{"id":"zmyL0UQl5JiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVhNxt7knqyu"},"outputs":[],"source":["# Train variational autoencoder on training set\n","history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128, validation_data=(X_valid, X_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rjYEYTbnqyv"},"outputs":[],"source":["# Visualize reconstructions\n","show_reconstructions(variational_ae)"]},{"cell_type":"markdown","metadata":{"id":"fbxteWG9nqyw"},"source":["#### Generating Fashion MNIST Images using Variational autoencoder"]},{"cell_type":"markdown","metadata":{"id":"2OVU9f__nqyw"},"source":["We can use the variational autoencoder to generate images that look like fashion items by sampling random codings from a Gaussian distribution and decode them:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIL_7Yswnqyx"},"outputs":[],"source":["def plot_multiple_images(images, n_cols=None):\n","    ''' Plot multiple images '''\n","\n","    n_cols = n_cols or len(images)\n","    n_rows = (len(images) - 1) // n_cols + 1\n","    plt.figure(figsize=(n_cols, n_rows))\n","    for index, image in enumerate(images):\n","        plt.subplot(n_rows, n_cols, index + 1)\n","        plt.imshow(image, cmap=\"binary\")\n","        plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTOLTZsvnqyx"},"outputs":[],"source":["# Generate a few random codings, decode them and plot the resulting images\n","codings = tf.random.normal(shape = [12, codings_size])\n","images = variational_decoder(codings).numpy()\n","plot_multiple_images(images, 4)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}