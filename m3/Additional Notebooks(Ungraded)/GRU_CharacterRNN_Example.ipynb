{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DX4A5pOv6u67"},"source":["In this example, we are going to generating Shakespearean text using a Character RNN. We will build and train an RNN to predict the next\n","character in a sentence."]},{"cell_type":"markdown","metadata":{"id":"Tbfz1vfs6HVC"},"source":["#### Importing Libraries"]},{"cell_type":"code","metadata":{"id":"b8NAJmt0IUY1","executionInfo":{"status":"ok","timestamp":1740638182496,"user_tz":-330,"elapsed":9690,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import os"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kw-k3p4S6N7E"},"source":["#### Getting Data"]},{"cell_type":"code","metadata":{"id":"xAxkolbWIpWO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740638183013,"user_tz":-330,"elapsed":513,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}},"outputId":"2fbf54e8-50cc-49e6-824f-c669cd75103e"},"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]}]},{"cell_type":"markdown","metadata":{"id":"X339D4Cq7fUu"},"source":["#### Printing few characters and getting length of corpus"]},{"cell_type":"code","metadata":{"id":"LExGya1rIxEY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740638183035,"user_tz":-330,"elapsed":26,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}},"outputId":"fb3321f2-54d3-4ec7-a323-54e4596dc928"},"source":["text = open(path_to_file, 'rb').read()\n","text = text.decode(encoding='utf-8')\n","print ('Total number of characters in the corpus is:', len(text))\n","print('The first 100 characters of the corpus are as follows:\\n', text[:100])"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of characters in the corpus is: 1115394\n","The first 100 characters of the corpus are as follows:\n"," First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n"]}]},{"cell_type":"markdown","metadata":{"id":"0sSUhyqz70Wx"},"source":["#### Seeing the all unique  characters"]},{"cell_type":"code","metadata":{"id":"xN1FHQmLJMBd","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1740638183079,"user_tz":-330,"elapsed":35,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}},"outputId":"da67f453-ecd8-4e0b-8df2-5bda3fdcb92f"},"source":["\"\".join(sorted(set(text.lower())))"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"kYB_uRpb8Ae8"},"source":["#### Tokenization\n","In this step, we encode every character as an integer. We set char_level=True to get character-level encoding rather than the default word-level encoding. Tokenizer converts the text to lowercase by default."]},{"cell_type":"code","metadata":{"id":"L85MH_loJTOK","executionInfo":{"status":"ok","timestamp":1740638185880,"user_tz":-330,"elapsed":2783,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}}},"source":["tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","tokenizer.fit_on_texts(text)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvTHwImz88-O"},"source":["Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells us how many distinct characters there are and the total number of characters in the text."]},{"cell_type":"code","metadata":{"id":"ghGohMVzJjvv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740638185897,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}},"outputId":"d26c30f1-3c13-413c-9a83-97448e14da2a"},"source":["tokenizer.texts_to_sequences([\"First\"])"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[20, 6, 9, 8, 3]]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"RAgPY8XXJoCp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740638186015,"user_tz":-330,"elapsed":103,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}},"outputId":"b59de246-7a25-4746-e549-449dd37e1df0"},"source":["tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['f i r s t']"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"UKCzD97AJsxq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740638186047,"user_tz":-330,"elapsed":28,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}},"outputId":"1a6b00aa-5286-4680-b6bd-c6a0316cd43f"},"source":["max_id = len(tokenizer.word_index) # number of distinct characters\n","dataset_size = tokenizer.document_count # total number of characters\n","print(\"number of distinct characters:\", max_id,'\\n',\"total number of characters:\",dataset_size)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["number of distinct characters: 39 \n"," total number of characters: 1115394\n"]}]},{"cell_type":"markdown","metadata":{"id":"PkpDGZp_9rH5"},"source":["Let’s encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39):"]},{"cell_type":"code","metadata":{"id":"ejprUQBYJw51","executionInfo":{"status":"ok","timestamp":1740638186565,"user_tz":-330,"elapsed":516,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}}},"source":["[encoded] = np.array(tokenizer.texts_to_sequences([text])) - 1\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zv4vGWyS9-ZN"},"source":["#### Splitting the Sequential Dataset\n","Avoid any overlap between the training set, the validation set, and the test set. For example, we can take the first 90% of the text for the training set, then the next 5% for the validation set, and the final 5% for the test set.\n","\n","Creating a tf.data.Dataset that will return each character one by one from this set."]},{"cell_type":"code","metadata":{"id":"eCTyyvKi9-u2","executionInfo":{"status":"ok","timestamp":1740638188320,"user_tz":-330,"elapsed":1751,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}}},"source":["train_size = dataset_size * 90 // 100\n","dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMHntGw6-jkl"},"source":["#### Chopping the Sequential Dataset into Multiple Windows\n","\n","By default, the window() method creates non-overlapping windows, but to get the largest possible training set we use shift=1 so that the first window contains characters 0 to 100, the second contains characters 1 to 101, and so on. To ensure that all windows are exactly 101 characters long (which will allow us to create batches\n","without having to do any padding), we set drop_remainder=True (otherwise the last 100 windows will contain 100 characters, 99 characters, and so on down to 1 character).\n","\n","The window() method creates a dataset that contains windows, each of which is also represented as a dataset. It’s a nested dataset, analogous to a list of lists. This is useful when you want to transform each window by calling its dataset methods (e.g., toshuffle them or batch them)."]},{"cell_type":"code","metadata":{"id":"c9U7ezoiJ3rp","executionInfo":{"status":"ok","timestamp":1740638188371,"user_tz":-330,"elapsed":49,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}}},"source":["n_steps = 100\n","window_length = n_steps + 1 # target = input shifted 1 character ahead\n","dataset = dataset.window(window_length, shift=1, drop_remainder=True)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_BvHNwG_ml7"},"source":["We cannot use a nested dataset directly for training, as our model will expect tensors as input, not datasets. So, we must call the\n","flat_map() method: it converts a nested dataset into a flat dataset (one that does not contain datasets). For example, suppose {1, 2, 3} represents a dataset containing the sequence of tensors 1, 2, and 3. If you flatten the nested dataset {{1, 2}, {3, 4, 5, 6}}, you get back the flat dataset {1, 2, 3, 4, 5, 6}. Moreover, the flat_map() method takes a function as an argument, which allows you to transform each dataset in a nested dataset before flattening. For example, if you pass the function lambda ds: ds.batch(2) to flat_map(), then it will transform the nested dataset {{1, 2}, {3, 4, 5, 6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it’s a dataset of tensors of size 2. With that in mind, we are ready to flatten our dataset."]},{"cell_type":"code","metadata":{"id":"0AE5ThshJ8If","executionInfo":{"status":"ok","timestamp":1740638188542,"user_tz":-330,"elapsed":164,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}}},"source":["dataset = dataset.flat_map(lambda window: window.batch(window_length))"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5nRH4Qs_8HG"},"source":["In generating Shakespearean text using a Character RNN, when the instances in the training set are independent and identically distributed, we need to shuffle the windows. Then we can batch the windows and separate the inputs (the first 100 characters) from the target (the last character)."]},{"cell_type":"code","metadata":{"id":"O_JyV21kKAjl","executionInfo":{"status":"ok","timestamp":1740638192170,"user_tz":-330,"elapsed":3624,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}}},"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","batch_size = 32\n","dataset = dataset.shuffle(10000).batch(batch_size)\n","dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TaecL_0LDVgW"},"source":["The figure below summarizes the dataset preparation steps discussed so far (showing windows of length 11 rather than 101, and a batch size of 3 instead of 32)."]},{"cell_type":"markdown","metadata":{"id":"SyHlx1NqQHpB"},"source":["![Img](https://cdn.iisc.talentsprint.com/CDS/Images/Prep_shuffled_windows.PNG)\n","\n","$\\text{Figure :  Preparing a dataset of shuffled windows\n","}$ [Image Source : Ref.1]"]},{"cell_type":"markdown","metadata":{"id":"AEqt9woFALrb"},"source":["The categorical input features should generally be encoded, usually as one-hot vectors or as embeddings. Here, we will encode each character using a one-hot vector because there are fairly few distinct characters (only 39):\n"]},{"cell_type":"code","metadata":{"id":"saYcr1U8KQSd","executionInfo":{"status":"ok","timestamp":1740638192222,"user_tz":-330,"elapsed":50,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}}},"source":["dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iE5GvSa3Aagd"},"source":["Finally, we just need to add prefetching:\n"]},{"cell_type":"code","metadata":{"id":"NrEk1YprKXcf","executionInfo":{"status":"ok","timestamp":1740638192238,"user_tz":-330,"elapsed":14,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}}},"source":["dataset = dataset.prefetch(1)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8-4r8GcKbH_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740638194796,"user_tz":-330,"elapsed":2556,"user":{"displayName":"Aarnav Singh","userId":"12976078607062448521"}},"outputId":"ad0c28b4-08d1-43bc-b19f-6946e8087ffb"},"source":["for X_batch, Y_batch in dataset.take(1):\n","    print(X_batch.shape, Y_batch.shape)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 100, 39) (32, 100)\n"]}]},{"cell_type":"markdown","metadata":{"id":"inZNTGgLKjVp"},"source":["#### Creating and Training the Model # Run on GPU It may take 1 to 2 hour\n","\n","The GRU class will only use the GPU, when using the default values for the following arguments: activation, recurrent_activation, recurrent_dropout, unroll, use_bias and reset_after."]},{"cell_type":"code","source":["model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation=\"softmax\"))\n","])\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","history = model.fit(dataset, epochs=10)"],"metadata":{"id":"_YDKwr30p3EO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"74609247-a149-4ac4-db01-2cb3d091fe02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","   1061/Unknown \u001b[1m89s\u001b[0m 76ms/step - loss: 2.4727"]}]},{"cell_type":"markdown","metadata":{"id":"6p0mvOUpLbuA"},"source":["#### Using the Model to Generate Text\n","Now we have a model that can predict the next character in text written by Shake‐\n","speare. To feed it some text, we first need to preprocess it like we did earlier, so let’s\n","create a little function for this:\n"]},{"cell_type":"code","metadata":{"id":"pq4jvkuALU2q"},"source":["def preprocess(texts):\n","    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n","    return tf.one_hot(X, max_id)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hEW_YFRvBi__"},"source":["Now let’s use the model to predict the next letter in some text:\n"]},{"cell_type":"code","metadata":{"id":"1FGCWA69Loj2"},"source":["X_new = preprocess([\"How are yo\"])\n","Y_pred = np.argmax(model(X_new), axis=-1)\n","tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BpaJRyELBmDv"},"source":["The model guessed right. Now let’s use this model to generate new text.\n"]},{"cell_type":"markdown","metadata":{"id":"dC2Wt8kbBvBz"},"source":["#### Generating Fake Shakespearean Text\n","\n","To generate new text using the Char-RNN model, we could feed it some text, make\n","the model predict the most likely next letter, add it at the end of the text, then give the\n","extended text to the model to guess the next letter, and so on. But in practice this\n","often leads to the same words being repeated over and over again.\n","\n","Instead, we can\n","pick the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s `tf.random.categorical()` function. This will generate more\n","diverse and interesting text. The categorical() function samples random class indices, given the class log probabilities (logits). To have more control over the diversity of the generated text, we can divide the logits by a number called the temperature, which we can tweak as we wish: a temperature close to 0 will favor the high\u0002probability characters, while a very high temperature will give all characters an equal probability. The following `next_char()` function uses this approach to pick the next character to add to the input text:\n"]},{"cell_type":"code","metadata":{"id":"O_kaSXsmL061"},"source":["tf.random.set_seed(42)\n","tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQzMHAssL5di"},"source":["def next_char(text, temperature=1):\n","    X_new = preprocess([text])\n","    y_proba = model(X_new)[0, -1:, :]\n","    rescaled_logits = tf.math.log(y_proba) / temperature\n","    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n","    return tokenizer.sequences_to_texts(char_id.numpy())[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kE3swdJbL88h"},"source":["tf.random.set_seed(42)\n","\n","next_char(\"How are yo\", temperature=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0i7UosnMAl3"},"source":["def complete_text(text, n_chars=50, temperature=1):\n","    for _ in range(n_chars):\n","        text += next_char(text, temperature)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2TaOH4LMEZx"},"source":["tf.random.set_seed(42)\n","\n","print(complete_text(\"t\", temperature=0.2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBzIvrTxMH7s"},"source":["print(complete_text(\"t\", temperature=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b8OzzyGhMLEi"},"source":["print(complete_text(\"t\", temperature=2))"],"execution_count":null,"outputs":[]}]}