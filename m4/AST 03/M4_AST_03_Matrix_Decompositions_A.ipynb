{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Vf7UdufgUVRm"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 03:  Matrix Decompositions"]},{"cell_type":"markdown","metadata":{"id":"cs0B3ScgUVRv"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"Sq5piVQRUVRw"},"source":["At the end of the experiment, you will be able to\n","\n","* know what is matrix decomposition\n","* perform different types of matrix decompositions\n","* see the pythonic implementation of use-cases in SVD and PCA"]},{"cell_type":"code","source":["#@title Walkthrough Video\n","from IPython.display import HTML\n","HTML(\"\"\"<video width=\"420\" height=\"240\" controls>\n","<source src=\"https://cdn-exec.ap-south-1.linodeobjects.com/content/Matrix_Decomposition_v1_Debasish_Bhaskar.mp4\">\n","</video>\"\"\")"],"metadata":{"id":"094vfY0F3nQb","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LmQjymFwUVRy"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"t90A_1c5GIhY"},"source":["A **matrix decomposition** or **matrix factorization** is a factorization of a matrix into a product of matrices. It is an approach that simplifies more complex matrix operations by using a decomposed matrix rather than on the original matrix itself.\n","\n","There are many ways to decompose a matrix, hence there is a range of different matrix decomposition techniques.\n","\n","Decompositions related to solving systems of linear equations\n","* QR Factorization\n","* Cholesky Decomposition\n","\n","Decompositions based on eigenvalues and related concepts\n","* Eigenvalue Decomposition\n","* Singular Value Decomposition"]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"M4_AST_03_Matrix_Decompositions_A\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Images/houses.jpg\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://learn-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBVY6izAad2r"},"source":["### Import Required Packages"]},{"cell_type":"code","metadata":{"id":"HbXyhHpgac3U"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from PIL import Image              # to import images in notebook\n","import sympy as sy\n","from pprint import pprint\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zcM8E7zUr0PL"},"source":["### QR Factorization"]},{"cell_type":"markdown","metadata":{"id":"BCsexwOCr3Vg"},"source":["The QR decomposition (or QR factorization) of a matrix is a decomposition of the matrix into an orthogonal matrix and a triangular matrix. A QR decomposition of a real square matrix $A$ is a decomposition of $A$ as\n","$$A = QR$$\n","where $Q$ is an orthogonal matrix (i.e. $Q^TQ = I$) and $R$ is an upper triangular matrix.\n","<center>\n","<img src=\"http://www.sharetechnote.com/image/EngMath_Matrix_QRDecomposition_02.png\" width= 400 px/>\n","</center>\n","\n","There are several methods for actually computing the QR decomposition. One such method is the Gram-Schmidt process."]},{"cell_type":"markdown","metadata":{"id":"hTgY93E2zH22"},"source":["#### Computing the QR factorization with Gram–Schmidt process"]},{"cell_type":"markdown","metadata":{"id":"5wm3Rsl3P7OK"},"source":["The Gram-Schmidt process works by finding an orthogonal projection $q_n$ for each column vector $a_n$ and then subtracting its projections onto the previous projections $(q_j)$. The resulting vector is then divided by the length of that vector to produce a unit vector.\n","\n","For example, let's find the QR-factorization of matrix A given as\n","$$A = \\begin{bmatrix} 1 & 1 & 0 \\\\ -1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}.$$\n","\n","Denote the columns of $A$ as $c_1$, $c_2$, and $c_3$.\n","\n","If we apply the Gram-Schmidt algorithm to these columns, the result is:\n","$$f_1 = c1 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\ \\ f_2 = c_2 - \\frac{1}{2}f_1 = \\begin{bmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 1 \\\\ 0 \\end{bmatrix},\\ \\ and\\ \\ f_3 = c_3 + \\frac{1}{2}f_1 - f_2 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.$$\n","\n","Write $q_j = \\frac{1}{||f_j||^2}f_j$ for each j, so {$q_1, q_2, q_3$} is orthonormal.\n","\n","Then A can be written as A = QR where\n","\n","$$Q = \\begin{bmatrix} q_1 & q_2 & q_3 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} & 0 \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} & 0 \\\\ 0 & \\frac{2}{\\sqrt{6}} & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\frac{1}{\\sqrt{6}} \\begin{bmatrix} \\sqrt{3} & 1 & 0 \\\\ -\\sqrt{3} & 1 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & \\sqrt{6} \\end{bmatrix}$$\n","\n","$$R = \\begin{bmatrix} ||f_1|| & c_2.q_1 & c_3.q_1 \\\\ 0 & ||f_2|| & c_3.q_2 \\\\ 0 & 0& ||f_3|| \\end{bmatrix} = \\begin{bmatrix} \\sqrt{2} & \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ 0 & \\frac{\\sqrt{3}}{\\sqrt{2}} & \\frac{\\sqrt{3}}{\\sqrt{2}} \\\\ 0 & 0 & 1 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 2 & 1 & -1 \\\\ 0 & \\sqrt{3} & \\sqrt{3} \\\\ 0 & 0 & \\sqrt{2} \\end{bmatrix}$$\n","\n","We can verify that $A = QR.$"]},{"cell_type":"markdown","metadata":{"id":"TrVONyo1pIU5"},"source":["Let us now implement QR factorization using a Python library"]},{"cell_type":"markdown","metadata":{"id":"EwmbPAaaMoUY"},"source":["**Exercise 1:** Find a QR factorization of\n","\n","$$\\mathbf{A} = \\begin{bmatrix} 2 & -2 & 18 \\\\ 2 & 1 & 0 \\\\ 1 & 2 & 0 \\end{bmatrix}.$$"]},{"cell_type":"code","metadata":{"id":"IadmJW49_0at"},"source":["A = sy.Matrix([[2, -2, 18], [2, 1, 0], [1, 2, 0]])\n","A"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgzgjD4OXWL3"},"source":["Q, R = A.QRdecomposition()\n","print(\"A:\")\n","pprint(A)\n","\n","# YOUR CODE HERE to display 'Q'\n","\n","# YOUR CODE HERE to display 'R'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kzlt-E0wkKUu"},"source":["# QR = A\n","Q*R == A"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JQ88ab57cuqC"},"source":["### Cholesky Decomposition"]},{"cell_type":"markdown","metadata":{"id":"K9skXMPCcuqC"},"source":["In linear algebra, the Cholesky decomposition or Cholesky factorization is a decomposition of a symmetric, positive-definite matrix into the product of a lower triangular matrix and its transpose, which is useful for efficient numerical solutions.\n","\n","The Cholesky decomposition of a symmetric positive-definite matrix A, is a decomposition of the form\n","$$A = LL^T$$\n","\n","where $L$ is a real lower triangular matrix with positive diagonal entries and $L^T$ denotes the transpose of $L$."]},{"cell_type":"markdown","metadata":{"id":"PNhf7yj8cuqD"},"source":["Consider a 3x3 symmetric matrix $A$ and its Cholesky decomposition\n","\n","$$A = LL^T$$\n","\n","$$\\begin{bmatrix} a_{11} & a_{21} & a_{31} \\\\ a_{21} & a_{22} & a_{32} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} =  \\begin{bmatrix} l_{11} & 0 & 0 \\\\ l_{21} & l_{22} & 0 \\\\ l_{31} & l_{32} & l_{33} \\end{bmatrix} \\begin{bmatrix} l_{11} & l_{21} & l_{31} \\\\ 0 & l_{22} & l_{32} \\\\ 0 & 0 & l_{33} \\end{bmatrix}$$\n","\n","\n","In order to solve for the lower triangular matrix, we use the Cholesky-Banachiewicz Algorithm. First, we calculate the values for $L$ on the main diagonal and then the off-diagonals for the elements below the diagonal:\n","$$l_{kk} = \\sqrt{a_{kk} - \\sum^{k-1}_{j=1} l^2_{kj}}$$\n","$$l_{ik} = \\frac{1}{l_{kk}} \\left( a_{ik} - \\sum^{k-1}_{j=1} l_{ij} l_{kj} \\right), i > k $$\n","\n","To know more about Cholesky decomposition click [here](https://cdn.exec.talentsprint.com/static/cds/content/Cholesky_decompositions.pdf)."]},{"cell_type":"markdown","metadata":{"id":"KTPzN1mscuqE"},"source":["**Exercise 2:** Find the Cholesky decomposition of a symmetric matrix\n","\n","$$\\mathbf{A} = \\begin{bmatrix} 25 & 15 & -5 \\\\ 15 & 18 & 0 \\\\ -5 & 0 & 11 \\end{bmatrix}.$$"]},{"cell_type":"code","metadata":{"id":"VeUkOIYecuqE"},"source":["A = sy.Matrix([[25, 15, -5], [15, 18, 0], [-5, 0, 11]])\n","A"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JuhlIqBkcuqF"},"source":["# Lower triangular matrix\n","L = A.cholesky()\n","# YOUR CODE HERE to dispaly 'L'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cctDeabcuqF"},"source":["# L.L_transpose = A\n","L*L.transpose() == A"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l-yJ0TouHY4p"},"source":["Before going through Eigenvalue decomposition we first need to understand two terms related to eigenvalues:\n","\n","**Algebraic multiplicity** of an eigenvalue λ of matrix A is the number of times λ appears as a root of characteristic polynomial (i.e., the polynomial whose roots are the eigenvalues of matrix A).\n","\n","**Geometric multiplicity** of an eigenvalue is the number of linearly independent eigenvectors associated with it."]},{"cell_type":"markdown","metadata":{"id":"wMc3jknGcuqG"},"source":["### Eigenvalue Decomposition"]},{"cell_type":"markdown","metadata":{"id":"Teo5QBDTrQNI"},"source":["In linear algebra, eigendecomposition (or spectral decomposition) is the factorization of a matrix into a matrix that is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way. Refer to [this](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module1/Eigen_Vectors.pdf) article to delve deeper.\n","\n","It is applicable to a square matrix $A$ with linearly independent eigenvectors (not necessarily distinct eigenvalues).\n","\n","Most matrices are complete, i.e. their (complex) eigenvectors form a basis of the underlying vector space.\n","\n","- A particularly important class is the symmetric matrices, whose eigenvectors form an orthogonal basis of $R^n$. A non-square matrix A does not have eigenvalues. In their place, one uses the square roots of the eigenvalues of the associated square Gram matrix $K = A^TA$, which are called singular values of the original matrix."]},{"cell_type":"markdown","metadata":{"id":"IP67HBhFcuqG"},"source":["**Existence:** An n-by-n matrix $A$ always has $n$ (complex) eigenvalues, which can be ordered (in more than one way) to form an n-by-n diagonal matrix $D$ and a corresponding matrix of nonzero columns $V$ that satisfies the eigenvalue equation $$AV=VD$$\n","\n","Decomposition is therefore given as: $$ A=VDV^{-1}$$ where $D$ is a diagonal matrix formed from the eigenvalues of $A$,\n","\n","and the columns of $V$ are the corresponding eigenvectors of $A$.\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/EigenDecomposition.png\" width= 500 px/>\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"7IPlmH1ysYSF"},"source":["Now let us look at a numerical example of eigenvalue decomposition"]},{"cell_type":"markdown","metadata":{"id":"EM6vmdiEcuqH"},"source":["**Exercise 3:** Find the eigenvalue decomposition of\n","\n","$$\\mathbf{A} = \\begin{bmatrix} 5 & 2 & 0 \\\\ 2 & 5 & 0 \\\\ -3 & 4 & 6 \\end{bmatrix}.$$"]},{"cell_type":"code","metadata":{"id":"b81tgjfNcuqH"},"source":["# YOUR CODE HERE to create 'A'\n","A"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHoAPLqGcuqH"},"source":["# Eigen values and eigen vectors of A\n","E = A.eigenvects()   # returns eigenvalues, their algebraic multiplicity, eigenvectors\n","E                    # The algebraic multiplicity of an eigenvalue λ of A is the number of times λ appears as a root of polynomial of A."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dChulK_XcuqI"},"source":["# Matrix consist of eigen vectors as columns\n","V = sy.Matrix()\n","for i in range(len(E)):\n","    V = V.col_insert(i, sy.Matrix(E[i][2]))\n","V"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUkgjEPEcuqI"},"source":["# Diagonal matrix consist of eigen values\n","D = sy.Matrix([[E[0][0], 0, 0], [0, E[1][0], 0], [0, 0, E[2][0]]])\n","D"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OTRIqcrMcuqJ"},"source":["# V.D.V_inverse = A\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"495NZMLeojY5"},"source":["### Singular Value Decomposition"]},{"cell_type":"markdown","metadata":{"id":"mBlfI1FlCkoi"},"source":["Let $A$ be an m x n matrix with rank $r$. Then there exists an m x n matrix $Σ$ for which the diagonal entries in $D$ are the first $r$ singular values of $A$, $\\sigma_1≥\\sigma_2≥\\cdots≥\\sigma_r>0$, and there exists an m x m orthogonal matrix $U$ and an n x n orthogonal matrix $V$ such that\n","\n","$$\\begin{equation}\n","A = U\\Sigma V^{T}\n","\\end{equation}.$$\n","\n","\n","Any factorization $\\begin{equation}A = U\\Sigma V^{T}\\end{equation}$, with $U$ and $V$ orthogonal, $Σ$ as\n","$\\mathbf{Σ} = \\begin{bmatrix} D & 0 \\\\ 0 & 0 \\end{bmatrix}$\n","and positive diagonal entries in $D$, is called a **singular value decomposition** (or SVD) of A. The matrices $U$ and $V$ are not uniquely determined by $A$, but the diagonal entries of $Σ$ are necessarily the singular values of $A$. The columns of $U$ in such a decomposition are called left singular vectors of $A$, and the columns of $V$ are called\n","right singular vectors of $A$."]},{"cell_type":"markdown","metadata":{"id":"WBQwwD31dyyJ"},"source":["**Exercise 4:** Find the singular value decomposition of A where,\n","\n","$$\\mathbf{A} = \\begin{bmatrix} 4 & 11 & 14 \\\\ 8 & 7 & -2 \\end{bmatrix}$$"]},{"cell_type":"code","metadata":{"id":"V0s7yQ5JevJO"},"source":["# YOUR CODE HERE to create 'A'\n","A"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zLlrnt-xj8er"},"source":["**Step 1: Find $A^TA$**\n","\n","First, we compute the singular values $σ_i$ by finding the eigenvalues of $A^TA$."]},{"cell_type":"code","metadata":{"id":"LMjkuJddfRYG"},"source":["ATA = A.transpose() * A\n","ATA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSfftlhae_qN"},"source":["E = ATA.eigenvects()\n","E"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZAndrGqckQ0R"},"source":["**Step 2: Set up $V$ and $\\Sigma$**\n","\n","From the above results, the eigenvalues of $A^TA$ are 360, 90, and 0. Arrange the eigenvalues in decreasing order. Corresponding unit eigenvectors are shown below."]},{"cell_type":"code","metadata":{"id":"S6s29b_1hHWz"},"source":["e1, e2, e3 = E[2][0], E[1][0], E[0][0]                                          # eigen values\n","print(\"Eigen values:\", e1, e2, e3)\n","\n","v1 = sy.Matrix(E[2][2])                                                         # eigen vectors\n","# YOUR CODE HERE to create 'v2'\n","# YOUR CODE HERE to create 'v3'\n","\n","v1 = v1/v1.norm()                                                               # unit eigen vectors\n","# YOUR CODE HERE to update 'v2'\n","# YOUR CODE HERE to update 'v3'\n","print(\"Unit eigen vectors:\")\n","v1, v2, v3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eI0hTMdLolOL"},"source":[" The corresponding unit eigenvectors, v1, v2, and v3, are the right singular vectors of $A$. Using these we can construct orthogonal matrix $V$."]},{"cell_type":"code","metadata":{"id":"GguMfs1PmAFS"},"source":["eig_vec = {0: v1, 1: v2, 2: v3}\n","# YOUR CODE HERE to create an empty matrix 'V'\n","for k,v in eig_vec.items():\n","    V = V.col_insert(k, v)\n","V"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PipHBAcJrEt8"},"source":["The square roots of the eigenvalues of $A^TA$ are the singular values of $A$:\n","\n"," $σ_1$ = √360 = 6√10,\n","\n"," $σ_2$ = √90 = 3√10 and\n","\n"," $σ_3$ = 0.\n","\n"," The nonzero singular values are the diagonal entries of $D$. The matrix $\\Sigma$ is the same size as $A$, with $D$ in its upper left corner and with $0$’s elsewhere.\n","\n","$$\\mathbf{D} = \\begin{bmatrix} 6√10 & 0 \\\\ 0 & 3√10 \\end{bmatrix},\\ \\ \\ \\mathbf{\\Sigma} = \\begin{bmatrix} D & 0 \\end{bmatrix} = \\begin{bmatrix} 6√10 & 0 & 0 \\\\ 0 & 3√10 & 0 \\end{bmatrix}$$"]},{"cell_type":"code","metadata":{"id":"Qxn6KebNs06M"},"source":["s1, s2 = e1**0.5, e2**0.5                                     # singular values\n","sigma = sy.Matrix([[s1, 0, 0], [0, s2, 0]])\n","sigma"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"molt3COMvOu6"},"source":["**Step 3: Construct $U$**\n","\n","When $A$ has rank $r$, the first $r$ columns of $U$ are the normalized\n","vectors obtained from $Av_1,\\cdots, Av_r$ . Here, $A$ has two nonzero singular values, so rank $A$ = 2.\n","\n"," Also, from $||Av_i||^2 = \\lambda_i $ where $\\lambda$ denotes eigen value\n","\n","we have\n","  $||Av_1|| = √\\lambda_1 = \\sigma_1$ and $||Av_2|| = √\\lambda_2 = \\sigma_2$.\n","  \n","Thus\n","\n","$u_1 = \\frac{1}{||Av_1||}Av_1 = \\frac{1}{\\sigma_1}Av_1$\n","\n","$u_2 = \\frac{1}{||Av_2||}Av_2 = \\frac{1}{\\sigma_2}Av_2$\n","\n","Note that {$u_1, u_2$} is already a basis for $\\mathbb{R}^2$. Thus no additional vectors are needed for $U$, and\n","$U = \\begin{bmatrix} u_1 \\ u_2 \\end{bmatrix}$."]},{"cell_type":"code","metadata":{"id":"hq0SasKhvHe0"},"source":["u1 = A * v1 / s1\n","# YOUR CODE HERE to create 'u2'\n","# YOUR CODE HERE to create empty matrix 'U'\n","U = U.col_insert(0, u1)\n","# YOUR CODE HERE to insert 'u2' column to 'U' at position 1\n","U"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9a9nIDPdyjmh"},"source":["# U.Σ.V^T = A\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAYzQ7gI8HLw"},"source":["**Note:** Take the case when $\\sigma_2 = 0$, then  $Av_2 = 0$ as $||Av_i|| = \\sigma_i$ and only column we found for $U$ is {$u_1$} then other columns of $U$ are found by extending the set {$u_1$} to an orthonormal basis for $\\mathbb{R}^3$. Say we need two orthogonal unit vectors $u_2$ and $u_3$ that are orthogonal to $u_1$ then each vector must satisfy $u^T_1 x = 0.$"]},{"cell_type":"markdown","metadata":{"id":"XuRpQxJycupy"},"source":["#### Image compression using SVD"]},{"cell_type":"markdown","metadata":{"id":"lsW3SYsbcupx"},"source":["**Low-rank approximations of $A$**\n","\n","Let $k$ a natural number, where $k\\leq\\text{rank}(A)\\leq\\min\\{n, m\\}$.\n","\n","Using SVD-decomposition of $A$, if $A = UDV^T$, then we keep the first $k$ values in $D$ as it is and set the subsequent singular values to zero. Let us denote the resulting diagonal matrix by $D_k$. It is easy to see that we only have to keep the first $k$ columns of $U$ and the first $k$ rows of $V$ since their other columns would be multiplied by zeros anyway as shown in the figure below. To sum up, the matrix $A_k = U_kD_kV_k^T$ is the closest matrix to $A$ having rank $k$, where $U_k$ and $V_k$ consist of the first $k$ columns and rows of $U$ and $V$, respectively.\n","\n","If $A$ is a large matrix, that is $n,m$ are large and $k$ is relatively small, then the information we need to store to approximate the information content stored in $A$ is much smaller. That is, we can reduce the storage space significantly and we are still able to store almost the same information that the original matrix has.\n","<center>\n","<img src= \"https://cdn.iisc.talentsprint.com/CDS/Images/svd.png\" width= 400 px/>\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"4bKFq9bqcupz"},"source":["Here we will see how low-rank approximation of a matrix provides a solution to compress an image.\n","\n","Images are represented in a rectangular array where each element corresponds to the grayscale value for that pixel. For colored images we have a $3$-dimensional array of size $n\\times m\\times 3$, where $n$ and $m$ represents the number of pixels vertically and horizontally, respectively, and for each pixel we store the intensity for colors red, green and blue.\n","\n","We will repeat the low-rank approximation procedure above on a larger matrix, that is, we create the low-rank approximation of a matrix that represents an image for each color separately. The resulting $3$-dimensional array will be a good approximation of the original image."]},{"cell_type":"code","metadata":{"id":"FrT5fZmacup0"},"source":["# Read image store it as array\n","image_ = np.array(Image.open('houses.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wz9lNvvHcup1"},"source":["# Normalize the intensity values in each pixel\n","image = image_ / 255\n","row, col, _ = image.shape\n","print(\"pixels: \", row, \"*\", col)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Em10qx7jcup3"},"source":["# Display image\n","fig = plt.figure(figsize=(12, 10))\n","a = fig.add_subplot(1, 1, 1)\n","imgplot = plt.imshow(image)\n","a.set_title('Houses')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WyXVy6mcup6"},"source":["# Break the image into three different arrays based on colors\n","image_red = image[:, :, 0]\n","image_green = image[:, :, 1]\n","# YOUR CODE HERE to create 'image_blue'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLmiM8u-cup7"},"source":["# Check for the space needed for original image\n","original_bytes = image.nbytes\n","print(\"The space (in bytes) needed to store this image is\", original_bytes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LHEnGj4Acup7"},"source":["Now perform the SVD-decomposition on the $3$ matrices corresponding to the different colors separately."]},{"cell_type":"code","metadata":{"id":"FgMWc_Egcup8"},"source":["# SVD decomposition of three matrices\n","U_r, D_r, VT_r = np.linalg.svd(image_red, full_matrices=True)\n","# YOUR CODE HERE to create U_g, D_g, VT_g\n","# YOUR CODE HERE to create U_b, D_b, VT_b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"envwkMfncup8"},"source":["# Check for the bytes to be stored\n","bytes_to_be_stored = sum([matrix.nbytes for matrix in [U_r, D_r, VT_r, U_g, D_g, VT_g, U_b, D_b, VT_b]])\n","print(\"The matrices that we store have total size (in bytes):\", bytes_to_be_stored)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7vclXqIKcup9"},"source":["Now we decide that the information that the image contains and represented in $1024$ columns can be represented with $k=50$ columns as well, but these $k$ columns will be taken from the decomposition matrices."]},{"cell_type":"code","metadata":{"id":"AZBMXrbHcup9"},"source":["# Specify value of k\n","k = 50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lR-wIU7Hcup9"},"source":["# Selecting k columns from U matrix and k rows from VT matrix\n","U_r_k  = U_r[:, 0:k]\n","VT_r_k = VT_r[0:k, :]\n","# YOUR CODE HERE to create 'U_g_k'\n","# YOUR CODE HERE to create 'VT_g_k'\n","U_b_k  = U_b[:, 0:k]\n","# YOUR CODE HERE to create 'VT_b_k'\n","\n","D_r_k = D_r[0:k]\n","# YOUR CODE HERE to create 'D_g_k'\n","# YOUR CODE HERE to create 'D_b_k'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"42TzI9C0cup-"},"source":["compressed_bytes = sum([matrix.nbytes for matrix in [U_r_k, D_r_k, VT_r_k, U_g_k, D_g_k, VT_g_k, U_b_k, D_b_k, VT_b_k]])\n","print(\"The compressed matrices that we store now have total size (in bytes):\", compressed_bytes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wzo2p8XQcup-"},"source":["ratio = compressed_bytes / original_bytes\n","print(\"The compression ratio between the original image size and the total size of the compressed factors is\", ratio)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F6eWrfzOcup_"},"source":["Let's construct the approximate matrices for each color and merge them together. We also need to correct those pixels where the intensity value is outside of the range $[0,1]$."]},{"cell_type":"code","metadata":{"id":"vOdJW9RHcup_"},"source":["# Reconstruct matrices for each color\n","image_red_approx = np.dot(U_r_k, np.dot(np.diag(D_r_k), VT_r_k))\n","image_green_approx = np.dot(U_g_k, np.dot(np.diag(D_g_k), VT_g_k))\n","# YOUR CODE HERE to create 'image_blue_approx'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRuD106rcuqA"},"source":["# Reconstruct the original image from color matrices\n","image_reconstructed = np.zeros((row, col, 3))\n","\n","image_reconstructed[:, :, 0] = image_red_approx\n","image_reconstructed[:, :, 1] = image_green_approx\n","# YOUR CODE HERE for image_blue_approx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SllVV-B6cuqA"},"source":["# Correct the pixels where intensity value is outside the range [0,1]\n","image_reconstructed[image_reconstructed < 0] = 0\n","image_reconstructed[image_reconstructed > 1] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qL5CKa0cuqB"},"source":["# Display the reconstructed image\n","\n","fig = plt.figure(figsize=(12, 10))\n","# YOUR CODE HERE\n","# YOUR CODE HERE using 'image_reconstructed'\n","a.set_title('Houses, compressed image using the rank-{} approximation'.format(k))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t02Zv7pqcuqB"},"source":["Different values of $k$ results in different compression quality, the higher the $k$ is, the closer the compressed image to the original, but increasing $k$ means larger matrices."]},{"cell_type":"markdown","metadata":{"id":"zeWrzgzOcuqK"},"source":["### PageRank"]},{"cell_type":"markdown","metadata":{"id":"WMPY9fHhbBda"},"source":["PageRank (PR) is an algorithm used by Google Search to rank web pages in their search engine results.\n","\n","* It is a way of measuring the importance of website pages.\n","* It works by counting the number and quality of links to a page to determine a rough estimate of how important the website is.\n","* More important websites are likely to receive more links from other websites.\n","* Outputs a **probability distribution** used to represent the likelihood that a person randomly clicking on links will arrive at any particular page.\n","* Distribution is evenly divided among all pages in the collection at the beginning of the computational process.\n","* PageRank computations require several passes, called \"iterations\", through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value.\n","\n","For further information, refer [here](http://mason.gmu.edu/~acurran3/EigenValues/Eigenvalues.html)."]},{"cell_type":"markdown","metadata":{"id":"vzX6M7uIcuqM"},"source":["<img src= \"https://cdn.iisc.talentsprint.com/CDS/Images/pagerank.JPG\" width= 200 px/>"]},{"cell_type":"markdown","metadata":{"id":"HsnRj23vcuqM"},"source":["The diagram above represents a model mini Internet, where\n","* each bubble is a webpage, and\n","* each arrow represents a link on that webpage that takes you to one of the others\n","\n","We will build an expression that tells us which of these web pages is most relevant to the person who made the search.\n","\n","We can describe the links present on page $A$ as a vector, where each row is either a one or a zero based on whether there is a link to the corresponding page. And then normalize the vector by the total number of the links, such that they can be used to describe a probability for that page. For example, the vector of links from page A will be 0 1 1 1. So we can write, $L_A = (0, 1/3, 1/3, 1/3)$. Similarly, the link vectors in the next three sites are $L_B = (1/2, 0, 0, 1/2)$, $L_C = (0, 0, 0, 1)$ and $L_D = (0, 1/2, 1/2, 0)$.\n","\n","We can now build our link matrix $L$ by using each of the linked vectors as a column. With matrix $L$ we are trying to represent the probability of ending up on each of the pages.\n","\n","$$L = \\begin{bmatrix} 0 & \\frac{1}{2} & 0 & 0 \\\\ \\frac{1}{3} & 0 & 0& \\frac{1}{2} \\\\ \\frac{1}{3} & 0 & 0 & \\frac{1}{2} \\\\ \\frac{1}{3} & \\frac{1}{2} & 1& 0 \\end{bmatrix}$$\n","\n","We use the vector $r$ to store the rank of all web pages. We can write the expression applied to all web pages as simple matrix multiplication. So $$r = Lr$$\n","\n","We assume that all the ranks are equal and normalized by the total number of web pages in the analysis. So $r = (\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$. Then, each time we multiply $r$ by the matrix $L$, this gives us an updated value for $r$. So we can say that $$r_{i+1} = L  r_i$$\n","\n","Applying this expression repeatedly means that we are solving this problem iteratively. Each time we do this, we update the values in $r$ until, eventually, $r$ stops changing. So now $r$ does equal $Lr$. This implies that $r$ is now an eigenvector of matrix $L$, with an eigenvalue of $1$."]},{"cell_type":"code","metadata":{"id":"GNWz7az2cuqN"},"source":["import sympy as sy\n","# Build L matrix\n","L = sy.Matrix([[0, 1/2, 0, 0],\n","               [1/3, 0, 0, 1/2],\n","               [1/3, 0, 0, 1/2],\n","               [1/3, 1/2, 1, 0]])\n","print(\"L:\")\n","pprint(L)\n","# Build initial r vector\n","r = sy.Matrix([[1/4], [1/4], [1/4], [1/4]])\n","previous_r = r\n","for i in range(1,100):\n","     r = L * r\n","     if previous_r==r:\n","                break\n","     previous_r = r\n","print('\\n r: ',r)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gCW48mOFcuqN"},"source":["So now we have our result, which says that as we randomly click around our network, we would expect to spend about 40% of the time on page D. But only about 12% of the time on page A, with 24% on each of pages B and C.\n","\n","The same can be calculated by finding the eigen vector of matrix $L$ with an eigenvalue of 1.\n","\n","For an eigen vector $x$ the following equation should hold\n","$$|L - \\lambda I|x = 0$$\n","\n","where $\\lambda$ is the corresponding eigen value and $I$ is an identity matrix."]},{"cell_type":"code","metadata":{"id":"7xWu31n5cuqO"},"source":["I = sy.eye(4)\n","eig_val = 1\n","A = L - eig_val * I\n","A"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OhToJX5zcuqO"},"source":["# Create an augmented matrix\n","Aug = A.col_insert(4, sy.Matrix([[0], [0], [0], [0]]))\n","Aug.rref()                  # show the rref form"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95LFhaI6cuqO"},"source":["From above we get three equations with four variables.\n","$$x_1 - 0.3x_4 = 0$$\n","$$x_2 - 0.6x_4 = 0$$\n","$$x_3 - 0.6x_4 = 0$$\n"," As the results should be a probability distribution we can add one more condition to it\n","$$x_1 + x_2 + x_3 + x_4 = 1$$\n","\n","Now new augmented matrix will be,"]},{"cell_type":"code","metadata":{"id":"cMKBaPDkcuqP"},"source":["Aug_new = sy.Matrix([[1, 0, 0, -0.3, 0],\n","                    [0, 1, 0, -0.6, 0],\n","                    [0, 0, 1, -0.6, 0],\n","                    [1, 1, 1, 1, 1]])\n","Aug_new.rref()                          # show rref form"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKrSdzGacuqP"},"source":["The results are the same as in the case of iterative method."]},{"cell_type":"markdown","metadata":{"id":"v7t8xOYAcuqQ"},"source":["### Principal Component Analysis"]},{"cell_type":"markdown","metadata":{"id":"HlpwnuyzcuqQ"},"source":["The problem we face when analyzing higher-dimensional datasets is referred to as “The curse of dimensionality”. Principal component analysis or PCA is known as a dimensionality reduction technique.\n","\n","Steps to implement PCA:\n","\n","* Subtract the mean of each variable so that the dataset will be centered on the origin.\n","* Calculate the Covariance Matrix\n","* Compute the Eigenvalues and Eigenvectors for the calculated Covariance matrix. The Eigenvectors of the Covariance matrix are Orthogonal to each other and each vector represents a principal axis. A Higher Eigenvalue corresponds to a higher variability. Hence the principal axis with the higher Eigenvalue will be an axis capturing higher variability in the data.\n","\n","* Sort Eigenvalues in descending order: each column in the Eigen vector-matrix corresponds to a principal component, so arranging them in descending order of their Eigenvalue will automatically arrange the principal component in descending order of their variability.\n","\n","* Select a subset from the rearranged Eigenvalue matrix as per need\n","* Transform the data: transform the data by having a dot product between the transpose of the Eigenvector subset and the transpose of the mean-centered data. By transposing the outcome of the dot product, the result we get is the data reduced to lower dimensions from higher dimensions.\n","\n","In below example we will see how PCA works."]},{"cell_type":"code","metadata":{"id":"qE_IyAobcuqR"},"source":["# Write a function for PCA\n","def PCA(X , num_components):\n","\n","    # Subtract the mean\n","    X_meaned = X - np.mean(X , axis = 0)\n","\n","    # Calculate the covariance matrix\n","    cov_mat = np.cov(X_meaned , rowvar = False)\n","\n","    # Compute the eigen values and eigen vectors\n","    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n","\n","    # Sort the eigen values in descending order\n","    sorted_index = np.argsort(eigen_values)[::-1]\n","    sorted_eigenvalue = eigen_values[sorted_index]\n","    sorted_eigenvectors = eigen_vectors[:,sorted_index]\n","\n","    # Select a subset\n","    eigenvector_subset = sorted_eigenvectors[:, 0:num_components]\n","\n","    # Transform the data\n","    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()\n","\n","    return X_reduced"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HK7J-lqlcuqR"},"source":["**Dataset**\n","\n","The dataset we use here for performing principal component analysis is Iris dataset. It contains four features (length and width of sepals and petals measured in cm) of 50 samples of three species of Iris plant (Iris setosa, Iris virginica and Iris versicolor).\n","\n","<img src=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png\" width= 500 px/>"]},{"cell_type":"code","metadata":{"id":"xAvd1L7AcuqS"},"source":["# Import Iris Dataset\n","from sklearn import datasets\n","\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","data = pd.DataFrame(X, columns=iris.feature_names)\n","data = pd.concat([data , pd.DataFrame(y, columns= ['target'])] , axis = 1)\n","for i in range(len(data)):\n","    if data.loc[i, 'target'] == 0:\n","        data.loc[i, 'target'] = iris.target_names[0]\n","    if data.loc[i, 'target'] == 1:\n","        data.loc[i, 'target'] = iris.target_names[1]\n","    if data.loc[i, 'target'] == 2:\n","        # YOUR CODE HERE\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oUq6Rl-LcuqT"},"source":["# Prepare the data\n","x = data.iloc[:,0:4]\n","\n","# Prepare the target\n","target = data.iloc[:,4]\n","\n","# Applying it to PCA function\n","# YOUR CODE HERE to create 'mat_reduced' by applying PCA function to 'x' and using num_coponents to 2\n","\n","# Creating a Pandas DataFrame of reduced Dataset\n","principal_df = pd.DataFrame(mat_reduced , columns = ['PC1','PC2'])\n","\n","# Concat it with target variable to create a complete Dataset\n","principal_df = pd.concat([principal_df , pd.DataFrame(target, columns= ['target'])] , axis = 1)\n","# YOUR CODE HERE to display few rows of 'principal_df'\n","sns.scatterplot(data = principal_df, x= 'PC1', y= 'PC2', hue= 'target')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkjkth3qcuqT"},"source":["From the above plot, it can be seen even after reducing the data to only two variables its variability is still intact."]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"DCu_mPiQY8Tw"},"source":["#@title Select the FALSE statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\", \"Eigendecomposition can only be applied to diagonalizable matrices\", \"Singular value decomposition is defined only for symmetric matrices\", \"Cholesky factorization is a decomposition of a symmetric, positive-definite matrix into the product of a lower triangular matrix and its transpose\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}