{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7rc1"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"competent-productivity"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Additional Notebook (Ungraded): Automatic differentiation"],"id":"competent-productivity"},{"cell_type":"markdown","metadata":{"id":"rubber-supplement"},"source":["## Learning Objectives"],"id":"rubber-supplement"},{"cell_type":"markdown","metadata":{"id":"underlying-brain"},"source":["At the end of the experiment, you will be able to:\n","\n","* understand the basics of automatic differentiation\n","\n","* understand the backward and forward propagation for a given neural network"],"id":"underlying-brain"},{"cell_type":"code","source":["#@title Walkthrough Video\n","from IPython.display import HTML\n","HTML(\"\"\"<video width=\"420\" height=\"240\" controls>\n","<source src=\"https://cdn-exec.ap-south-1.linodeobjects.com/content/Automatic_Differentiation_v1_Debasish_Bhaskar_edited.mp4\">\n","</video>\"\"\")"],"metadata":{"cellView":"form","id":"43DvGwczBmQ6"},"id":"43DvGwczBmQ6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GOaaHHiyw_Ae"},"source":["## Information"],"id":"GOaaHHiyw_Ae"},{"cell_type":"markdown","metadata":{"id":"vi8GMPVfxWMC"},"source":["**Understanding Basics of Neural Network and its parameters**\n","\n","**Neural Network**\n","\n","Neural networks are a class of machine learning algorithms used to model complex patterns in datasets using multiple hidden layers and non-linear activation functions. A neural network takes an input, passes it through multiple layers of hidden neurons (mini-functions with unique coefficients that must be learned), and outputs a prediction representing the combined input of all the neurons.\n","\n","Neural networks are trained iteratively using optimization techniques like gradient descent. After each cycle of training, an error metric is calculated based on the difference between prediction and target.\n","\n","**Neuron**\n","\n","A neuron takes a group of weighted inputs, applies an activation function, and returns an output.\n","\n","**Weights**\n","\n","Weights are values that control the strength of the connection between two neurons. That is, inputs are typically multiplied by weights, and that defines how much influence the input will have on the output. In other words: when the inputs are transmitted between neurons, the weights are applied to the inputs along with an additional value (the bias)\n","\n","**Bias**\n","\n","Bias terms are additional constants attached to neurons and added to the weighted input before the activation function is applied. Bias terms help models represent patterns that do not necessarily pass through the origin.\n","\n","**Layers**\n","\n","*Input Layer*\n","\n","Holds the data your model will train on. Each neuron in the input layer represents a unique attribute in your dataset (e.g. height, hair color, etc.).\n","\n","*Hidden Layer*\n","\n","Sits between the input and output layers and applies an activation function before passing on the results. There are often multiple hidden layers in a network.\n","\n","*Output Layer*\n","\n","The final layer in a network. It receives input from the previous hidden layer, optionally applies an activation function, and returns an output representing your model’s prediction."],"id":"vi8GMPVfxWMC"},{"cell_type":"markdown","metadata":{"id":"immune-shopper"},"source":["#### Importing required packages"],"id":"immune-shopper"},{"cell_type":"code","metadata":{"id":"catholic-reach"},"source":["import numpy as np\n","import scipy\n","import matplotlib.pyplot as plt\n","import numpy.linalg as npl #Linear algebra from numpy\n","from scipy.optimize import differential_evolution #Finds the global minimum of a multivariate function.\n","import math\n","from scipy.stats import norm"],"id":"catholic-reach","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6pD72cY_5HZP"},"source":["## Automatic Differentiation"],"id":"6pD72cY_5HZP"},{"cell_type":"markdown","metadata":{"id":"2QPxPjWn8W5I"},"source":["How do neural networks calculate the partial derivatives of an expression? The answer lies in a process known as automatic differentiation. Automatic differentiation can only calculate the partial derivative of an expression on a certain point."],"id":"2QPxPjWn8W5I"},{"cell_type":"markdown","metadata":{"id":"9NxCmhaW3L74"},"source":["## Backpropagation\n","\n","Backpropagation is a special case of automatic differentiation. We can think of automatic differentiation as a set of techniques to numerically (in contrast to\n","symbolically) evaluate the exact gradient of a\n","function by working with intermediate variables and applying the chain\n","rule. Automatic differentiation applies a series of elementary arithmetic operations, e.g., addition and multiplication and elementary functions,\n","e.g., sin, cos, exp, log. By applying the chain rule to these operations, the\n","gradient of quite complicated functions can be computed automatically.\n","Automatic differentiation applies to general computer programs and has\n","forward and reverse modes."],"id":"9NxCmhaW3L74"},{"cell_type":"markdown","metadata":{"id":"msi4InSQ37AY"},"source":["Let us look at an instuctive example to understand reverse mode of propagation.\n","\n","*Example:* Consider the function\n","\n","$f(x) = \\sqrt{x^{2}+ \\exp{(x^{2})}} + \\cos{(x^{2}+ \\exp{(x^{2})}) } $\n","\n","If we were to implement a function f on a computer, we would be able to save some computation by using intermediate variables:\n","\n","$a=x^{2},$\n","\n","$b=\\exp{(a)},$\n","\n","$c=a+b,$\n","\n","$d=\\sqrt{c},$\n","\n","$e=\\cos{c},$\n","\n","$f=d+e.$\n","\n","![Image]( https://cdn.iisc.talentsprint.com/CDS/Images/Automatic_differentiation.png)\n","\n","$\\text{Figure: Computation graph with inputs x, function values f, and intermediate variables a, b, c, d, e.}$\n","\n","The set of equations that include intermediate variables can be thought\n","of as a computation graph, a representation that is widely used in implementations of neural network software libraries. We can directly compute\n","the derivatives of the intermediate variables with respect to their corresponding inputs by recalling the definition of the derivative of elementary\n","functions. We obtain the following:\n","\n","$\\frac{\\partial a}{\\partial x} = 2x $\n","\n","$\\frac{\\partial b}{\\partial a} = \\exp{(a)} $\n","\n","$\\frac{\\partial c}{\\partial a} = 1 = \\frac{\\partial c}{\\partial b} $\n","\n","$\\frac{\\partial d}{\\partial c}= \\frac{1}{2\\sqrt{c}}$\n","\n","$\\frac{\\partial e}{\\partial c} = -\\sin{(c)} $\n","\n","$\\frac{\\partial f}{\\partial e} = 1 = \\frac{\\partial f}{\\partial d}$\n","\n","By looking at the computation graph in Figure above, we can compute\n","$∂f /∂x$ by working backward from the output and obtain:\n","\n","$\\frac{∂f}{∂c} = \\frac{∂f}{∂d}  \\frac{∂d}{∂c} + \\frac{∂f}{∂e}  \\frac{∂e}{∂c}$\n","\n","$\\frac{∂f}{∂b} = \\frac{∂f}{∂c}\\frac{∂c}{∂b}$\n","\n","$\\frac{∂f}{∂a} = \\frac{∂f}{∂b}\\frac{∂b}{∂a} + \\frac{∂f}{∂c}\\frac{∂c}{∂a}$\n","\n","$\\frac{∂f}{∂x} = \\frac{∂f}{∂a}\\frac{∂a}{∂x}$\n","\n","Note that we implicitly applied the chain rule to obtain $∂f/∂x$. By substituting the results of the derivatives of the elementary functions, we get\n","\n","$\\frac{∂f}{∂c} = 1 · \\frac{1}{2√c} + 1 · (− sin(c))$\n","\n","${∂f}{∂b} = \\frac{∂f}{∂c} · 1$\n","\n","$\\frac{∂f}{∂a} = \\frac{∂f}{∂b} exp(a) + \\frac{∂f}{∂c} · 1$\n","\n","$\\frac{∂f}{∂x} = \\frac{∂f}{∂a} · 2x$\n","\n","By thinking of each of the derivatives above as a variable, we observe\n","that the computation required for calculating the derivative is of similar\n","complexity as the computation of the function itself."],"id":"msi4InSQ37AY"},{"cell_type":"markdown","metadata":{"id":"AckjdFQYR1wn"},"source":["Backpropagation is a standard method of training artificial neural networks.  This method is used for fine-tuning the weights of a neural net based on the error rate obtained in the previous iteration. Proper tuning of the weights reduces the error rates and allows the model to make increasingly reliable predictions. It traverses the network in reverse order, from the output to the input layer, according to the chain rule from calculus and helps to calculate the gradient of a loss function with respect to all the weights in the network."],"id":"AckjdFQYR1wn"},{"cell_type":"markdown","metadata":{"id":"XK0xaEPlnt0y"},"source":["![NN](https://cdn.iisc.talentsprint.com/CDS/NN.jpg)"],"id":"XK0xaEPlnt0y"},{"cell_type":"markdown","metadata":{"id":"false-russia"},"source":["# Training a neural network: Forward and Backward propagation\n","\n","- Forward propagation sequentially calculates and stores intermediate variables within the computational graph defined by the neural network. It proceeds from the input to the output layer.\n","\n","- Backpropagation sequentially calculates and stores the gradients of intermediate variables and parameters within the neural network in the reversed order."],"id":"false-russia"},{"cell_type":"markdown","metadata":{"id":"conscious-arabic"},"source":["**Implementing backpropagation**"],"id":"conscious-arabic"},{"cell_type":"markdown","metadata":{"id":"sought-blood"},"source":["The back propagation algorithm begins by comparing the actual value output by the forward propagation process to the expected value and then moves backward through the network, slightly adjusting each of the weights in a direction that reduces the size of the error by a small degree. Both forward and back propagation are re-run thousands of times on each input combination until the network can accurately predict the expected output of the possible inputs using forward propagation.\n","Here we take a simple example consisting of input X and output y as given below"],"id":"sought-blood"},{"cell_type":"code","metadata":{"id":"regular-interim"},"source":["# Initialize the input and output\n","X = np.array(([0, 0], [0, 1], [1, 0], [1, 1]), dtype=float)\n","y = np.array(([0], [1], [1], [0]), dtype=float)"],"id":"regular-interim","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"entertaining-genetics"},"source":["# Initialize the parameters\n","iterations = 5000\n","output = None\n","learning_rate = 0.1\n","weights = [np.random.uniform(low=-0.2, high=0.2, size=(2, 2)), np.random.uniform(low=-2, high=2, size=(2, 1)) ]"],"id":"entertaining-genetics","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"missing-granny"},"source":["#### Forward propagation\n","\n","In forward propagation, input is multiplied with weights and resultant output is passed as input to hidden layers and finally output is carried at final layer.\n","\n","Below function `feed_forward_pass()` takes input as argument and produces the output by multiplying with weights in sequential layers. It return final layer output and also all the layers outputs which can be useful in backpropagation.  \n"],"id":"missing-granny"},{"cell_type":"code","metadata":{"id":"earned-container"},"source":["def feed_forward_pass(x_values):\n","    # forward\n","    input_layer = x_values\n","    hidden_layer = tang(np.dot(input_layer, weights[0]))\n","    # dot product of hidden layer output with weights and applying activation over it\n","    output_layer = tang(np.dot(hidden_layer, weights[1]))\n","    layers = [input_layer,hidden_layer,output_layer]\n","    # last layer is an output\n","    return layers, layers[2]"],"id":"earned-container","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"associate-figure"},"source":["#### Backpropagation\n","\n","Backpropagation is an algorithm commonly used to train neural networks. When the neural network is initialized, weights are set for its individual elements, called neurons. Inputs are loaded, they are passed through the network of neurons, and the network provides an output for each one, given the initial weights. Backpropagation helps to adjust the weights of the neurons so that the result comes closer and closer to the known true result.\n","\n","![image.png](https://cdn.iisc.talentsprint.com/CDS/BP.JPG)"],"id":"associate-figure"},{"cell_type":"code","metadata":{"id":"leading-information"},"source":["# back propagation error through the network layers\n","def backward_pass(target_output, actual_output, layers):\n","    global weights\n","    # divergence of network output\n","    err = (target_output - actual_output)\n","    # backward from output to input layer\n","    # propagate gradients using chain rule\n","    for backward in range(2, 0, -1):\n","        err_delta = err * derivative_tang(layers[backward])\n","        # update weights using computed gradient\n","        weights[backward - 1] += learning_rate * np.dot(layers[backward - 1].T, err_delta)\n","        # propagate error using updated weights of previous layer\n","        err = np.dot(err_delta, weights[backward - 1].T)\n","    return err"],"id":"leading-information","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"efficient-israeli"},"source":["#### Activation functions\n","\n","Activation function is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function)"],"id":"efficient-israeli"},{"cell_type":"code","metadata":{"id":"applicable-stream"},"source":["# activation functions\n","def tang(y):\n","    return np.tanh(y)\n","\n","# derivative of tang function to use in backpropagation\n","def derivative_tang(y):\n","    return 1.0 - y ** 2\n","\n","def sigmoid(y):\n","    return 1 / (1 + np.exp(-y))\n","\n","# derivative of sigmoid function to use in backpropagation\n","def derivative_sigmoid(y):\n","    return y * (1 - y)"],"id":"applicable-stream","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"radical-express"},"source":["#### Train the network by calling `feed_forward_pass` and `backward_pass`"],"id":"radical-express"},{"cell_type":"code","metadata":{"id":"retained-filter"},"source":["def train(x_values, target):\n","    # produce the output from forward pass\n","    layers , output = feed_forward_pass(x_values)\n","    # calculate the error and update the weights\n","    error = backward_pass(target, output,layers)\n","    return output"],"id":"retained-filter","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rocky-plaintiff"},"source":["##### we train the network for n iterations to update the weights accordingly"],"id":"rocky-plaintiff"},{"cell_type":"code","metadata":{"id":"supposed-palmer"},"source":["# training the network for 500 iterations i.e. weights will update 500 times\n","for i in range(iterations):\n","    # invoke the train function to get output\n","    output = train(X, y)\n","\n","    # To print the output for every 50 iterations\n","    ten = iterations // 10\n","    if i % ten == 0:\n","        print(\"Iteration number: {} / Squared loss:{} \".format(str(i), str(np.mean(np.square(y - output)))))"],"id":"supposed-palmer","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"committed-poker"},"source":["#### Predict\n","\n","let us define a function to predict the input by forward_pass using the updated weights"],"id":"committed-poker"},{"cell_type":"code","metadata":{"id":"funded-coach"},"source":["def predict(x_values):\n","    # passing inputs through the forward pass\n","    return feed_forward_pass(x_values)[1]"],"id":"funded-coach","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"animal-pride"},"source":["# predict\n","for i in range(len(X)):\n","    print('-' * 20)\n","    print('Input value: ' + str(X[i]))\n","    print('Predicted target: ' + str(predict(X[i])))\n","    print('Actual target: ' + str(y[i]))"],"id":"animal-pride","execution_count":null,"outputs":[]}]}