{"cells":[{"cell_type":"markdown","source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Additional Notebook (Ungraded): Introduction to CuPy"],"metadata":{"id":"rb5tAWQakmNn"},"id":"rb5tAWQakmNn"},{"cell_type":"markdown","id":"4941058a-6c14-4330-8dee-a9e137c516f3","metadata":{"id":"4941058a-6c14-4330-8dee-a9e137c516f3"},"source":["## Learning Objectives"]},{"cell_type":"markdown","source":["At the end of the experiment, you will be able to\n","\n","* understand the purpose and benefits of using CuPy as a GPU-accelerated library for numerical computations\n","* learn to install CuPy and set up GPU configurations to leverage GPU power for faster computations\n","* explore basic array creation, manipulation, and mathematical operations using CuPy arrays\n","* perform element-wise operations, matrix operations\n","* to merge multiple kernels together into a single combined kernel\n","* memory Pool Operations"],"metadata":{"id":"JjXQsHzvlGgq"},"id":"JjXQsHzvlGgq"},{"cell_type":"markdown","source":["## Information"],"metadata":{"id":"6WLfal7ovpZo"},"id":"6WLfal7ovpZo"},{"cell_type":"markdown","source":["**CuPy**\n","\n","CuPy is a powerful numerical computation library designed to work seamlessly with NVIDIA GPUs, providing accelerated computing capabilities for array operations and mathematical computations. CuPy is a GPU array backend that implements a subset of NumPy interface.\n","\n","By making use of CuPy, developers can get the advantage of the parallel processing power of GPUs to significantly speed up array manipulations, matrix multiplications, and various mathematical operations. CuPy implements many functions on cupy.ndarray objects. See the reference for the supported subset of NumPy API. Knowledge of NumPy will help you utilize most of the CuPy features.\n","\n","CuPy's syntax and functionality are highly compatible with NumPy, making it easy for users familiar with NumPy to transition and take advantage of GPU acceleration. The compatibility extends to common array operations, linear algebra functions, and statistical computations, enabling users to scale their computations efficiently and handle large datasets with ease. CuPy has a concept of a current device, which is the default GPU device on which the allocation, manipulation, calculation, etc., of arrays take place.\n","\n","  - All CuPy operations (except for multi-GPU features and device-to-device copy) are performed on the currently active device.\n","\n","  - In general, CuPy functions expect that the array is on the same device as the current one. Passing an array stored on a non-current device may work depending on the hardware configuration but is generally discouraged as it may not be performant."],"metadata":{"id":"d5e7X5HbwoIf"},"id":"d5e7X5HbwoIf"},{"cell_type":"markdown","id":"fb079367-934a-4d8e-a5c3-ec5592ac69ae","metadata":{"id":"fb079367-934a-4d8e-a5c3-ec5592ac69ae"},"source":["**Introduction to Workshop Lab Environment**\n","\n","For documents on CuPy, you can [refer to this link](https://docs.cupy.dev/en/stable/install.html)"]},{"cell_type":"markdown","source":["Reference:\n","\n","For sample code reference, you can [click here](https://github.com/cupy/cupy/tree/main)"],"metadata":{"id":"eLFNEXVclOQ7"},"id":"eLFNEXVclOQ7"},{"cell_type":"markdown","id":"8fe05310-7998-4a4f-a660-f762f3359fab","metadata":{"id":"8fe05310-7998-4a4f-a660-f762f3359fab"},"source":["To verify that the Python environment is set up correctly. The code cells contain interactive Python code."]},{"cell_type":"code","execution_count":null,"id":"2d06d849-08b0-4dfc-b76d-b60be5b172cf","metadata":{"id":"2d06d849-08b0-4dfc-b76d-b60be5b172cf"},"outputs":[],"source":["print(\"Hello World!!!\")"]},{"cell_type":"markdown","id":"5c0f80bd-c004-4d16-9f1b-974c971d2235","metadata":{"id":"5c0f80bd-c004-4d16-9f1b-974c971d2235"},"source":["The following command is used to install the CuPy library using the Python package manager 'pip'"]},{"cell_type":"code","source":["!pip install cupy"],"metadata":{"id":"k_a-SyqXcLKu"},"id":"k_a-SyqXcLKu","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Query for some basic information about the system. `nvidia-smi` is like `top` for NVIDIA GPUs."],"metadata":{"id":"ry-XIR6IZsff"},"id":"ry-XIR6IZsff"},{"cell_type":"code","execution_count":null,"id":"b74837c5-d1ed-4a7f-ba8c-a0d99c668093","metadata":{"id":"b74837c5-d1ed-4a7f-ba8c-a0d99c668093"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","id":"611fd29b-e458-4c99-ab63-cb456524d5e3","metadata":{"id":"611fd29b-e458-4c99-ab63-cb456524d5e3"},"source":["Check out the connection topology of the system."]},{"cell_type":"code","execution_count":null,"id":"c03404cd-d681-46cd-b91a-1d1784eefcee","metadata":{"id":"c03404cd-d681-46cd-b91a-1d1784eefcee"},"outputs":[],"source":["!nvidia-smi topo -m"]},{"cell_type":"markdown","id":"fa2c68ba-47c7-4f75-ba06-2244fc1ce936","metadata":{"id":"fa2c68ba-47c7-4f75-ba06-2244fc1ce936"},"source":["Check out the type of allocated CPU"]},{"cell_type":"code","execution_count":null,"id":"77ce24e3-22d0-4cd8-9691-132259ef2bdf","metadata":{"id":"77ce24e3-22d0-4cd8-9691-132259ef2bdf"},"outputs":[],"source":["!lscpu"]},{"cell_type":"markdown","source":["### Import required packages"],"metadata":{"id":"kEnCy_VAd1pX"},"id":"kEnCy_VAd1pX"},{"cell_type":"code","source":["import numpy as np\n","import cupy as cp\n","import math\n","from time import perf_counter\n","import time\n","import cupy\n","import numpy\n","import os\n","from time import time\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import IPython\n","from IPython.display import Image, display"],"metadata":{"id":"BYWwJFCdd6BG"},"id":"BYWwJFCdd6BG","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c50a5376-6bd0-4d23-b61a-7a2954abab6c","metadata":{"id":"c50a5376-6bd0-4d23-b61a-7a2954abab6c"},"source":["### Introduction to CuPy"]},{"cell_type":"markdown","id":"2cbcc303-3c50-45c6-82f1-e7068e6d0665","metadata":{"id":"2cbcc303-3c50-45c6-82f1-e7068e6d0665"},"source":["NumPy is a widely used library for numerical computing in Python.\n","  - it will measure the execution time of the QR decomposition operation (np.linalg.qr(A)) and provide with timing information based on the number of iterations specified (-n 5 in the following code cell). This is useful for comparing the performance of different code implementations or algorithms."]},{"cell_type":"code","execution_count":null,"id":"dcf823dc-3fef-431f-9abb-ee827c0cf838","metadata":{"id":"dcf823dc-3fef-431f-9abb-ee827c0cf838"},"outputs":[],"source":["size = 512\n","\n","A = np.random.randn(size, size)\n","\n","%timeit -n 5 Q, R = np.linalg.qr(A)"]},{"cell_type":"markdown","id":"0c7dcb8c-7141-46f3-bd08-ea07261e9e65","metadata":{"id":"0c7dcb8c-7141-46f3-bd08-ea07261e9e65"},"source":["CuPy uses a NumPy-like interface. Porting a Numpy code to CuPy can be as simple as changing your import statement. In this workshop, we'll always use `import cupy as cp` for clarity."]},{"cell_type":"code","execution_count":null,"id":"95d196d0-c618-4805-863a-3b57e873d807","metadata":{"id":"95d196d0-c618-4805-863a-3b57e873d807"},"outputs":[],"source":["size = 512\n","\n","A = cp.random.randn(size, size)\n","\n","Q, R = cp.linalg.qr(A)\n","%timeit -n 5 Q, R = cp.linalg.qr(A) ; cp.cuda.Device().synchronize()"]},{"cell_type":"markdown","id":"e954a9de-3519-400e-8a44-d59668bd26f2","metadata":{"id":"e954a9de-3519-400e-8a44-d59668bd26f2"},"source":["We already see a substantial speedup with no real code changes!\n","\n","Notice the additional call to `cp.cuda.Device().synchronize()` in the CuPy version. GPU kernel calls are asynchronous with respect to the CPU. Our call to `synchronize()` ensures the GPU finishes to completion, so we can accurately measure  the elapsed time. We don't generally need to add these calls to production CuPy codes.\n","\n","NumPy is typically used to perform computations on _arrays_ of data. The data is stored in the `numpy.ndarray` object. CuPy implements a similar class called the `cupy.ndarray`. But while the `numpy.ndarray` data resides in host memory, the contents of a `cupy.ndarray` persistent in GPU memory. CuPy provides several helper functions to convert between Cupy and NumPy `ndarrays` - facilitating data transfer to/from the GPU device."]},{"cell_type":"code","execution_count":null,"id":"eca4b14c-fe6e-4ba7-82fc-0c50219e1358","metadata":{"id":"eca4b14c-fe6e-4ba7-82fc-0c50219e1358"},"outputs":[],"source":["#Initialize the data on the host\n","A_cpu = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n","\n","print(\"A_cpu is a\", type(A_cpu))\n","print(\"With initial values:\\n\", A_cpu)\n","\n","#Copy data, host to device\n","A_gpu = cp.asarray(A_cpu)\n","print(\"A_gpu is a\", type(A_gpu))\n","\n","#Square the data on the device\n","A_gpu = cp.square(A_gpu)\n","\n","#Copy data, device to host\n","A_cpu = cp.asnumpy(A_gpu)\n","\n","print(\"Squared values:\\n\", A_cpu)\n"]},{"cell_type":"markdown","id":"3570d8dd-7f54-4d65-83b7-6cc38d6b4cc2","metadata":{"id":"3570d8dd-7f54-4d65-83b7-6cc38d6b4cc2"},"source":["Note that NumPy and CuPy ndarrys are not implicitly convertible."]},{"cell_type":"code","execution_count":null,"id":"f7eacf74-5f07-42b0-9b6f-8af7a302d25a","metadata":{"id":"f7eacf74-5f07-42b0-9b6f-8af7a302d25a"},"outputs":[],"source":["#cp.square(A_cpu)\n","cp.square(A_gpu)"]},{"cell_type":"markdown","id":"204ba905-52c8-44af-9bea-461cd63e13c4","metadata":{"id":"204ba905-52c8-44af-9bea-461cd63e13c4"},"source":["CuPy is useful for programming multi-GPU nodes as well. We can orchestrate computation, data movement, and other low-level CUDA operations with functions in the `cupy.cuda` namespace."]},{"cell_type":"code","execution_count":null,"id":"d968a53d-3a98-4c2b-92b8-a780f5949d03","metadata":{"id":"d968a53d-3a98-4c2b-92b8-a780f5949d03"},"outputs":[],"source":["#Initialize array on GPU 0\n","with cp.cuda.Device(0):\n","    A_gpu_1 = cp.array([[1, 2, 3], [4, 5, 6]], cp.int32)\n","\n","# Synchronize devices\n","cp.cuda.Device().synchronize()\n","#Copy array from A_gpu_1 to A_gpu_0\n","A_gpu_0 = cp.asarray(A_gpu_1)\n","\n","print(A_gpu_0)\n"]},{"cell_type":"markdown","id":"fc84591b-2329-44a8-a266-baa30866086b","metadata":{"id":"fc84591b-2329-44a8-a266-baa30866086b"},"source":["The GPU is a powerhouse of parallel computing performance, and can process math operations much more quickly than the CPU. This is easy to see by comparing performance of CuPy vs NumPy, particularly for dense linear algebra operations. Let's look at a multiplication of 4096x4096 matrices. Notice the similarity of the two versions of the code (NumPy and CuPy)."]},{"cell_type":"code","execution_count":null,"id":"3c64595b-a9b7-43c4-a5ed-87b579aca4b6","metadata":{"id":"3c64595b-a9b7-43c4-a5ed-87b579aca4b6"},"outputs":[],"source":["size = 4096\n","\n","start_time = perf_counter( )\n","A_cpu = np.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(np.float32)\n","B_cpu = np.random.uniform(low=-1., high=1., size=(size,size) ).astype(np.float32)\n","C_cpu = np.matmul(A_cpu,B_cpu)\n","stop_time = perf_counter( )\n","\n","print('')\n","print('    Elapsed wall clock time for numpy = %g seconds.' % (stop_time - start_time) )\n","print('')\n","\n","del A_cpu\n","del B_cpu\n","del C_cpu\n","\n","\n","\n","A_gpu = cp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(cp.float32)\n","B_gpu = cp.random.uniform(low=-1., high=1., size=(size,size) ).astype(cp.float32)\n","C_gpu = cp.matmul(A_gpu,B_gpu) #Exclude one-time JIT overhead\n","start_time = perf_counter( )\n","C_gpu = cp.matmul(A_gpu,B_gpu)\n","cp.cuda.Device(0).synchronize()\n","stop_time = perf_counter( )\n","\n","print('')\n","print('    Elapsed wall clock time for cupy = %g seconds.' % (stop_time - start_time) )\n","print('')\n","\n","del A_gpu\n","del B_gpu\n","del C_gpu"]},{"cell_type":"markdown","id":"c1d9b000-e8fa-4f67-a219-4640aea54abf","metadata":{"id":"c1d9b000-e8fa-4f67-a219-4640aea54abf"},"source":["The GPU's strenghts in computational throughput and memory bandwidth can lead to terrific application speedups. But we need to be considerate of two types of overhead when evaluating our problem for acceleration on the GPU with CuPy: kernel overhead, and data movement overhead."]},{"cell_type":"markdown","source":["### Raw kernels"],"metadata":{"id":"zZMgzEhgltvl"},"id":"zZMgzEhgltvl"},{"cell_type":"markdown","source":["Raw kernels can be defined by the RawKernel class. By using raw kernels, you can define kernels from raw CUDA source.\n","\n","RawKernel object allows you to call the kernel with CUDA’s 'cuLaunchKernel' interface. In other words, you have control over grid size, block size, shared memory size and stream."],"metadata":{"id":"YZZfDAJ4l0cQ"},"id":"YZZfDAJ4l0cQ"},{"cell_type":"code","source":["add_kernel = cp.RawKernel(r'''\n","extern \"C\" __global__\n","void my_add(const float* x1, const float* x2, float* y) {\n","    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n","    y[tid] = x1[tid] + x2[tid];\n","}\n","''', 'my_add')\n","x1 = cp.arange(25, dtype=cp.float32).reshape(5, 5)\n","x2 = cp.arange(25, dtype=cp.float32).reshape(5, 5)\n","y = cp.zeros((5, 5), dtype=cp.float32)\n","add_kernel((5,), (5,), (x1, x2, y))  # grid, block and arguments\n","y"],"metadata":{"id":"UjMPrZYCmEde"},"id":"UjMPrZYCmEde","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Raw kernels operating on complex-valued arrays can be created as well:"],"metadata":{"id":"3VNhUph7mPI5"},"id":"3VNhUph7mPI5"},{"cell_type":"code","source":["complex_kernel = cp.RawKernel(r'''\n","#include <cupy/complex.cuh>\n","extern \"C\" __global__\n","void my_func(const complex<float>* x1, const complex<float>* x2,\n","             complex<float>* y, float a) {\n","    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n","    y[tid] = x1[tid] + a * x2[tid];\n","}\n","''', 'my_func')\n","x1 = cupy.arange(25, dtype=cupy.complex64).reshape(5, 5)\n","x2 = 1j*cupy.arange(25, dtype=cupy.complex64).reshape(5, 5)\n","y = cupy.zeros((5, 5), dtype=cupy.complex64)\n","complex_kernel((5,), (5,), (x1, x2, y, cupy.float32(2.0)))  # grid, block and arguments\n","y"],"metadata":{"id":"VttwjoSbmUjb"},"id":"VttwjoSbmUjb","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Kernel Overhead"],"metadata":{"id":"Ozq5HG5_faHN"},"id":"Ozq5HG5_faHN"},{"cell_type":"markdown","source":["CuPy compiles kernel codes on-the-fly using JIT compilation. Therefore, there is a compilation overhead the first time a given function is called with CuPy. The compiled kernel code is cached, so compilation overhead is avoided for subsequent executions of the function."],"metadata":{"id":"nwXqdIEdfges"},"id":"nwXqdIEdfges"},{"cell_type":"code","execution_count":null,"id":"ae1a0d2b-da82-4e46-999c-55bbad4c0517","metadata":{"id":"ae1a0d2b-da82-4e46-999c-55bbad4c0517"},"outputs":[],"source":["size = 512\n","for _ in range(5):\n","    A = cp.random.randn(size, size).astype(np.float32)\n","    t1 = time()\n","    cp.linalg.det(A)\n","    cp.cuda.Device().synchronize()\n","    t2 = time()\n","    print('%.4f' % (t2 - t1))"]},{"cell_type":"markdown","id":"827d8d9b-e4b2-4376-a441-b6bbf9a78af8","metadata":{"id":"827d8d9b-e4b2-4376-a441-b6bbf9a78af8"},"source":["You may also notice a one-time overhead upon first calling a CuPy function in a program. This overhead is associated with the creation of a CUDA context by the CUDA driver, which happens the first time any CUDA API is invoked in a program.\n","\n","In addition, there is a CUDA kernel launch overhead that is penalized each time a GPU kernel is launched. The overhead is on the order of a few microseconds. For this reason, launching many small CUDA kernels in an application will generally lead to poor performance. The kernel launch overhead may dominate your runtime for very small problems, but for large datasets the overhead will be small compared to the actual GPU computation work."]},{"cell_type":"code","execution_count":null,"id":"2061b6b0-169f-4e64-8556-b3fd86b97dca","metadata":{"id":"2061b6b0-169f-4e64-8556-b3fd86b97dca"},"outputs":[],"source":["for size in [64, 128, 256, 512, 1024, 2048]:\n","    print(\"\\nInput Matrix size: %d\" % size, \"x %d \" % size)\n","    for xp in [np, cp]:\n","        A=xp.random.uniform(low=-1.0, high=1.0, size=(size,size) ).astype(xp.float32)\n","        xp.linalg.qr(A)#Exclude potential one-time JIT overhead\n","        t1 = time()\n","        xp.linalg.qr(A)\n","        cp.cuda.Device().synchronize()\n","        t2 = time()\n","        print(xp.__name__, '%f' % (t2 - t1))\n","        del A"]},{"cell_type":"markdown","id":"ff335660-dfa6-4657-9489-7d31bb491d93","metadata":{"id":"ff335660-dfa6-4657-9489-7d31bb491d93"},"source":["It's clear that increasing the problem size can help amoritize the overhead of launching GPU kernels. Another common strategy is to merge multiple kernels together into a single combined kernel, reducing the total number of kernel launches in your program. CuPy supports kernel fusion in this manner via the `@cupy.fuse()` decorator.\n","\n","  - The following code is comparing the performance of the regular squared_diff function and the fused fused_squared_diff function in terms of execution time when applied to CuPy arrays x and y.\n","  - The fused function is expected to have better performance due to optimization for GPU execution.\n","  - The %timeit magic command is used to run each operation multiple times and measure the average execution time."]},{"cell_type":"code","execution_count":null,"id":"689b1c62-62f6-4da9-a3de-3e5afd5beda6","metadata":{"id":"689b1c62-62f6-4da9-a3de-3e5afd5beda6"},"outputs":[],"source":["def squared_diff(x, y):\n","    return (x - y) * (x - y)\n","\n","@cp.fuse\n","def fused_squared_diff(x, y):\n","    return (x - y) * (x - y)\n","\n","size = 10000\n","\n","x = cp.arange(size)\n","y = cp.arange(size)[::-1]\n","\n","%timeit -n 10 squared_diff(x, y); cp.cuda.Device().synchronize()\n","%timeit -n 10 fused_squared_diff(x, y); cp.cuda.Device().synchronize()\n","\n","del x\n","del y\n"]},{"cell_type":"markdown","source":["### Streams and Events"],"metadata":{"id":"yBhGzv-mowgB"},"id":"yBhGzv-mowgB"},{"cell_type":"markdown","source":["In this section we discuss basic usages for CUDA streams and events. For the API reference please see [Streams and events](https://docs.cupy.dev/en/stable/reference/cuda.html#stream-event-api).\n","\n","CuPy provides high-level Python APIs Stream and Event for creating streams and events, respectively. Data copies and kernel launches are enqueued onto the Current Stream, which can be queried via get_current_stream() and changed either by setting up a context manager:"],"metadata":{"id":"3SRDKR-Zo4Fy"},"id":"3SRDKR-Zo4Fy"},{"cell_type":"code","source":["a_np = np.arange(10)\n","s = cp.cuda.Stream()\n","with s:\n","    a_cp = cp.asarray(a_np)  # H2D transfer on stream s\n","    b_cp = cp.sum(a_cp)      # kernel launched on stream s\n","    assert s == cp.cuda.get_current_stream()\n","\n","# fall back to the previous stream in use (here the default stream)\n","# when going out of the scope of s"],"metadata":{"id":"cVQ0y4K3phu6"},"id":"cVQ0y4K3phu6","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["or by using the use() method:"],"metadata":{"id":"L9ReIyJjpo5D"},"id":"L9ReIyJjpo5D"},{"cell_type":"code","source":["s = cp.cuda.Stream()\n","s.use()  # any subsequent operations are done on steam s\n","\n","b_np = cp.asnumpy(b_cp)\n","assert s == cp.cuda.get_current_stream()\n","cp.cuda.Stream.null.use()  # fall back to the default (null) stream\n","\n","assert cp.cuda.Stream.null == cp.cuda.get_current_stream()"],"metadata":{"id":"8A6N_DQFpvkM"},"id":"8A6N_DQFpvkM","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Events can be created either manually or through the record() method.\n","- Event objects can be used for timing GPU activities (via get_elapsed_time()) or setting up inter-stream dependencies:"],"metadata":{"id":"7eA1Vh5xp_sB"},"id":"7eA1Vh5xp_sB"},{"cell_type":"code","source":["e1 = cp.cuda.Event()\n","e1.record()\n","a_cp = b_cp * a_cp + 8\n","e2 = cp.cuda.get_current_stream().record()\n","\n","# set up a stream order\n","s2 = cp.cuda.Stream()\n","s2.wait_event(e2)\n","with s2:\n","    # the a_cp is guaranteed updated when this copy (on s2) starts\n","    a_np = cp.asnumpy(a_cp)\n","\n","# timing\n","e2.synchronize()\n","t = cp.cuda.get_elapsed_time(e1, e2)  # only include the compute time, not the copy time\n","print('Compute time: %.4f' % (t2 - t1))"],"metadata":{"id":"yQ1mv5ddqK7D"},"id":"yQ1mv5ddqK7D","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Just like the Device objects, Stream and Event objects can also be used for synchronization."],"metadata":{"id":"freXE5awqbry"},"id":"freXE5awqbry"},{"cell_type":"markdown","source":["### Data Movement Overhead"],"metadata":{"id":"Bwlf0koOfn18"},"id":"Bwlf0koOfn18"},{"cell_type":"markdown","id":"b36e7785-3b82-4e6e-9742-76b1caed35c3","metadata":{"id":"b36e7785-3b82-4e6e-9742-76b1caed35c3"},"source":["Try to minimize data movement to or from the GPU. The FLOP rate and memory bandwidth of a GPU can process data much more quickly than it can be fed with data over the PCIe bus. This problem is being tackled with novel interconnect technologies like NVLink. But it's a real inbalance we have to deal with for now.\n","Let's look at an example where we initialize our input data GPU and then computes the dot product. Note that the result of the multiplication, the C matrix, is available on the GPU in case we need it later.\n","\n","Notice again the similarity of the two parts of the code (NumPy and CuPy). They are virtually identical."]},{"cell_type":"code","execution_count":null,"id":"0ebb0179-8137-4827-8eb2-2c960967593c","metadata":{"id":"0ebb0179-8137-4827-8eb2-2c960967593c"},"outputs":[],"source":["size = int(1e8)\n","\n","for i in range(3):\n","    print(\"Iteration \", i)\n","    start_time = perf_counter( )\n","    A_cpu=np.random.rand(size).astype(np.float32)\n","    B_cpu=np.random.rand(size).astype(np.float32)\n","    C_cpu = np.dot(A_cpu,B_cpu)\n","    stop_time = perf_counter( )\n","    cpu_time = stop_time - start_time\n","    print('numpy = %g seconds' % cpu_time )\n","\n","    start_time = perf_counter( )\n","    A_gpu=cp.random.rand(size).astype(cp.float32)\n","    B_gpu=cp.random.rand(size).astype(cp.float32)\n","    C_gpu = cp.dot(A_gpu,B_gpu)\n","    cp.cuda.Device(0).synchronize()\n","    stop_time = perf_counter( )\n","    gpu_time = stop_time - start_time\n","\n","    print('cupy = %g seconds' % gpu_time )\n","    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n","    print('')"]},{"cell_type":"markdown","id":"087ea831-f9e3-41c1-888d-8fed77d9fd1f","metadata":{"id":"087ea831-f9e3-41c1-888d-8fed77d9fd1f"},"source":["But what if the input data for the `dot` operation resides in the system memory? We need to move the data over the PCIe bus (from the host to the GPU) using `cp.asarray()`.\n","\n","Modify the following cell to initialize the ndarray data with Numpy.\n","\n","How does the speedup change after the additional cost of data movement?"]},{"cell_type":"code","execution_count":null,"id":"1be63e60-fb09-4936-886d-7e91aebacd16","metadata":{"id":"1be63e60-fb09-4936-886d-7e91aebacd16"},"outputs":[],"source":["size = int(1e8)\n","\n","for i in range(3):\n","    print(\"Iteration \", i)\n","    start_time = perf_counter( )\n","    A_cpu=np.random.rand(size).astype(np.float32)\n","    B_cpu=np.random.rand(size).astype(np.float32)\n","\n","    # Start time\n","    start_time = perf_counter( )\n","    # Stop time\n","    stop_time = perf_counter( )\n","    gpu_time = stop_time - start_time\n","\n","    print('cupy = %g seconds' % gpu_time )\n","    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n","    print('')"]},{"cell_type":"markdown","id":"72fc692d-137a-44ab-8743-3a2fa3ff60bc","metadata":{"id":"72fc692d-137a-44ab-8743-3a2fa3ff60bc"},"source":["Let's look into the following code cell to reveal the solution."]},{"cell_type":"code","execution_count":null,"id":"47a42a53-e98c-4434-95ab-2114ee4d6dcd","metadata":{"jupyter":{"source_hidden":true},"tags":[],"id":"47a42a53-e98c-4434-95ab-2114ee4d6dcd"},"outputs":[],"source":["size = int(1e8)\n","\n","for i in range(3):\n","    print(\"Iteration \", i)\n","\n","    start_time = perf_counter( )\n","\n","    A_cpu=np.random.rand(size).astype(np.float32)\n","    B_cpu=np.random.rand(size).astype(np.float32)\n","\n","    A_gpu=cp.asarray(A_cpu)\n","    B_gpu=cp.asarray(B_cpu)\n","    C_gpu = cp.dot(A_gpu,B_gpu)\n","    cp.cuda.Device(0).synchronize()\n","\n","    stop_time = perf_counter( )\n","    gpu_time = stop_time - start_time\n","\n","    print('cupy = %g seconds' % gpu_time )\n","    print(\"Speedup = %.2f\" % (cpu_time/gpu_time))\n","    print('')\n"]},{"cell_type":"markdown","source":["### Managing GPU Memory"],"metadata":{"id":"E4zckNhgfwpB"},"id":"E4zckNhgfwpB"},{"cell_type":"markdown","id":"332e8fff-90ec-4af3-b453-d1a97d83c86f","metadata":{"id":"332e8fff-90ec-4af3-b453-d1a97d83c86f"},"source":["Modern datacenter GPUs have as much as 80GB of high-bandwidth memory on a single accelerator. But in general, the host system memory will have a larger capacity. We need to be conscious of GPU memory limitations when transfering data from the host. We can query the amount of free and total memory with nvidia-smi:"]},{"cell_type":"markdown","source":["**Memory Pool Operations:**\n","  - cupy.get_default_memory_pool(): It helps to get the default memory pool for CuPy, which manages memory allocation on the GPU.\n","  - cupy.get_default_pinned_memory_pool(): This function helps to get the default pinned memory pool for CuPy, which manages pinned (page-locked) memory on the host (CPU) that can be quickly transferred to and from the GPU."],"metadata":{"id":"OSnAoTjTzz4P"},"id":"OSnAoTjTzz4P"},{"cell_type":"code","source":["mempool = cupy.get_default_memory_pool()\n","pinned_mempool = cupy.get_default_pinned_memory_pool()\n","\n","# Create an array on CPU.\n","# NumPy allocates 400 bytes in CPU (not managed by CuPy memory pool).\n","a_cpu = numpy.ndarray(100, dtype=numpy.float32)\n","print(a_cpu.nbytes)                      # 400\n","\n","# You can access statistics of these memory pools.\n","print(mempool.used_bytes())              # 0\n","print(mempool.total_bytes())             # 0\n","print(pinned_mempool.n_free_blocks())    # 0"],"metadata":{"id":"_MOIT7DzzTkZ"},"id":"_MOIT7DzzTkZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transfer the array from CPU to GPU.\n","# This allocates 400 bytes from the device memory pool, and another 400\n","# bytes from the pinned memory pool.  The allocated pinned memory will be\n","# released just after the transfer is complete.  Note that the actual\n","# allocation size may be rounded to larger value than the requested size\n","# for performance.\n","a = cupy.array(a_cpu)\n","print(a.nbytes)                          # 400\n","print(mempool.used_bytes())              # 512\n","print(mempool.total_bytes())             # 512\n","print(pinned_mempool.n_free_blocks())    # 1"],"metadata":{"id":"GZzgzB5ezhd2"},"id":"GZzgzB5ezhd2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# When the array goes out of scope, the allocated device memory is released\n","# and kept in the pool for future reuse.\n","a = None  # (or `del a`)\n","print(mempool.used_bytes())              # 0\n","print(mempool.total_bytes())             # 512\n","print(pinned_mempool.n_free_blocks())    # 1"],"metadata":{"id":"0pMkRJVizoR3"},"id":"0pMkRJVizoR3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Clear the memory pool**"],"metadata":{"id":"RhwzX-OecY6G"},"id":"RhwzX-OecY6G"},{"cell_type":"code","source":["# You can clear the memory pool by calling `free_all_blocks`.\n","mempool.free_all_blocks()\n","pinned_mempool.free_all_blocks()\n","print(mempool.used_bytes())              # 0\n","print(mempool.total_bytes())             # 0\n","print(pinned_mempool.n_free_blocks())    # 0"],"metadata":{"id":"E4ovIUJ1ztD-"},"id":"E4ovIUJ1ztD-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(cupy.get_default_memory_pool().get_limit())"],"metadata":{"id":"WGX8bH2t0SKd"},"id":"WGX8bH2t0SKd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mempool = cupy.get_default_memory_pool()\n","\n","with cupy.cuda.Device(0):\n","    mempool.set_limit(size=1024**3)  # 1 GiB"],"metadata":{"id":"IkCIahSe0g9O"},"id":"IkCIahSe0g9O","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting the locale to UTF-8\n","# !export LC_ALL=C.UTF-8\n","# !export LANG=C.UTF-8"],"metadata":{"id":"Ciww13qqhap_"},"id":"Ciww13qqhap_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Running nvidia-smi command to query GPU memory\n","# !nvidia-smi -i 0 --query-gpu=memory.free,memory.total --format=csv"],"metadata":{"id":"TclLDzYoheUk"},"id":"TclLDzYoheUk","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"7f18ee84-662a-4611-b142-0844ff46bb0b","metadata":{"id":"7f18ee84-662a-4611-b142-0844ff46bb0b"},"source":["Or natively with CuPy"]},{"cell_type":"code","execution_count":null,"id":"fbbc8213-3cad-4d2b-a6dc-c345db8c1ec4","metadata":{"id":"fbbc8213-3cad-4d2b-a6dc-c345db8c1ec4"},"outputs":[],"source":["print(\"GPU (free, total) memory in bytes:\")\n","print(cp.cuda.Device().mem_info)"]},{"cell_type":"markdown","source":["**Clear all GPU memory**"],"metadata":{"id":"l7-TlDKbckiW"},"id":"l7-TlDKbckiW"},{"cell_type":"markdown","id":"038935eb-bce2-406e-9a4e-a07b23764c08","metadata":{"id":"038935eb-bce2-406e-9a4e-a07b23764c08"},"source":["Let's clear all GPU memory for good measure."]},{"cell_type":"code","execution_count":null,"id":"e2e4d096-f36f-43eb-bdc7-7af50327342b","metadata":{"id":"e2e4d096-f36f-43eb-bdc7-7af50327342b"},"outputs":[],"source":["cp.get_default_memory_pool().free_all_blocks()\n","\n","print(\"GPU (free, total) memory in bytes:\")\n","print(cp.cuda.Device().mem_info)"]},{"cell_type":"markdown","id":"7659485d-679c-49f3-81ec-4be29295fe9d","metadata":{"id":"7659485d-679c-49f3-81ec-4be29295fe9d"},"source":["What happens if we try to allocate too much space on the GPU? In the following example, arrays A and B are 8GB each."]},{"cell_type":"code","execution_count":null,"id":"1e011c0b-1b76-4320-bd09-9769379c543f","metadata":{"id":"1e011c0b-1b76-4320-bd09-9769379c543f"},"outputs":[],"source":["size = 32768\n","try:\n","  A = cp.ones((size, size))\n","  B = cp.ones((size, size))\n","except cp.cuda.memory.OutOfMemoryError as e:\n","  print(f\"Error: {e}\")"]},{"cell_type":"markdown","id":"6ec8d6e1-240c-4883-8462-4e1dab976ca5","metadata":{"id":"6ec8d6e1-240c-4883-8462-4e1dab976ca5"},"source":["One possible solution is to switch over to unified memory. With unified memory, the CUDA runtime will migrate data between the CPU and GPU _on demand_. Data migrations are triggered by page faults, so we may be leaving some performance on the table by using unified memory instead of managing memory explicitly. But it's an extremely convenient feature for making GPUs easier to program. We can enable Unified Memory in CuPy as follows:"]},{"cell_type":"code","execution_count":null,"id":"8c121f48-b01e-4b17-a83c-3210472ae678","metadata":{"id":"8c121f48-b01e-4b17-a83c-3210472ae678"},"outputs":[],"source":["#Create a memory pool instance with malloc_managed allocator\n","pool = cp.cuda.MemoryPool(cp.cuda.malloc_managed)\n","cp.cuda.set_allocator(pool.malloc)"]},{"cell_type":"markdown","id":"21b0a2ab-da68-49df-b712-a7cbba943c3b","metadata":{"id":"21b0a2ab-da68-49df-b712-a7cbba943c3b"},"source":["Let's try that again (with reduced size = 8768)"]},{"cell_type":"code","execution_count":null,"id":"cf2ad44e-dcc0-475f-a085-b770dd105ced","metadata":{"id":"cf2ad44e-dcc0-475f-a085-b770dd105ced"},"outputs":[],"source":["size = 8768\n","A = cp.ones((size, size))\n","B = cp.ones((size, size))"]},{"cell_type":"markdown","id":"c58382e8-2e73-488c-b318-6974deb80eba","metadata":{"id":"c58382e8-2e73-488c-b318-6974deb80eba"},"source":["We can certainly perform computations on these new arrays. Performance will take a hit as the GPU swaps pages in-and-out of memory"]},{"cell_type":"code","execution_count":null,"id":"e27b8daa-ef79-42f6-ac3a-7213f7fbb99c","metadata":{"id":"e27b8daa-ef79-42f6-ac3a-7213f7fbb99c"},"outputs":[],"source":["cp.add(A,B)"]},{"cell_type":"code","source":["# Set the locale to UTF-8\n","# os.environ['LC_ALL'] = 'C.UTF-8'\n","# os.environ['LANG'] = 'C.UTF-8'"],"metadata":{"id":"Wh7-p5yCvwcl"},"id":"Wh7-p5yCvwcl","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !apt-get install -y locales"],"metadata":{"id":"wdQ7yTyKrehn"},"id":"wdQ7yTyKrehn","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mandelbrot plot"],"metadata":{"id":"7HDuAtQotN1i"},"id":"7HDuAtQotN1i"},{"cell_type":"markdown","source":["We are creating a Mandelbrot plot using CuPy. It involves generating a grid of complex numbers representing points in the complex plane.\n","It iterates through each point to determine its membership in the Mandelbrot set, and then it plots the results. CuPy provides efficient GPU-accelerated computation for such tasks, making it ideal for handling large datasets and speeding up the computation process."],"metadata":{"id":"zqdddxtohiQl"},"id":"zqdddxtohiQl"},{"cell_type":"code","source":["# Set the backend to Agg to prevent figure pop-ups\n","matplotlib.use('Agg') #this will prevent the figure from popping up\n","\n","# Initialize CuPy for GPU computation\n","cp.cuda.Device(0).use()\n","\n","# Create CuPy arrays for GPU computation\n","x_gpu = cp.arange(0, 10, 0.1)\n","y_gpu = cp.sin(x_gpu)\n","\n","# Transfer data back to CPU for plotting\n","x_cpu = cp.asnumpy(x_gpu)\n","y_cpu = cp.asnumpy(y_gpu)\n","\n","# Plot using Matplotlib\n","plt.plot(x_cpu, y_cpu)\n","plt.xlabel('x')\n","plt.ylabel('sin(x)')\n","plt.title('Sin(x) Plot using CuPy and Matplotlib')\n","plt.savefig('sin_plot.png')  # Save the plot as an image file\n","plt.close()  # Close the plot to prevent display"],"metadata":{"id":"cv37qR8hgWnV"},"id":"cv37qR8hgWnV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Optional: Load and display the saved image**"],"metadata":{"id":"0CLRvrlhg6j9"},"id":"0CLRvrlhg6j9"},{"cell_type":"code","source":["# Specify the file path\n","image_path = '/content/sin_plot.png'\n","\n","# Display the image\n","display(Image(filename=image_path))"],"metadata":{"id":"NQYhvExOx2hz"},"id":"NQYhvExOx2hz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Using cp.sin(x_gpu), we calculate the sine values of each element in the x_gpu array directly on the GPU.\n","- It results in the y_gpu array containing corresponding y-axis values (sin(x)) computed on the GPU.\n","- Since Matplotlib operates on CPU, we use cp.asnumpy() to transfer the x_gpu and y_gpu arrays back to the CPU as NumPy arrays.\n","- With the data now on the CPU, we can use Matplotlib to create a plot.\n","- Using Matplotlib, we are plotting the sinusoidal curve with x_cpu on the x-axis and y_cpu on the y-axis to visualize the sine function over the specified range.\n","\n","So, the objective of showing this Mandelbrot plot is to make use of CuPy for efficient GPU computation of the sine values, which can be beneficial for large datasets or complex mathematical operations. Transferring the results back to the CPU allows us to utilize the plotting capabilities of Matplotlib for visualization."],"metadata":{"id":"cNYTOImIzxH4"},"id":"cNYTOImIzxH4"},{"cell_type":"markdown","id":"82076ac4-4ec3-4913-a891-87f029760155","metadata":{"id":"82076ac4-4ec3-4913-a891-87f029760155"},"source":["**Please restart the kernel**"]},{"cell_type":"code","execution_count":null,"id":"63cbe283-6890-4758-b8b6-9d5f6d3fdfa6","metadata":{"id":"63cbe283-6890-4758-b8b6-9d5f6d3fdfa6"},"outputs":[],"source":["app = IPython.Application.instance()\n","app.kernel.do_shutdown(True)"]},{"cell_type":"markdown","source":["### Theory Questions"],"metadata":{"id":"K6mCE_SVf63Y"},"id":"K6mCE_SVf63Y"},{"cell_type":"markdown","source":["1. What are CuPy streams and events, and how are they used in GPU programming?\n","\n"," CuPy streams and events are important concepts in GPU programming. The CuPy streams and events help to manage and synchronize parallel operations on the GPU.\n","  - **CuPy Streams:**\n","    \n","    **a.** CuPy Streams represent independent sequences of operations on the GPU. They allow concurrent execution of kernels and memory transfers within a single GPU context.\n","\n","    **b.** By using CuPy Streams, developers can overlap computation and data transfers. It leads to better GPU utilization and performance improvement.\n","\n","    **c.** CuPy Streams are created using cupy.cuda.Stream() and can be used to execute CuPy functions and CUDA kernels asynchronously within the same GPU context.\n","  - **CuPy Events:**\n","\n","    **a.** CuPy Events are synchronization markers that allow precise timing control and synchronization of GPU operations.\n","\n","    **b.** CuPy Events can be used to record and measure time intervals between different GPU events, such as kernel launches or memory transfers.\n","\n","    **c.** CuPy Events are created using cupy.cuda.Event() and they can be used to synchronize streams, wait for kernel completion, and to measure GPU execution time accurately.\n","\n","2. Why Memory Pool Operations are necessary in CuPy programming on GPU?\n","\n"," CuPy uses memory pool for memory allocations by default. The memory pool significantly improves the performance by mitigating the overhead of memory allocation and CPU/GPU synchronization.\n","\n"," There are two different memory pools in CuPy:\n","\n"," - Device memory pool (GPU device memory), which is used for GPU memory allocations.\n"," - Pinned memory pool (non-swappable CPU memory), which is used during CPU-to-GPU data transfer.\n","\n"," The memory pool instance provides statistics about memory allocation. To access the default memory pool instance, we use **cupy.get_default_memory_pool()** and **cupy.get_default_pinned_memory_pool()**. These are significant due to the following reasons:\n","\n"," **Memory Management Efficiency:** Memory pool operations help in efficient management of GPU memory resources. They allocate and deallocate memory blocks in a controlled manner and reduce memory fragmentation. Thereby, Memory pool operations improve overall memory utilization.\n","\n"," **Reduction in Memory Fragmentation:** Continuous allocation and deallocation of GPU memory without memory pooling can lead to memory fragmentation. In that case, the free memory blocks are scattered and unusable for larger allocations. So, under this scenario, the Memory pool operations help to mitigate fragmentation by managing memory blocks more effectively.\n","\n"," **Optimized Memory Reuse:** Memory pools allow for optimized reuse of memory blocks. Instead of repeatedly allocating and releasing memory from the GPU, the Memory pool operations retain and recycle memory blocks within a pool. So, this reduces the overhead of memory allocation.\n","\n"," **Prevents Memory Leaks:** Proper memory pool management helps the developers to prevent memory leaks by ensuring that all allocated memory blocks are properly tracked and released when no longer needed.\n","\n"," **Solves Resource Contention:** In multi-threaded or concurrent GPU applications, the Memory pool operations help to mitigate resource contention by providing controlled access to memory blocks."],"metadata":{"id":"uvSQ_TdvgBwf"},"id":"uvSQ_TdvgBwf"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}