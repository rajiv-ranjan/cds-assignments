{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajiv-ranjan/cds-assignments/blob/main/m6/AST%203/M6_AST_03_LangChain_with_Open_Source_LLMs_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMNv0S7qBl1Z"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 3: Open Source LLMs with LangChain ðŸ¦œðŸ”—"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7nOHNxJqC2X"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* use open source LLMs: **`zephyr-7b-beta`**, **`Mistral-7B-Instruct-v0.2`**,  and **`Llama2`** through HuggingFaceHub with LangChain\n",
        "* understand & use the concept of Prompt template, Memory and output parsers in LangChain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQ0Fvl_jNqU"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dG904xmKyK31"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2418709\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UOc9lw37yK4A"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9902028293\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "g9aMvFszyK4A",
        "outputId": "c1a59d17-9019-4a23-95fa-ab009e592b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2418709&recordId=5994\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M6_AST_03_LangChain_with_Open_Source_LLMs_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWUXHwdSLFMA"
      },
      "source": [
        "### Steps for Creating Hugging Face access tokens:\n",
        "\n",
        "* **Visit the Hugging Face Website:** Head to the Hugging Face website (https://huggingface.co/) to begin the account creation process.\n",
        "\n",
        "* **Click on â€œSign Upâ€:** Locate the â€œSign Upâ€ button on the top right corner of the homepage and click on it.\n",
        "\n",
        "* **Choose a Sign-Up Method:** Hugging Face offers multiple sign-up methods, including Google, GitHub, and email. Select your preferred method and follow the prompts to complete the registration.\n",
        "\n",
        "* **Verify Your Email (if applicable):** If you choose to sign up via email, verify your email address by clicking on the confirmation link sent to your inbox.\n",
        "\n",
        "* **Complete Your Profile:** Enhance your Hugging Face experience by completing your profile. Add a profile picture, a short bio, and any other details youâ€™d like to share with the community.\n",
        "\n",
        "* **Create Your Access Token:** Go to the link (https://huggingface.co/settings/tokens)\n",
        "\n",
        "* **Click on the option 'Access Tokens' from the left pane.**\n",
        "\n",
        "* **Then under the User Access Tokens, click on the button 'New token'.**\n",
        "\n",
        "* **Select Token type: Write.**\n",
        "\n",
        "* **Put your desired Token name.**\n",
        "\n",
        "* **Then at the bottom end, click on 'Create token'. The Hugging Face access token will be generated. Copy and paste the access token in your Google Colab Notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeXRnNMfnFqF"
      },
      "source": [
        "### Install required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aKLiNSv6wTr6"
      },
      "outputs": [],
      "source": [
        "# Langchain\n",
        "!pip -q install langchain\n",
        "\n",
        "# Library to communicate with HF hub\n",
        "!pip -q install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UAsnCf4Z9e1K"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BrFlLJv3-_4N"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain_huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0sgU1wiq7Hw"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u0IydGx_q78h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMBDqFT0kkgH"
      },
      "source": [
        "### **Provide your HuggingFace api key/access token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "i9TSeu4cq1Jw",
        "outputId": "8791c030-eb1e-46ff-d6a3-45bfcc797401",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your HuggingFace access token: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "# Enter your HuggingFace access token when prompted\n",
        "\n",
        "pass_token = getpass(\"Enter your HuggingFace access token: \")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = pass_token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = pass_token\n",
        "\n",
        "del pass_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgzDSiux-EuO"
      },
      "source": [
        "### **Exploring Open Source LLMs hosted on HuggingFace**\n",
        "\n",
        ">**I.** `HuggingFaceH4/zephyr-7b-beta`\n",
        ">\n",
        ">**II.** `mistralai/Mistral-7B-Instruct-v0.2`\n",
        ">\n",
        ">**III.** `LlaMa2`\n",
        "\n",
        "[LangChain link](https://python.langchain.com/docs/integrations/chat/huggingface) for using Hugging Face LLM's as chat models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVK3LSt6mzKa"
      },
      "source": [
        "### **I.** [**HuggingFaceH4/zephyr-7b-beta**](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QudSou2y-T4i"
      },
      "outputs": [],
      "source": [
        "# Import HuggingFace model abstraction class from langchain\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jVRB1nztyTUm"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens = 512,\n",
        "    top_k = 30,\n",
        "    temperature = 0.1,\n",
        "    repetition_penalty = 1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qrneNiybIFl_",
        "outputId": "3f0c418a-eed7-4a5d-cfc5-b8b77eb43a3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "1. Start with the basics: Before diving into complex programming concepts, itâ€™s essential to understand the fundamentals of programming languages. Learn the syntax, data types, variables, and control structures of your chosen programming language.\n",
            "\n",
            "2. Practice, practice, practice: The more you code, the better you become. Set aside time each day to work on coding projects, even if theyâ€™re small. This will help you build confidence and develop a routine.\n",
            "\n",
            "3. Collaborate with others: Join online communities, attend meetups, or find a study group to connect with other programmers. Collaborating with others can provide valuable insights, feedback, and learning opportunities.\n",
            "\n",
            "4. Learn from examples: Study existing codebases and try to understand how they work. This can help you learn best practices, common patterns, and how to structure your code.\n",
            "\n",
            "5. Stay up-to-date: Keep up with the latest trends, technologies, and best practices in the programming world. Attend conferences, read blogs, and follow industry leaders on social media to stay informed.\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"How to learn programming? give 5 points\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4N14UjkBaws"
      },
      "source": [
        "#### **Prompt Template**\n",
        "\n",
        "Prompt templates are predefined recipes for generating prompts for language models.\n",
        "\n",
        "A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n",
        "\n",
        "LangChain provides tooling to create and work with prompt templates.\n",
        "\n",
        "To know more about Prompt template, refer [here](https://python.langchain.com/docs/modules/model_io/prompts/quick_start)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v-l3yx9hQs2"
      },
      "source": [
        "#### **Example-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NIjgIglFbjDG",
        "outputId": "18cac776-e044-4c40-a08b-4d2fb207d0d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a funny joke about Trump.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "messages = prompt_template.format(adjective=\"funny\", content=\"Trump\")\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iUOnAu5c_Hqw"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "chat_model = ChatHuggingFace(llm = llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DjM9t0INbjLg",
        "outputId": "d529163a-1f0b-4cc1-938a-2da1d714af64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did Donald Trump order a permanent marker with his takeout from KFC?\n",
            "\n",
            "Because he didn't want \"finger lickin' good\" to become \"finger gREATtin' good\"! \n",
            "\n",
            "(This joke plays off of Trump's famous catchphrase \"Make America Great Again\" and KFC's slogan \"Finger Lickin' Good.\")\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPeo4FQBhL0l"
      },
      "source": [
        "#### **Example-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bat0JTd7GrJx"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mQe8qxghewsF",
        "outputId": "f16e9e4f-6c32-4237-ec3c-4f9f4fca79ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me 5 facts about Tajmahal.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me {count} facts about {event_or_place}.\"\n",
        ")\n",
        "user_msg = prompt_template.format(count=5, event_or_place=\"Tajmahal\")\n",
        "user_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GdGDCoQhmuDL"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You're a knowledgeable historian\"),\n",
        "    HumanMessage(content=user_msg),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vCpfJTPCAqGP"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9kg1GDWsCRbL"
      },
      "outputs": [],
      "source": [
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "27Iyhp-5DJ5-",
        "outputId": "12c169d6-073f-4e41-d395-e09f8d83f810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HuggingFaceH4/zephyr-7b-beta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "chat_model.model_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s_EReM93znzV",
        "outputId": "39e160c2-0555-4e55-a33a-df4c51fbb65b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-21-4138686543.py\", line 2, in <cell line: 0>\n",
            "    from transformers import AutoTokenizer # Import AutoTokenizer\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\", line 27, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\", line 24, in <module>\n",
            "    from .args_doc import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/args_doc.py\", line 30, in <module>\n",
            "    from .generic import ModelOutput\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 46, in <module>\n",
            "    import torch  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1471, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou're a knowledgeable historian</s>\\n<|user|>\\nTell me 5 facts about Tajmahal.</s>\\n<|assistant|>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from langchain_huggingface import ChatHuggingFace\n",
        "from transformers import AutoTokenizer # Import AutoTokenizer\n",
        "\n",
        "# Assume llm is already defined as HuggingFaceEndpoint\n",
        "\n",
        "# Get the tokenizer from the repo_id used by the llm\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm.repo_id) # Get tokenizer from llm's repo_id\n",
        "\n",
        "# Initialize ChatHuggingFace with both llm and tokenizer\n",
        "chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)\n",
        "\n",
        "# Now the chat_model should have a tokenizer and the following line should work\n",
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9niBkmKIDMVI"
      },
      "outputs": [],
      "source": [
        "#chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "px6WPyxN2DYj",
        "outputId": "912312e3-cf88-4046-a83c-ae747f49d8f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You're a knowledgeable historian</s>\n",
            "<|user|>\n",
            "Tell me 5 facts about Tajmahal.</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(chat_model._to_chat_prompt(messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZduaUIkl-VpI",
        "outputId": "67268a1b-6449-4bf7-d232-48fff450ed03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. The Taj Mahal is a mausoleum located in Agra, India, built by Mughal Emperor Shah Jahan in memory of his beloved wife Mumtaz Mahal.\n",
            "2. Construction of the Taj Mahal began in 1632 and took 22 years to complete. It is made primarily of white marble and is an exquisite example of Mughal architecture, a blend of Indian, Persian, and Islamic styles.\n",
            "3. The Taj Mahal is not just a symbol of love but also a testament to the high level of skill and craftsmanship of the Mughal Empire's artisans. The intricate inlay work, known as Pietra Dura, featuring precious and semi-precious gems, is a defining feature of the mausoleum's interior.\n",
            "4. The Taj Mahal is not just a beautiful sight to see during the day but also at night. It is illuminated using a series of spotlights that give it a mystical and ethereal glow.\n",
            "5. The Taj Mahal has survived numerous wars, invasions, and natural disasters over the centuries. It was declared a UNESCO World Heritage site in 1983 and remains a popular tourist attraction, drawing millions of visitors every year.\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USL19U5MKapo"
      },
      "source": [
        "#### **Example-3**\n",
        "\n",
        "The prompt to *chat models* is a list of chat messages.\n",
        "\n",
        "Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jsZWlMZEEcth"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TBLyJ-sFk8Hf"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful A {persona}.\"),\n",
        "        (\"human\", \"Hello, how are you doing?\"),\n",
        "        (\"ai\", \"I'm doing well, thanks!\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1rr0S0bklaIr"
      },
      "outputs": [],
      "source": [
        "persona = \"\"\"trustworthy friend\"\"\"\n",
        "query= \"\"\"\n",
        "I am not able to understand the concept taught in class. \\\n",
        "Could you please suggest something? \\\n",
        "I need your help. Give 5 points to work on.\n",
        "\"\"\"\n",
        "messages = chat_template.format_messages(persona = persona, user_input=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XoEgU1O5LlK7",
        "outputId": "429eefd4-cc79-4b5e-b354-a0e37b5b277f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful A trustworthy friend.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='\\nI am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\\n', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aP9IQ7q8mYu8",
        "outputId": "09325afe-78fc-4ac0-fb63-c0f9466117d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou are a helpful A trustworthy friend.</s>\\n<|user|>\\nHello, how are you doing?</s>\\n<|assistant|>\\nI'm doing well, thanks!</s>\\n<|user|>\\n\\nI am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\\n</s>\\n<|assistant|>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SOm5ZjnQ24jm",
        "outputId": "f5f5322b-ec4c-433d-cd04-4c29b8d550c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a helpful A trustworthy friend.</s>\n",
            "<|user|>\n",
            "Hello, how are you doing?</s>\n",
            "<|assistant|>\n",
            "I'm doing well, thanks!</s>\n",
            "<|user|>\n",
            "\n",
            "I am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\n",
            "</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(chat_model._to_chat_prompt(messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tKH257JH_6qi",
        "outputId": "c5edd6e0-f1ca-48f3-c903-d21572a7bb1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, please share the topic you're struggling with, and some examples of what you're finding difficult. Based on that, here are five points to work on:\n",
            "\n",
            "1. Review the prerequisites: Sometimes we assume we already know a concept when we're actually still learning it. Make sure you understand the building blocks that this concept is built upon.\n",
            "\n",
            "2. Practice, practice, practice: Just because you don't understand it in lecture doesn't mean you never will. Set aside some time to work through examples and practice problems until you feel more comfortable.\n",
            "\n",
            "3. Seek clarification: Ask your teacher or a classmate for help in understanding the concept. Sometimes simply explaining it to someone else can help make it click.\n",
            "\n",
            "4. Use resources outside the classroom: Textbooks, online tutoring services, and videos can all be helpful tools in learning the material.\n",
            "\n",
            "5. Focus on the big picture: Too often we get bogged down in the details, missing out on the bigger picture of why the concept is important or how it relates to other topics. Take a step back and try to make sense of it all. \n",
            "\n",
            "Remember, learning takes time and practice, so be patient and persistent. Good luck!\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si6NaQVggzQq"
      },
      "source": [
        "**Practice :**\n",
        "Create a prompt template that takes context and a question from the user and answers the question based on the given context.\n",
        "\n",
        "Hint: Keep the context in the system message and the question in the human message.\n",
        "\n",
        "**Context:** Meet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\n",
        "audiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,  Aryan's captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\n",
        "ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled  talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\n",
        "\n",
        "**Question:** What awards did Aryan Kapoor win for his contributions to the entertainment industry, and in which years were they received?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "t4StVsQ7lQS7"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant and know this context ```{context}``\"),\n",
        "        (\"human\", \" pls reply ```{question}``` in points based on the context provided. Strictly don't add extra facts and information ?\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MN6-TQaliTMT"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"What awards did Aryan Kapoor win for his contributions to the entertainment industry,\n",
        "and in which years were they received?\"\"\"\n",
        "context= \"\"\"\n",
        "Meet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\n",
        "audiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the\n",
        " prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his\n",
        "  vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,\n",
        "  Aryan's captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\n",
        "  ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing\n",
        "   his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled\n",
        "   talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\n",
        "\"\"\"\n",
        "messages = chat_template.format_messages(context = context,  question=question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5u6PwxIqjdfn",
        "outputId": "588ea24f-4838-4931-c1e8-33ab0949aaa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|system|>\\nYou are a helpful assistant and know this context ```\\nMeet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\\naudiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the\\n prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his\\n  vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,\\n  Aryan\\'s captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\\n  ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing\\n   his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled\\n   talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\\n``</s>\\n<|user|>\\n pls reply ```What awards did Aryan Kapoor win for his contributions to the entertainment industry,\\nand in which years were they received?``` in points based on the context provided. Strictly don\\'t add extra facts and information ?</s>\\n<|assistant|>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hs-ph2ePAFb5",
        "outputId": "727d8a74-2506-4aa2-8767-baa3c68dd935",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. In 2023, Aryan Kapoor won the prestigious Best Actor award at the National Film Awards for his mesmerizing performance in the movie \"Echoes of Eternity.\"\n",
            "2. In 2024, Aryan showcased his vocal skills as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\"\n",
            "3. In the same year (2024), Aryan was awarded the Filmfare Critics Award for Best Actor for his captivating screen presence.\n",
            "4. In 2025, Aryan received the International Icon of the Year award at the Global Entertainment Awards, recognizing his global impact and widespread admiration.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BPY4s_hGQv-"
      },
      "source": [
        "#### **Output Parsers**\n",
        "\n",
        "Let's start with defining how we would like the LLM output to look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "x1nyuc_1GO6u",
        "outputId": "6f292915-f24a-4b08-e994-d6493aa01c0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# An example output format\n",
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qW37-qdYGaOY"
      },
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ppJOtjsEGdL_"
      },
      "outputs": [],
      "source": [
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift or present for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gQDimFP3Gp5a",
        "outputId": "cb04a700-bc13-48a1-fbb2-b915f35f6dd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift or present for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Creating prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "print(prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "MYur071rMz6D",
        "outputId": "4c3a4505-32c8-4587-ba66-3c1276afcb0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"gift\": true,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\"I think my wife liked it so much she was speechless.\", \"slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "response = chat_model.invoke(messages)\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cV1eVBvyHhfY",
        "outputId": "8098a4ab-e429-4bec-ddba-68d4582f3999",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "print(type(response.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3bgzsf7pgP"
      },
      "source": [
        "#### **Parse the LLM output string into a structured data**:\n",
        "\n",
        "Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support [built-in ways to return structured output](https://python.langchain.com/docs/how_to/structured_output/), not all do.\n",
        "\n",
        "Output parsers are classes that help structure language model responses.\n",
        "\n",
        "Below we go over the main type of output parser, the `PydanticOutputParser`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8N7zx3mCoIj"
      },
      "source": [
        "[Structured output parser](https://python.langchain.com/docs/how_to/output_parser_structured/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "EH0hArbY7n4t"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Define your desired data structure\n",
        "class Product_Info(BaseModel):\n",
        "    \"\"\" Product service info.\"\"\"\n",
        "\n",
        "    gift: str = Field(description=\"Was the item purchased\\\n",
        "                             as a gift for someone else? \\\n",
        "                             Answer True if yes,\\\n",
        "                             False if not or unknown.\")\n",
        "    delivery_days: int = Field(description=\"How many days\\\n",
        "                                      did it take for the product\\\n",
        "                                      to arrive? If this \\\n",
        "                                      information is not found,\\\n",
        "                                      output -1.\")\n",
        "    price_value: list = Field(description=\"Extract sentences about the value or \\\n",
        "                                    price, and output them as a \\\n",
        "                                    comma separated Python list.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QzpGk67j24Ti"
      },
      "outputs": [],
      "source": [
        "# Set up a parser + inject instructions into the prompt template\n",
        "parser = PydanticOutputParser(pydantic_object = Product_Info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sJp6QGTF3Ma7"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{text}\\n\",\n",
        "    input_variables=[\"text\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions() + \"\\nOutput the answer as a JSON object.\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "kLPLDAGm9c0F",
        "outputId": "6ac92966-d8fa-43c1-9421-8e0076c2680d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Product_Info(gift='False', delivery_days=2, price_value=['This leaf blower is pretty amazing. ', 'It has four settings:candle blower, gentle breeze, windy city, and tornado. ', \"It arrived in two days, just in time for my wife's anniversary present. \", 'I think my wife liked it so much she was speechless. ', \"So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. \", \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features. \"])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# And a query intended to prompt a language model to populate the data structure.\n",
        "prompt_and_model = prompt | llm\n",
        "output = prompt_and_model.invoke({\"text\": customer_review})\n",
        "result = parser.invoke(output)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "u3BgDcCa7tcI",
        "outputId": "c74a23b0-aee2-49e7-8899-78a38b562be3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "2\n",
            "['This leaf blower is pretty amazing. ', 'It has four settings:candle blower, gentle breeze, windy city, and tornado. ', \"It arrived in two days, just in time for my wife's anniversary present. \", 'I think my wife liked it so much she was speechless. ', \"So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. \", \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features. \"]\n"
          ]
        }
      ],
      "source": [
        "print(result.gift)\n",
        "print(result.delivery_days)\n",
        "print(result.price_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br30bwaPKs0N"
      },
      "source": [
        "#### [**Customizing Conversational Memory**](https://python.langchain.com/docs/how_to/chatbots_memory/)\n",
        "\n",
        "LangChain can helps in building better chatbots, or have\n",
        "an LLM with more effective chats by better managing\n",
        "what it remembers from the conversation you've had so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "TBDcA56ovm_w"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "C1_WmMeZv1HV"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-zGGnVncvtWt"
      },
      "outputs": [],
      "source": [
        "store = {}      # to store chat messages\n",
        "\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    runnable = chain,\n",
        "    get_session_history = get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "PTe4OV8Kwoat",
        "outputId": "6e09021b-d598-4cd5-ed46-53eeb2dcea0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Greetings, James! I'm not capable of speaking or meeting you in person, but I'm eager to assist you in any way I can. If you have any questions, just let me know, and I'll do my best to provide helpful answers.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 57, 'total_tokens': 114}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--4b66d85c-c046-4c3e-869f-1ebeebe33110-0', usage_metadata={'input_tokens': 57, 'output_tokens': 57, 'total_tokens': 114})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"Hi, my name is James\"},\n",
        "    {\"configurable\": {\"session_id\": \"user1\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ALK5ZboQw53S",
        "outputId": "35123d55-316a-459d-c60b-86a72002913c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Greetings, James! I'm not capable of speaking or meeting you in person, but I'm eager to assist you in any way I can. If you have any questions, just let me know, and I'll do my best to provide helpful answers.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 57, 'total_tokens': 114}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--4b66d85c-c046-4c3e-869f-1ebeebe33110-0', usage_metadata={'input_tokens': 57, 'output_tokens': 57, 'total_tokens': 114})])}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "HQPxFgMEwz0u",
        "outputId": "a006421d-f8a4-4a7f-fa89-b7ebdaff4e16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"As an artificial intelligence language model, I do not interact with people or have the ability to remember information outside of what you choose to share with me. When you first introduced yourself as James, I stored that information for future reference. However, if you weren't prompted to share your name before, I wouldn't have known your name. I'd be happy to address you as James any time you communicate with me in the future.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 146, 'total_tokens': 237}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--3133d4e4-0f13-4693-b186-346743f4a12a-0', usage_metadata={'input_tokens': 146, 'output_tokens': 91, 'total_tokens': 237})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "chain_with_message_history.invoke(\n",
        "    input = {\"input\": \"Do you remember my name?\"},\n",
        "    config = {\"configurable\": {\"session_id\": \"user1\"}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vsmS0nLgxdxe",
        "outputId": "1712328f-6c61-4e9b-85cc-3b5422aba7b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Greetings, James! I'm not capable of speaking or meeting you in person, but I'm eager to assist you in any way I can. If you have any questions, just let me know, and I'll do my best to provide helpful answers.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 57, 'total_tokens': 114}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--4b66d85c-c046-4c3e-869f-1ebeebe33110-0', usage_metadata={'input_tokens': 57, 'output_tokens': 57, 'total_tokens': 114}), HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an artificial intelligence language model, I do not interact with people or have the ability to remember information outside of what you choose to share with me. When you first introduced yourself as James, I stored that information for future reference. However, if you weren't prompted to share your name before, I wouldn't have known your name. I'd be happy to address you as James any time you communicate with me in the future.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 146, 'total_tokens': 237}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--3133d4e4-0f13-4693-b186-346743f4a12a-0', usage_metadata={'input_tokens': 146, 'output_tokens': 91, 'total_tokens': 237})])}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "gbW0TdOVxef3",
        "outputId": "18c94e62-c7b6-48c0-8ab3-09356b8f144d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but as an AI language model, I do not interact with people in a way to learn their names. I can only respond based on the information you have provided me previously, or assume that you're asking me about someone else's name based on the context you've provided. Please let me know whose name you're asking for, and I will do my best to provide it.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 272, 'total_tokens': 358}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--5bdfa235-21c2-4ae0-834b-b5d12ad30a3a-0', usage_metadata={'input_tokens': 272, 'output_tokens': 86, 'total_tokens': 358})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "chain_with_message_history.invoke(\n",
        "    input = {\"input\": \"Can you tell me what is my name?\"},\n",
        "    config = {\"configurable\": {\"session_id\": \"user1\"}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Wb2HR0MAvm3j",
        "outputId": "7af4602a-868d-4eaa-faf2-0c8586190b6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Greetings, James! I'm not capable of speaking or meeting you in person, but I'm eager to assist you in any way I can. If you have any questions, just let me know, and I'll do my best to provide helpful answers.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 57, 'total_tokens': 114}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--4b66d85c-c046-4c3e-869f-1ebeebe33110-0', usage_metadata={'input_tokens': 57, 'output_tokens': 57, 'total_tokens': 114}), HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an artificial intelligence language model, I do not interact with people or have the ability to remember information outside of what you choose to share with me. When you first introduced yourself as James, I stored that information for future reference. However, if you weren't prompted to share your name before, I wouldn't have known your name. I'd be happy to address you as James any time you communicate with me in the future.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 146, 'total_tokens': 237}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--3133d4e4-0f13-4693-b186-346743f4a12a-0', usage_metadata={'input_tokens': 146, 'output_tokens': 91, 'total_tokens': 237}), HumanMessage(content='Can you tell me what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm sorry, but as an AI language model, I do not interact with people in a way to learn their names. I can only respond based on the information you have provided me previously, or assume that you're asking me about someone else's name based on the context you've provided. Please let me know whose name you're asking for, and I will do my best to provide it.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 272, 'total_tokens': 358}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--5bdfa235-21c2-4ae0-834b-b5d12ad30a3a-0', usage_metadata={'input_tokens': 272, 'output_tokens': 86, 'total_tokens': 358})])}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSbCSUOfxXa-"
      },
      "source": [
        "### **II. mistralai/Mistral-7B-Instruct-v0.3**\n",
        "\n",
        "<font color='#990000'> **Note that you need to ask for access before using this model. Go to https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 and click on \"Agree and access repository\".** </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "h_733Aai1RG3"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "_XZllnjBzqw0"
      },
      "outputs": [],
      "source": [
        "question = \"How to learn programing? Give 5 examples. \""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# Remove max_new_tokens from model_kwargs and pass it directly\n",
        "model_kwargs = {\"token\": os.environ[\"HF_TOKEN\"]}\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"conversational\", # Changed task to conversational\n",
        "    temperature=0.5,\n",
        "    max_new_tokens=128\n",
        ")"
      ],
      "metadata": {
        "id": "qp3XpAZwYmNS"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-aqIG9-lyFdF"
      },
      "outputs": [],
      "source": [
        "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# model_kwargs = {\"max_new_tokens\": 128, \"token\": os.environ[\"HF_TOKEN\"]}\n",
        "\n",
        "# llm = HuggingFaceEndpoint(repo_id=repo_id,\n",
        "#                           temperature=0.5,\n",
        "#                           model_kwargs= model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "HBTtXIUh6WZ9"
      },
      "outputs": [],
      "source": [
        "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# llm = HuggingFaceEndpoint(\n",
        "#     repo_id=repo_id,\n",
        "#     temperature=0.5,\n",
        "#     max_new_tokens=128  # Pass it directly, not inside model_kwargs\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "bXJMBgfy0A5Q",
        "outputId": "e1dd96cd-7f53-404d-fcd1-5ba7cfa2d053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. **Online Courses**: Websites like Codecademy, freeCodeCamp, and Coursera offer interactive courses in various programming languages such as Python, JavaScript, and Java. These platforms provide a structured learning path, practical exercises, and community support.\n",
            "\n",
            "2. **Books**: Books are a traditional and effective way to learn programming. \"Learn Python the Hard Way\" by Zed Shaw, \"Eloquent JavaScript\" by Marijn Haverbeke, and \"Head First Java\" by Kathy Sierra and Bert Bates are popular choices for beginners.\n",
            "\n",
            "3. **Tutorials and Guides**: Websites like W3Schools, MDN Web Docs, and GeeksforGeeks provide comprehensive tutorials and guides for various programming languages and topics. These resources are often free and can be used at your own pace.\n",
            "\n",
            "4. **Coding Bootcamps**: These are intensive, short-term training programs that teach programming skills. They are usually more hands-on and project-based, and they often provide job placement assistance. Examples include General Assembly, Hack Reactor, and App Academy.\n",
            "\n",
            "5. **Open Source Projects**: Contributing to open source projects can be a great way to learn programming. Websites like GitHub allow you to find projects that need contributions, and the community can provide guidance and feedback. This approach not only helps you learn but also gives you real-world experience.\n"
          ]
        }
      ],
      "source": [
        "# Get the tokenizer from the repo_id used by the llm\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm.repo_id)\n",
        "\n",
        "# Initialize ChatHuggingFace with both llm and tokenizer\n",
        "chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)\n",
        "\n",
        "# Now use the chat_model to invoke with the HumanMessage\n",
        "response = chat_model.invoke([HumanMessage(content=question)])\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmRI-p9fp0SR"
      },
      "source": [
        "#### **Prompt Template**\n",
        "\n",
        "**Example-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "xiZNTCsGNpPC"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "WnF-Qc_FrTZV"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "SHJWqMg3yxa_"
      },
      "outputs": [],
      "source": [
        "template_s = \"\"\"You are a {style1}.\\\n",
        "Tell me  {count} facts about {event_or_place}.```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "wHucwPZ7yVPF"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(template_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "jH_lqJRbzaHt",
        "outputId": "15cbb09d-bab4-4667-8fa6-c26afa308a86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['count', 'event_or_place', 'style1'], input_types={}, partial_variables={}, template='You are a {style1}.Tell me  {count} facts about {event_or_place}.```\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "fDyBeGD8zej9",
        "outputId": "8ed4f03d-fce6-44d3-cfc9-90ccafcffd23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['count', 'event_or_place', 'style1']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "QXUiH0sszkUe"
      },
      "outputs": [],
      "source": [
        "user_messages = prompt_template.format_messages(\n",
        "                    style1=\"knowledgeable historian\",\n",
        "                    count=5,\n",
        "                    event_or_place=\"Tajmahal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "qP62G5bz0KBJ",
        "outputId": "4d427270-8698-4695-96e1-aa5e513e3e38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='You are a knowledgeable historian.Tell me  5 facts about Tajmahal.```\\n', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "user_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQvXHPho54pT"
      },
      "source": [
        "**<font color='#990000'> Note: Before running the below code cell, go to the following link and click on \"Agree and access repository\" button </font>**\n",
        "\n",
        "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Q5IfurZcyYt8",
        "outputId": "04f3183e-23a8-47cb-8612-7888c90b2f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mistralai/Mistral-7B-Instruct-v0.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "chat_model.model_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the tokenizer from the repo_id used by the llm\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm.repo_id)\n",
        "\n",
        "# Initialize ChatHuggingFace with both llm and tokenizer\n",
        "chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)\n",
        "\n",
        "# Now the chat_model should have a tokenizer and the following line should work\n",
        "chat_model.model_id"
      ],
      "metadata": {
        "id": "BvXF5WBYeHq5",
        "outputId": "2179306c-7f92-4729-898e-17e0dd239593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mistralai/Mistral-7B-Instruct-v0.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ybxEq_9JqywX",
        "outputId": "30f48d13-3ffa-4bc3-ef08-d8bde01a48b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] You are a knowledgeable historian.Tell me  5 facts about Tajmahal.```\\n[/INST]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "chat_model._to_chat_prompt(user_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "SYI0aD8Hsty_",
        "outputId": "537d98da-4886-4338-e12c-5c4ae3339460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. The Taj Mahal is a mausoleum located in Agra, India, built by Mughal Emperor Shah Jahan in memory of his third wife, Mumtaz Mahal. It is often considered the jewel of Muslim art in India and one of the universally admired masterpieces of the world's heritage.\n",
            "\n",
            "2. Construction of the Taj Mahal began in 1632 and was completed in 1653, taking approximately 20 years and 20,000 workers. The structure is built from white marble and is adorned with semi-precious stones, including jade, lapis lazuli, and turquoise.\n",
            "\n",
            "3. The Taj Mahal is symmetrical, with a central dome flanked by four minarets. The main chamber houses the cenotaphs of Shah Jahan and Mumtaz Mahal, while the actual graves are located in a crypt beneath the structure.\n",
            "\n",
            "4. The Taj Mahal is set in a large complex that includes a mosque, a guest house, and a reflecting pool. The complex is enclosed by high walls, and the entrance is through a massive gateway with ornate calligraphy and inlaid decorations.\n",
            "\n",
            "5. The Taj Mahal is not just a monument of love, but also a symbol of the Mughal Empire's wealth and power. The use of precious materials, intricate craftsmanship, and grand scale reflect the Mughal rulers' desire to demonstrate their status and patronage of the arts. The Taj Mahal is also a testament to the architectural and artistic achievements of the Mughal period.\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(user_messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wakjVUVZp2YH"
      },
      "source": [
        "**Example-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "jiFDpiyENyOS"
      },
      "outputs": [],
      "source": [
        "messages = [HumanMessage(content=\"How to learn programming? give 2 points\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "6SO2qIrju7rc",
        "outputId": "50315b85-0ba5-4718-e3da-7e058b37b894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] How to learn programming? give 2 points[/INST]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "jEhKW2msODT2",
        "outputId": "9367c665-d89c-4f85-d887-67b9ced78282",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. Start with the basics: Begin with a beginner-friendly programming language like Python or JavaScript. These languages have simple syntax and are widely used, making them great for beginners. You can find numerous online resources, tutorials, and courses to help you get started.\n",
            "\n",
            "2. Practice consistently: Programming is a skill that requires regular practice to master. Set aside time each day to work on coding projects, solve problems, and complete exercises. Websites like Codecademy, LeetCode, and HackerRank offer a variety of coding challenges that can help you improve your skills. Additionally, consider working on personal projects or contributing to open-source projects to gain real-world experience.\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaaC7-gu_KOK"
      },
      "source": [
        "### **III.** **[Llama2](https://ai.meta.com/llama/)** ***(Optional)***\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        ">For using this model you have to click `Download models` link available in [this](https://ai.meta.com/llama/) reference which re-direct to a **form for request**. It may take 1 hour to 2 days to get the **approval** for usage of this model through HuggingFace. You will get an email for the same.\n",
        "\n",
        ">Once the request is approved, connect to **GPU runtime** for below steps. Also, you need to provide your HF api key/access token.\n",
        "\n",
        "Trying Llama2-2-7b model:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate langchain langchain_community langchain_huggingface huggingface_hub xformers bitsandbytes\n"
      ],
      "metadata": {
        "id": "1DpHTglT4nE4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HbzbpWIK_J4T"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q transformers accelerate langchain xformers bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zW69WOWSzwkG",
        "outputId": "2722fff1-dfe4-4617-f282-7e2db990bb53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your HuggingFace access token: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "# Enter your HuggingFace access token when prompted\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "pass_token = getpass(\"Enter your HuggingFace access token: \")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = pass_token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = pass_token\n",
        "\n",
        "del pass_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw7oXd2pAE1f"
      },
      "source": [
        "## Initializing the Hugging Face Pipeline\n",
        "\n",
        "The first thing we need to do is initialize a `text-generation` pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:\n",
        "\n",
        "* A LLM, in this case it will be `meta-llama/Llama-2-7b-chat-hf`.\n",
        "\n",
        "* The respective tokenizer for the model.\n",
        "\n",
        "We'll explain these as we get to them, let's begin with our model.\n",
        "\n",
        "We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "--40_VMGJBEz",
        "outputId": "c7987ed5-cbf7-450a-a8fe-34c8fe4b6df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch._C' has no attribute '_has_magma'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-3179023601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initExtension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_manager_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_manager_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0mhas_half\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0mhas_magma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_magma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mdefault_generators\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_has_magma'"
          ]
        }
      ],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h49z-C1RJC9D"
      },
      "outputs": [],
      "source": [
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall existing versions to ensure a clean install\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "\n",
        "# Install compatible versions of torch, torchvision, and torchaudio with CUDA support\n",
        "# This command is for CUDA 11.8, which is common in Colab.\n",
        "# If you have a different CUDA version, refer to the PyTorch website for the correct command.\n",
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# The rest of the cell remains the same\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "# The error occurred before this point, so the rest of the code should now work\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "-UJjaHYA06hw",
        "outputId": "6968d30a-462d-451c-a2a9-acfbe299b972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.7.0\n",
            "Uninstalling torch-2.7.0:\n",
            "  Successfully uninstalled torch-2.7.0\n",
            "Found existing installation: torchvision 0.17.0+cu118\n",
            "Uninstalling torchvision-0.17.0+cu118:\n",
            "  Successfully uninstalled torchvision-0.17.0+cu118\n",
            "Found existing installation: torchaudio 2.2.0+cu118\n",
            "Uninstalling torchaudio-2.2.0+cu118:\n",
            "  Successfully uninstalled torchaudio-2.2.0+cu118\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.2.0\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp311-cp311-linux_x86_64.whl (811.7 MB)\n",
            "Collecting torchvision==0.17.0\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.17.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "Collecting torchaudio==2.2.0\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.2.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.8.86)\n",
            "Collecting triton==2.2.0 (from torch==2.2.0)\n",
            "  Using cached https://download.pytorch.org/whl/triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.0) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.0) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (2025.6.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
            "Installing collected packages: triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.3.0\n",
            "    Uninstalling triton-3.3.0:\n",
            "      Successfully uninstalled triton-3.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xformers 0.0.30 requires torch==2.7.0, but you have torch 2.2.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.2.0+cu118 torchaudio-2.2.0+cu118 torchvision-0.17.0+cu118 triton-2.2.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_cuda' from 'torch._utils' (/usr/local/lib/python3.11/dist-packages/torch/_utils.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-1407168820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# The rest of the cell remains the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_StorageBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypedStorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_LegacyStorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUntypedStorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_warn_typed_storage_removal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;31m# NOTE: New <type>Storage classes should never be added. When adding a new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_Dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_Optional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_cuda' from 'torch._utils' (/usr/local/lib/python3.11/dist-packages/torch/_utils.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqH62ls48J9k"
      },
      "source": [
        "<font color='#990000'> **Note: Before running the below code cell, go to the following link and execute the following steps.** </font>\n",
        "\n",
        "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "**Step-1:** You need to share contact information with Meta to access this model\n",
        "\n",
        "**Step-2:** Under **'LLAMA 2 COMMUNITY LICENSE AGREEMENT'** section, click on **'Expand to review and access'** button.\n",
        "\n",
        "**Step-3:** Then fill up the form and finally check-in the check box for accepting the terms of the license.\n",
        "\n",
        "**Step-4:** Click on **'Submit'** button.\n",
        "\n",
        "**Step-5:** After submitting you need to wait till the owner has given you the access. **You will receive one email notification.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2Cp6jnomS0S"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1000,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n7MjpX4nM-r"
      },
      "outputs": [],
      "source": [
        "res = pipeline(\"How to learn programming?\")\n",
        "print(res[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzqImT9RAdAg"
      },
      "source": [
        "#### **Now implementing with LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLkLetCxWF-t"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain\n",
        "!pip -q install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h71jByoBAffB"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0.7})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tr-YGpvnr0X"
      },
      "outputs": [],
      "source": [
        "print(llm.invoke(\"How to learn programming?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prWXot8wcKyS"
      },
      "source": [
        "#### **Prompt Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DTl7I1HaOJK"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqU1EAehaOJP"
      },
      "outputs": [],
      "source": [
        "template_s = \"\"\"Reply  the answer \\\n",
        "like  {style1}. \\\n",
        "text: ```{text1}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK6fMe1paOJP"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(template_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5AlexzxaOJP"
      },
      "outputs": [],
      "source": [
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJa3nBfbaOJQ"
      },
      "outputs": [],
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYlQrFsnaOJQ"
      },
      "outputs": [],
      "source": [
        "style = \"\"\"trustworthy friend\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9UtT3reaOJQ"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "I am not able to understand the concept taught in class. \\\n",
        "Could you please suggest something? \\\n",
        "I need your help. Give 5 points to work on.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUB4HHQTaOJQ"
      },
      "outputs": [],
      "source": [
        "user_messages = prompt_template.format_messages(\n",
        "                    style1=style,\n",
        "                    text1=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r6WC6OZaOJQ"
      },
      "outputs": [],
      "source": [
        "print(user_messages[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-zPBqhKaOJQ"
      },
      "outputs": [],
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "llm_response = llm.invoke(user_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sMDbkrmoIs_"
      },
      "outputs": [],
      "source": [
        "print(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErjQyyi4nR2n"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title Which of the following prompt techniques in LangChain allows flexible templated prompts that are suitable for better describing the role and content? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"Both\" #@param [\"\", \"PromptTemplate\", \"ChatPromptTemplate\", \"Both\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good, But Not Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}