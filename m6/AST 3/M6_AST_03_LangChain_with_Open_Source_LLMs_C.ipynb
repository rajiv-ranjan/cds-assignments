{"cells":[{"cell_type":"markdown","metadata":{"id":"RMNv0S7qBl1Z"},"source":["# Advanced Certification Program in Computational Data Science\n","## A Program by IISc and TalentSprint\n","### Assignment 3: Open Source LLMs with LangChain 🦜🔗"]},{"cell_type":"markdown","metadata":{"id":"R7nOHNxJqC2X"},"source":["## Learning Objectives\n","\n","At the end of the experiment, you will be able to:\n","\n","* use open source LLMs: **`zephyr-7b-beta`**, **`Mistral-7B-Instruct-v0.2`**,  and **`Llama2`** through HuggingFaceHub with LangChain\n","* understand & use the concept of Prompt template, Memory and output parsers in LangChain\n"]},{"cell_type":"markdown","metadata":{"id":"rKQ0Fvl_jNqU"},"source":["### Setup Steps:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dG904xmKyK31"},"outputs":[],"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOc9lw37yK4A"},"outputs":[],"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"g9aMvFszyK4A"},"outputs":[],"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"M5_AST_03_LangChain_with_Open_Source_LLMs_C\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://learn-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")"]},{"cell_type":"markdown","metadata":{"id":"fWUXHwdSLFMA"},"source":["### Steps for Creating Hugging Face access tokens:\n","\n","* **Visit the Hugging Face Website:** Head to the Hugging Face website (https://huggingface.co/) to begin the account creation process.\n","\n","* **Click on “Sign Up”:** Locate the “Sign Up” button on the top right corner of the homepage and click on it.\n","\n","* **Choose a Sign-Up Method:** Hugging Face offers multiple sign-up methods, including Google, GitHub, and email. Select your preferred method and follow the prompts to complete the registration.\n","\n","* **Verify Your Email (if applicable):** If you choose to sign up via email, verify your email address by clicking on the confirmation link sent to your inbox.\n","\n","* **Complete Your Profile:** Enhance your Hugging Face experience by completing your profile. Add a profile picture, a short bio, and any other details you’d like to share with the community.\n","\n","* **Create Your Access Token:** Go to the link (https://huggingface.co/settings/tokens)\n","\n","* **Click on the option 'Access Tokens' from the left pane.**\n","\n","* **Then under the User Access Tokens, click on the button 'New token'.**\n","\n","* **Select Token type: Write.**\n","\n","* **Put your desired Token name.**\n","\n","* **Then at the bottom end, click on 'Create token'. The Hugging Face access token will be generated. Copy and paste the access token in your Google Colab Notebook.**"]},{"cell_type":"markdown","metadata":{"id":"WeXRnNMfnFqF"},"source":["### Install required dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKLiNSv6wTr6"},"outputs":[],"source":["# Langchain\n","!pip -q install langchain\n","\n","# Library to communicate with HF hub\n","!pip -q install --upgrade huggingface_hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAsnCf4Z9e1K"},"outputs":[],"source":["!pip -q install langchain_community"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BrFlLJv3-_4N"},"outputs":[],"source":["!pip -q install langchain_huggingface"]},{"cell_type":"markdown","metadata":{"id":"T0sgU1wiq7Hw"},"source":["### Import required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0IydGx_q78h"},"outputs":[],"source":["import os\n","from getpass import getpass\n","\n","from langchain_community.llms import HuggingFaceEndpoint\n","from langchain.prompts import PromptTemplate"]},{"cell_type":"markdown","metadata":{"id":"dMBDqFT0kkgH"},"source":["### **Provide your HuggingFace api key/access token**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9TSeu4cq1Jw"},"outputs":[],"source":["# Enter your HuggingFace access token when prompted\n","\n","pass_token = getpass(\"Enter your HuggingFace access token: \")\n","\n","os.environ[\"HF_TOKEN\"] = pass_token\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = pass_token\n","\n","del pass_token"]},{"cell_type":"markdown","metadata":{"id":"GgzDSiux-EuO"},"source":["### **Exploring Open Source LLMs hosted on HuggingFace**\n","\n",">**I.** `HuggingFaceH4/zephyr-7b-beta`\n",">\n",">**II.** `mistralai/Mistral-7B-Instruct-v0.2`\n",">\n",">**III.** `LlaMa2`\n","\n","[LangChain link](https://python.langchain.com/docs/integrations/chat/huggingface) for using Hugging Face LLM's as chat models."]},{"cell_type":"markdown","metadata":{"id":"oVK3LSt6mzKa"},"source":["### **I.** [**HuggingFaceH4/zephyr-7b-beta**](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QudSou2y-T4i"},"outputs":[],"source":["# Import HuggingFace model abstraction class from langchain\n","from langchain_huggingface import HuggingFaceEndpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVRB1nztyTUm"},"outputs":[],"source":["llm = HuggingFaceEndpoint(\n","    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n","    task=\"text-generation\",\n","    max_new_tokens = 512,\n","    top_k = 30,\n","    temperature = 0.1,\n","    repetition_penalty = 1.03,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrneNiybIFl_"},"outputs":[],"source":["response = llm.invoke(\"How to learn programming? give 5 points\")\n","print(response)"]},{"cell_type":"markdown","metadata":{"id":"F4N14UjkBaws"},"source":["#### **Prompt Template**\n","\n","Prompt templates are predefined recipes for generating prompts for language models.\n","\n","A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n","\n","LangChain provides tooling to create and work with prompt templates.\n","\n","To know more about Prompt template, refer [here](https://python.langchain.com/docs/modules/model_io/prompts/quick_start)."]},{"cell_type":"markdown","metadata":{"id":"-v-l3yx9hQs2"},"source":["#### **Example-1**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIjgIglFbjDG"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"Tell me a {adjective} joke about {content}.\"\n",")\n","messages = prompt_template.format(adjective=\"funny\", content=\"Trump\")\n","messages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUOnAu5c_Hqw"},"outputs":[],"source":["from langchain_huggingface import ChatHuggingFace\n","\n","chat_model = ChatHuggingFace(llm = llm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjM9t0INbjLg"},"outputs":[],"source":["response = chat_model.invoke(messages)\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"ZPeo4FQBhL0l"},"source":["#### **Example-2**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bat0JTd7GrJx"},"outputs":[],"source":["from langchain.schema import (\n","    HumanMessage,\n","    SystemMessage,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQe8qxghewsF"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"Tell me {count} facts about {event_or_place}.\"\n",")\n","user_msg = prompt_template.format(count=5, event_or_place=\"Tajmahal\")\n","user_msg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdGDCoQhmuDL"},"outputs":[],"source":["messages = [\n","    SystemMessage(content=\"You're a knowledgeable historian\"),\n","    HumanMessage(content=user_msg),\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCpfJTPCAqGP"},"outputs":[],"source":["from langchain_huggingface import ChatHuggingFace"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kg1GDWsCRbL"},"outputs":[],"source":["chat_model = ChatHuggingFace(llm=llm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27Iyhp-5DJ5-"},"outputs":[],"source":["chat_model.model_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_EReM93znzV"},"outputs":[],"source":["from langchain_huggingface import ChatHuggingFace\n","from transformers import AutoTokenizer # Import AutoTokenizer\n","\n","# Assume llm is already defined as HuggingFaceEndpoint\n","\n","# Get the tokenizer from the repo_id used by the llm\n","tokenizer = AutoTokenizer.from_pretrained(llm.repo_id) # Get tokenizer from llm's repo_id\n","\n","# Initialize ChatHuggingFace with both llm and tokenizer\n","chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)\n","\n","# Now the chat_model should have a tokenizer and the following line should work\n","chat_model._to_chat_prompt(messages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9niBkmKIDMVI"},"outputs":[],"source":["#chat_model._to_chat_prompt(messages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"px6WPyxN2DYj"},"outputs":[],"source":["print(chat_model._to_chat_prompt(messages))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZduaUIkl-VpI"},"outputs":[],"source":["response = chat_model.invoke(messages)\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"USL19U5MKapo"},"source":["#### **Example-3**\n","\n","The prompt to *chat models* is a list of chat messages.\n","\n","Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsZWlMZEEcth"},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBLyJ-sFk8Hf"},"outputs":[],"source":["chat_template = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", \"You are a helpful A {persona}.\"),\n","        (\"human\", \"Hello, how are you doing?\"),\n","        (\"ai\", \"I'm doing well, thanks!\"),\n","        (\"human\", \"{user_input}\"),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rr0S0bklaIr"},"outputs":[],"source":["persona = \"\"\"trustworthy friend\"\"\"\n","query= \"\"\"\n","I am not able to understand the concept taught in class. \\\n","Could you please suggest something? \\\n","I need your help. Give 5 points to work on.\n","\"\"\"\n","messages = chat_template.format_messages(persona = persona, user_input=query)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoEgU1O5LlK7"},"outputs":[],"source":["messages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aP9IQ7q8mYu8"},"outputs":[],"source":["chat_model._to_chat_prompt(messages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOm5ZjnQ24jm"},"outputs":[],"source":["print(chat_model._to_chat_prompt(messages))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKH257JH_6qi"},"outputs":[],"source":["response = chat_model.invoke(messages)\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"si6NaQVggzQq"},"source":["**Practice :**\n","Create a prompt template that takes context and a question from the user and answers the question based on the given context.\n","\n","Hint: Keep the context in the system message and the question in the human message.\n","\n","**Context:** Meet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\n","audiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,  Aryan's captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\n","ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled  talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\n","\n","**Question:** What awards did Aryan Kapoor win for his contributions to the entertainment industry, and in which years were they received?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4StVsQ7lQS7"},"outputs":[],"source":["chat_template = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", \"You are a helpful assistant and know this context ```{context}``\"),\n","        (\"human\", \" pls reply ```{question}``` in points based on the context provided. Strictly don't add extra facts and information ?\"),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MN6-TQaliTMT"},"outputs":[],"source":["question = \"\"\"What awards did Aryan Kapoor win for his contributions to the entertainment industry,\n","and in which years were they received?\"\"\"\n","context= \"\"\"\n","Meet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\n","audiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the\n"," prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his\n","  vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,\n","  Aryan's captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\n","  ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing\n","   his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled\n","   talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\n","\"\"\"\n","messages = chat_template.format_messages(context = context,  question=question)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5u6PwxIqjdfn"},"outputs":[],"source":["chat_model._to_chat_prompt(messages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hs-ph2ePAFb5"},"outputs":[],"source":["response = chat_model.invoke(messages)\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"7BPY4s_hGQv-"},"source":["#### **Output Parsers**\n","\n","Let's start with defining how we would like the LLM output to look like:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1nyuc_1GO6u"},"outputs":[],"source":["# An example output format\n","{\n","  \"gift\": False,\n","  \"delivery_days\": 5,\n","  \"price_value\": \"pretty affordable!\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qW37-qdYGaOY"},"outputs":[],"source":["customer_review = \"\"\"\\\n","This leaf blower is pretty amazing.  It has four settings:\\\n","candle blower, gentle breeze, windy city, and tornado. \\\n","It arrived in two days, just in time for my wife's \\\n","anniversary present. \\\n","I think my wife liked it so much she was speechless. \\\n","So far I've been the only one using it, and I've been \\\n","using it every other morning to clear the leaves on our lawn. \\\n","It's slightly more expensive than the other leaf blowers \\\n","out there, but I think it's worth it for the extra features.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppJOtjsEGdL_"},"outputs":[],"source":["review_template = \"\"\"\\\n","For the following text, extract the following information:\n","\n","gift: Was the item purchased as a gift or present for someone else? \\\n","Answer True if yes, False if not or unknown.\n","\n","delivery_days: How many days did it take for the product \\\n","to arrive? If this information is not found, output -1.\n","\n","price_value: Extract any sentences about the value or price,\\\n","and output them as a comma separated Python list.\n","\n","Format the output as JSON with the following keys:\n","gift\n","delivery_days\n","price_value\n","\n","text: {text}\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQDimFP3Gp5a"},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# Creating prompt template\n","prompt_template = ChatPromptTemplate.from_template(review_template)\n","print(prompt_template)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYur071rMz6D"},"outputs":[],"source":["messages = prompt_template.format_messages(text=customer_review)\n","response = chat_model.invoke(messages)\n","print(response.content)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cV1eVBvyHhfY"},"outputs":[],"source":["print(type(response.content))"]},{"cell_type":"markdown","metadata":{"id":"Pf3bgzsf7pgP"},"source":["#### **Parse the LLM output string into a structured data**:\n","\n","Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support [built-in ways to return structured output](https://python.langchain.com/docs/how_to/structured_output/), not all do.\n","\n","Output parsers are classes that help structure language model responses.\n","\n","Below we go over the main type of output parser, the `PydanticOutputParser`.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d8N7zx3mCoIj"},"source":["[Structured output parser](https://python.langchain.com/docs/how_to/output_parser_structured/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EH0hArbY7n4t"},"outputs":[],"source":["from langchain_core.output_parsers import PydanticOutputParser\n","from pydantic import BaseModel, Field\n","\n","# Define your desired data structure\n","class Product_Info(BaseModel):\n","    \"\"\" Product service info.\"\"\"\n","\n","    gift: str = Field(description=\"Was the item purchased\\\n","                             as a gift for someone else? \\\n","                             Answer True if yes,\\\n","                             False if not or unknown.\")\n","    delivery_days: int = Field(description=\"How many days\\\n","                                      did it take for the product\\\n","                                      to arrive? If this \\\n","                                      information is not found,\\\n","                                      output -1.\")\n","    price_value: list = Field(description=\"Extract sentences about the value or \\\n","                                    price, and output them as a \\\n","                                    comma separated Python list.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzpGk67j24Ti"},"outputs":[],"source":["# Set up a parser + inject instructions into the prompt template\n","parser = PydanticOutputParser(pydantic_object = Product_Info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJp6QGTF3Ma7"},"outputs":[],"source":["prompt = PromptTemplate(\n","    template=\"Answer the user query.\\n{format_instructions}\\n{text}\\n\",\n","    input_variables=[\"text\"],\n","    partial_variables={\"format_instructions\": parser.get_format_instructions() + \"\\nOutput the answer as a JSON object.\"},\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLPLDAGm9c0F"},"outputs":[],"source":["# And a query intended to prompt a language model to populate the data structure.\n","prompt_and_model = prompt | llm\n","output = prompt_and_model.invoke({\"text\": customer_review})\n","result = parser.invoke(output)\n","result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3BgDcCa7tcI"},"outputs":[],"source":["print(result.gift)\n","print(result.delivery_days)\n","print(result.price_value)"]},{"cell_type":"markdown","metadata":{"id":"Br30bwaPKs0N"},"source":["#### [**Customizing Conversational Memory**](https://python.langchain.com/docs/how_to/chatbots_memory/)\n","\n","LangChain can helps in building better chatbots, or have\n","an LLM with more effective chats by better managing\n","what it remembers from the conversation you've had so far."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBDcA56ovm_w"},"outputs":[],"source":["prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n","        ),\n","        (\"placeholder\", \"{chat_history}\"),\n","        (\"human\", \"{input}\"),\n","    ]\n",")\n","\n","chain = prompt | chat_model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1_WmMeZv1HV"},"outputs":[],"source":["from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_core.runnables.history import RunnableWithMessageHistory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zGGnVncvtWt"},"outputs":[],"source":["store = {}      # to store chat messages\n","\n","def get_session_history(session_id: str) -> ChatMessageHistory:\n","    if session_id not in store:\n","        store[session_id] = ChatMessageHistory()\n","    return store[session_id]\n","\n","\n","chain_with_message_history = RunnableWithMessageHistory(\n","    runnable = chain,\n","    get_session_history = get_session_history,\n","    input_messages_key=\"input\",\n","    history_messages_key=\"chat_history\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTe4OV8Kwoat"},"outputs":[],"source":["chain_with_message_history.invoke(\n","    {\"input\": \"Hi, my name is James\"},\n","    {\"configurable\": {\"session_id\": \"user1\"}},\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALK5ZboQw53S"},"outputs":[],"source":["store"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQPxFgMEwz0u"},"outputs":[],"source":["chain_with_message_history.invoke(\n","    input = {\"input\": \"Do you remember my name?\"},\n","    config = {\"configurable\": {\"session_id\": \"user1\"}}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsmS0nLgxdxe"},"outputs":[],"source":["store"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbW0TdOVxef3"},"outputs":[],"source":["chain_with_message_history.invoke(\n","    input = {\"input\": \"Can you tell me what is my name?\"},\n","    config = {\"configurable\": {\"session_id\": \"user1\"}}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wb2HR0MAvm3j"},"outputs":[],"source":["store"]},{"cell_type":"markdown","metadata":{"id":"zSbCSUOfxXa-"},"source":["### **II. mistralai/Mistral-7B-Instruct-v0.3**\n","\n","<font color='#990000'> **Note that you need to ask for access before using this model. Go to https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 and click on \"Agree and access repository\".** </font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_733Aai1RG3"},"outputs":[],"source":["from langchain_huggingface import HuggingFaceEndpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XZllnjBzqw0"},"outputs":[],"source":["question = \"How to learn programing? Give 5 examples. \""]},{"cell_type":"code","source":["repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","\n","# Remove max_new_tokens from model_kwargs and pass it directly\n","model_kwargs = {\"token\": os.environ[\"HF_TOKEN\"]}\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id=repo_id,\n","    task=\"conversational\", # Changed task to conversational\n","    temperature=0.5,\n","    max_new_tokens=128\n",")"],"metadata":{"id":"qp3XpAZwYmNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aqIG9-lyFdF"},"outputs":[],"source":["# repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","\n","# model_kwargs = {\"max_new_tokens\": 128, \"token\": os.environ[\"HF_TOKEN\"]}\n","\n","# llm = HuggingFaceEndpoint(repo_id=repo_id,\n","#                           temperature=0.5,\n","#                           model_kwargs= model_kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBTtXIUh6WZ9"},"outputs":[],"source":["# repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","\n","# llm = HuggingFaceEndpoint(\n","#     repo_id=repo_id,\n","#     temperature=0.5,\n","#     max_new_tokens=128  # Pass it directly, not inside model_kwargs\n","# )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXJMBgfy0A5Q"},"outputs":[],"source":["# Get the tokenizer from the repo_id used by the llm\n","tokenizer = AutoTokenizer.from_pretrained(llm.repo_id)\n","\n","# Initialize ChatHuggingFace with both llm and tokenizer\n","chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)\n","\n","# Now use the chat_model to invoke with the HumanMessage\n","response = chat_model.invoke([HumanMessage(content=question)])\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"OmRI-p9fp0SR"},"source":["#### **Prompt Template**\n","\n","**Example-1**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xiZNTCsGNpPC"},"outputs":[],"source":["from langchain.schema import (\n","    HumanMessage,\n","    SystemMessage,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnF-Qc_FrTZV"},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHJWqMg3yxa_"},"outputs":[],"source":["template_s = \"\"\"You are a {style1}.\\\n","Tell me  {count} facts about {event_or_place}.```\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHucwPZ7yVPF"},"outputs":[],"source":["prompt_template = ChatPromptTemplate.from_template(template_s)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jH_lqJRbzaHt"},"outputs":[],"source":["prompt_template.messages[0].prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDyBeGD8zej9"},"outputs":[],"source":["prompt_template.messages[0].prompt.input_variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXUiH0sszkUe"},"outputs":[],"source":["user_messages = prompt_template.format_messages(\n","                    style1=\"knowledgeable historian\",\n","                    count=5,\n","                    event_or_place=\"Tajmahal\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qP62G5bz0KBJ"},"outputs":[],"source":["user_messages"]},{"cell_type":"markdown","metadata":{"id":"RQvXHPho54pT"},"source":["**<font color='#990000'> Note: Before running the below code cell, go to the following link and click on \"Agree and access repository\" button </font>**\n","\n","https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5IfurZcyYt8"},"outputs":[],"source":["from langchain_huggingface import ChatHuggingFace\n","\n","chat_model = ChatHuggingFace(llm=llm)\n","\n","chat_model.model_id"]},{"cell_type":"code","source":["# Get the tokenizer from the repo_id used by the llm\n","tokenizer = AutoTokenizer.from_pretrained(llm.repo_id)\n","\n","# Initialize ChatHuggingFace with both llm and tokenizer\n","chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)\n","\n","# Now the chat_model should have a tokenizer and the following line should work\n","chat_model.model_id"],"metadata":{"id":"BvXF5WBYeHq5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybxEq_9JqywX"},"outputs":[],"source":["chat_model._to_chat_prompt(user_messages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYI0aD8Hsty_"},"outputs":[],"source":["response = chat_model.invoke(user_messages)\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"wakjVUVZp2YH"},"source":["**Example-2**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiFDpiyENyOS"},"outputs":[],"source":["messages = [HumanMessage(content=\"How to learn programming? give 2 points\")]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SO2qIrju7rc"},"outputs":[],"source":["chat_model._to_chat_prompt(messages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEhKW2msODT2"},"outputs":[],"source":["response = chat_model.invoke(messages)\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"XaaC7-gu_KOK"},"source":["### **III.** **[Llama2](https://ai.meta.com/llama/)** ***(Optional)***\n","\n","**NOTE:**\n","\n",">For using this model you have to click `Download models` link available in [this](https://ai.meta.com/llama/) reference which re-direct to a **form for request**. It may take 1 hour to 2 days to get the **approval** for usage of this model through HuggingFace. You will get an email for the same.\n","\n",">Once the request is approved, connect to **GPU runtime** for below steps. Also, you need to provide your HF api key/access token.\n","\n","Trying Llama2-2-7b model:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbzbpWIK_J4T"},"outputs":[],"source":["%%capture\n","!pip install -q transformers accelerate langchain xformers bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zW69WOWSzwkG"},"outputs":[],"source":["# Enter your HuggingFace access token when prompted\n","import os\n","from getpass import getpass\n","\n","pass_token = getpass(\"Enter your HuggingFace access token: \")\n","\n","os.environ[\"HF_TOKEN\"] = pass_token\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = pass_token\n","\n","del pass_token"]},{"cell_type":"markdown","metadata":{"id":"Rw7oXd2pAE1f"},"source":["## Initializing the Hugging Face Pipeline\n","\n","The first thing we need to do is initialize a `text-generation` pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:\n","\n","* A LLM, in this case it will be `meta-llama/Llama-2-7b-chat-hf`.\n","\n","* The respective tokenizer for the model.\n","\n","We'll explain these as we get to them, let's begin with our model.\n","\n","We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--40_VMGJBEz"},"outputs":[],"source":["from torch import cuda, bfloat16\n","import transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h49z-C1RJC9D"},"outputs":[],"source":["model_id = 'meta-llama/Llama-2-7b-chat-hf'\n","\n","device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","metadata":{"id":"jqH62ls48J9k"},"source":["<font color='#990000'> **Note: Before running the below code cell, go to the following link and execute the following steps.** </font>\n","\n","https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n","\n","**Step-1:** You need to share contact information with Meta to access this model\n","\n","**Step-2:** Under **'LLAMA 2 COMMUNITY LICENSE AGREEMENT'** section, click on **'Expand to review and access'** button.\n","\n","**Step-3:** Then fill up the form and finally check-in the check box for accepting the terms of the license.\n","\n","**Step-4:** Click on **'Submit'** button.\n","\n","**Step-5:** After submitting you need to wait till the owner has given you the access. **You will receive one email notification.**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2Cp6jnomS0S"},"outputs":[],"source":["tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n","\n","pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model_id,\n","    tokenizer=tokenizer,\n","    torch_dtype=bfloat16,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    max_length=1000,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8n7MjpX4nM-r"},"outputs":[],"source":["res = pipeline(\"How to learn programming?\")\n","print(res[0][\"generated_text\"])"]},{"cell_type":"markdown","metadata":{"id":"IzqImT9RAdAg"},"source":["#### **Now implementing with LangChain**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLkLetCxWF-t"},"outputs":[],"source":["!pip -q install langchain\n","!pip -q install langchain_community"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h71jByoBAffB"},"outputs":[],"source":["from langchain.llms import HuggingFacePipeline\n","\n","llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0.7})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tr-YGpvnr0X"},"outputs":[],"source":["print(llm.invoke(\"How to learn programming?\"))"]},{"cell_type":"markdown","metadata":{"id":"prWXot8wcKyS"},"source":["#### **Prompt Template**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DTl7I1HaOJK"},"outputs":[],"source":["from langchain.prompts import ChatPromptTemplate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqU1EAehaOJP"},"outputs":[],"source":["template_s = \"\"\"Reply  the answer \\\n","like  {style1}. \\\n","text: ```{text1}```\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YK6fMe1paOJP"},"outputs":[],"source":["prompt_template = ChatPromptTemplate.from_template(template_s)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5AlexzxaOJP"},"outputs":[],"source":["prompt_template.messages[0].prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJa3nBfbaOJQ"},"outputs":[],"source":["prompt_template.messages[0].prompt.input_variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYlQrFsnaOJQ"},"outputs":[],"source":["style = \"\"\"trustworthy friend\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9UtT3reaOJQ"},"outputs":[],"source":["query = \"\"\"\n","I am not able to understand the concept taught in class. \\\n","Could you please suggest something? \\\n","I need your help. Give 5 points to work on.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUB4HHQTaOJQ"},"outputs":[],"source":["user_messages = prompt_template.format_messages(\n","                    style1=style,\n","                    text1=query)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6r6WC6OZaOJQ"},"outputs":[],"source":["print(user_messages[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-zPBqhKaOJQ"},"outputs":[],"source":["# Call the LLM to translate to the style of the customer message\n","llm_response = llm.invoke(user_messages)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sMDbkrmoIs_"},"outputs":[],"source":["print(llm_response)"]},{"cell_type":"markdown","metadata":{"id":"ErjQyyi4nR2n"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgSwVENIPcM6"},"outputs":[],"source":["#@title Which of the following prompt techniques in LangChain allows flexible templated prompts that are suitable for better describing the role and content? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"PromptTemplate\" #@param [\"\", \"PromptTemplate\", \"ChatPromptTemplate\", \"Both\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMzKSbLIgFzQ"},"outputs":[],"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjcH1VWSFI2l"},"outputs":[],"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VBk_4VTAxCM"},"outputs":[],"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH91cL1JWH7m"},"outputs":[],"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8xLqj7VWIKW"},"outputs":[],"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FzAZHt1zw-Y-"},"outputs":[],"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}