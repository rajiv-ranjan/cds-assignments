{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajiv-ranjan/cds-assignments/blob/m6-ast3-troubleshooting/m6/AST%203/M6_AST_03_LangChain_with_Open_Source_LLMs_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMNv0S7qBl1Z"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 3: Open Source LLMs with LangChain ðŸ¦œðŸ”—"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7nOHNxJqC2X"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* use open source LLMs: **`zephyr-7b-beta`**, **`Mistral-7B-Instruct-v0.2`**,  and **`Llama2`** through HuggingFaceHub with LangChain\n",
        "* understand & use the concept of Prompt template, Memory and output parsers in LangChain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQ0Fvl_jNqU"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dG904xmKyK31"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2418709\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UOc9lw37yK4A"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9902028293\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "g9aMvFszyK4A",
        "outputId": "2329b87d-ec7f-42c2-d19a-23791245ceb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2418709&recordId=6001\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M6_AST_03_LangChain_with_Open_Source_LLMs_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWUXHwdSLFMA"
      },
      "source": [
        "### Steps for Creating Hugging Face access tokens:\n",
        "\n",
        "* **Visit the Hugging Face Website:** Head to the Hugging Face website (https://huggingface.co/) to begin the account creation process.\n",
        "\n",
        "* **Click on â€œSign Upâ€:** Locate the â€œSign Upâ€ button on the top right corner of the homepage and click on it.\n",
        "\n",
        "* **Choose a Sign-Up Method:** Hugging Face offers multiple sign-up methods, including Google, GitHub, and email. Select your preferred method and follow the prompts to complete the registration.\n",
        "\n",
        "* **Verify Your Email (if applicable):** If you choose to sign up via email, verify your email address by clicking on the confirmation link sent to your inbox.\n",
        "\n",
        "* **Complete Your Profile:** Enhance your Hugging Face experience by completing your profile. Add a profile picture, a short bio, and any other details youâ€™d like to share with the community.\n",
        "\n",
        "* **Create Your Access Token:** Go to the link (https://huggingface.co/settings/tokens)\n",
        "\n",
        "* **Click on the option 'Access Tokens' from the left pane.**\n",
        "\n",
        "* **Then under the User Access Tokens, click on the button 'New token'.**\n",
        "\n",
        "* **Select Token type: Write.**\n",
        "\n",
        "* **Put your desired Token name.**\n",
        "\n",
        "* **Then at the bottom end, click on 'Create token'. The Hugging Face access token will be generated. Copy and paste the access token in your Google Colab Notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeXRnNMfnFqF"
      },
      "source": [
        "### Install required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aKLiNSv6wTr6"
      },
      "outputs": [],
      "source": [
        "# Langchain\n",
        "!pip -q install langchain\n",
        "\n",
        "# Library to communicate with HF hub\n",
        "!pip -q install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UAsnCf4Z9e1K"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BrFlLJv3-_4N"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain_huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0sgU1wiq7Hw"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u0IydGx_q78h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMBDqFT0kkgH"
      },
      "source": [
        "### **Provide your HuggingFace api key/access token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "i9TSeu4cq1Jw"
      },
      "outputs": [],
      "source": [
        "# Enter your HuggingFace access token when prompted\n",
        "from google.colab import userdata\n",
        "pass_token = userdata.get('rajiv-ranjan-redhat-cds-miniproject-1')\n",
        "\n",
        "# pass_token = getpass(\"Enter your HuggingFace access token: \")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = pass_token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = pass_token\n",
        "\n",
        "del pass_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgzDSiux-EuO"
      },
      "source": [
        "### **Exploring Open Source LLMs hosted on HuggingFace**\n",
        "\n",
        ">**I.** `HuggingFaceH4/zephyr-7b-beta`\n",
        ">\n",
        ">**II.** `mistralai/Mistral-7B-Instruct-v0.2`\n",
        ">\n",
        ">**III.** `LlaMa2`\n",
        "\n",
        "[LangChain link](https://python.langchain.com/docs/integrations/chat/huggingface) for using Hugging Face LLM's as chat models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVK3LSt6mzKa"
      },
      "source": [
        "### **I.** [**HuggingFaceH4/zephyr-7b-beta**](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QudSou2y-T4i"
      },
      "outputs": [],
      "source": [
        "# Import HuggingFace model abstraction class from langchain\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jVRB1nztyTUm"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens = 512,\n",
        "    top_k = 30,\n",
        "    temperature = 0.1,\n",
        "    repetition_penalty = 1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qrneNiybIFl_",
        "outputId": "216e2500-067e-4dde-ddd4-7037b0279ba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "1. Start with the basics: Before diving into complex programming concepts, itâ€™s essential to start with the basics. Learn the syntax and structure of the programming language you want to learn. This will help you build a strong foundation and make it easier to understand more advanced concepts later on.\n",
            "\n",
            "2. Practice, practice, practice: The key to learning programming is to practice regularly. Set aside time each day or week to work on coding projects, even if theyâ€™re small. This will help you build your skills and confidence over time.\n",
            "\n",
            "3. Collaborate with others: Join online communities or attend local meetups to connect with other programmers. Collaborating with others can help you learn new techniques and approaches, as well as provide support and feedback as you work through challenges.\n",
            "\n",
            "4. Use resources wisely: There are many online resources available for learning programming, but itâ€™s important to use them wisely. Focus on resources that are clear, concise, and provide practical examples. Avoid resources that are too theoretical or abstract, as these can be confusing and difficult to apply in practice.\n",
            "\n",
            "5. Stay motivated: Learning programming can be challenging, especially when you encounter errors or bugs in your code. Stay motivated by setting achievable goals, celebrating small wins, and seeking out inspiration from successful programmers. Remember that programming is a journey, not a destination, and that progress takes time and persistence.\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"How to learn programming? give 5 points\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4N14UjkBaws"
      },
      "source": [
        "#### **Prompt Template**\n",
        "\n",
        "Prompt templates are predefined recipes for generating prompts for language models.\n",
        "\n",
        "A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n",
        "\n",
        "LangChain provides tooling to create and work with prompt templates.\n",
        "\n",
        "To know more about Prompt template, refer [here](https://python.langchain.com/docs/modules/model_io/prompts/quick_start)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v-l3yx9hQs2"
      },
      "source": [
        "#### **Example-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NIjgIglFbjDG",
        "outputId": "6385a84a-e970-40ca-bca1-9240cc3c9143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a funny joke about Trump.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "messages = prompt_template.format(adjective=\"funny\", content=\"Trump\")\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iUOnAu5c_Hqw"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "chat_model = ChatHuggingFace(llm = llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DjM9t0INbjLg",
        "outputId": "76054543-2450-4591-9082-472f98dedc92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did Donald Trump take a bath with a soldier?\n",
            "\n",
            "To make America drain again!\n",
            "\n",
            "(Author's note: This joke plays off the \"Make America Great Again\" slogan used by President Trump during his campaign, and how he has also touted the phrase \"Make America Drain Again\" during the COVID-19 crisis in response to claims that urine in men's toilets at some Trump properties wasn't being cleaned up.)\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPeo4FQBhL0l"
      },
      "source": [
        "#### **Example-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bat0JTd7GrJx"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mQe8qxghewsF",
        "outputId": "211cb674-bbdc-421c-9786-3e4b9c7bb6a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me 5 facts about Tajmahal.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me {count} facts about {event_or_place}.\"\n",
        ")\n",
        "user_msg = prompt_template.format(count=5, event_or_place=\"Tajmahal\")\n",
        "user_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GdGDCoQhmuDL"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You're a knowledgeable historian\"),\n",
        "    HumanMessage(content=user_msg),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vCpfJTPCAqGP"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9kg1GDWsCRbL"
      },
      "outputs": [],
      "source": [
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "27Iyhp-5DJ5-",
        "outputId": "c8fa2c2a-da49-414a-9f00-6624f5af3623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HuggingFaceH4/zephyr-7b-beta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "chat_model.model_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s_EReM93znzV",
        "outputId": "59087e7f-063b-42c7-c037-98f493d5b2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou're a knowledgeable historian</s>\\n<|user|>\\nTell me 5 facts about Tajmahal.</s>\\n<|assistant|>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from langchain_huggingface import ChatHuggingFace\n",
        "from transformers import AutoTokenizer # Import AutoTokenizer\n",
        "\n",
        "# Assume llm is already defined as HuggingFaceEndpoint\n",
        "\n",
        "# Get the tokenizer from the repo_id used by the llm\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm.repo_id) # Get tokenizer from llm's repo_id\n",
        "\n",
        "# Initialize ChatHuggingFace with both llm and tokenizer\n",
        "chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)\n",
        "\n",
        "# Now the chat_model should have a tokenizer and the following line should work\n",
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9niBkmKIDMVI"
      },
      "outputs": [],
      "source": [
        "#chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "px6WPyxN2DYj",
        "outputId": "56ad00b8-ec7b-4aab-d5ad-13c434a4b6f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You're a knowledgeable historian</s>\n",
            "<|user|>\n",
            "Tell me 5 facts about Tajmahal.</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(chat_model._to_chat_prompt(messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZduaUIkl-VpI",
        "outputId": "eabf8fa6-0924-470d-d2df-f3918beb9e39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Construction of the Taj Mahal began in 1632 and was completed in 1653. It was built by the Mughal Emperor Shah Jahan in memory of his wife Mumtaz Mahal, who died giving birth to their 14th child.\n",
            "\n",
            "2. The Taj Mahal is made of white marble and is a fine example of Mughal architecture, which combines elements of Indian, Persian, and Islamic design. The intricate carvings and calligraphy on the walls are decorated with precious gems such as lapis lazuli, turquoise, and jade.\n",
            "\n",
            "3. The Taj Mahal is not just a mausoleum but also a garden complex with water features and symmetrical buildings. The main structure is surrounded by a series of gardens, walkways, and reflecting pools, all laid out in a precise geometric pattern.\n",
            "\n",
            "4. The Taj Mahal is not just a cultural icon but also a significant historical and archaeological site. The tombs of Mumtaz Mahal and Shah Jahan are buried inside the main structure, and the complex is also home to the graves of two other family members.\n",
            "\n",
            "5. The Taj Mahal is now a UNESCO World Heritage Site and one of the most visited tourist attractions in India. It attracts millions of visitors every year, and its iconic beauty and historical significance have made it a symbol of love, loss, and Indian heritage.\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USL19U5MKapo"
      },
      "source": [
        "#### **Example-3**\n",
        "\n",
        "The prompt to *chat models* is a list of chat messages.\n",
        "\n",
        "Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jsZWlMZEEcth"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TBLyJ-sFk8Hf"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful A {persona}.\"),\n",
        "        (\"human\", \"Hello, how are you doing?\"),\n",
        "        (\"ai\", \"I'm doing well, thanks!\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1rr0S0bklaIr"
      },
      "outputs": [],
      "source": [
        "persona = \"\"\"trustworthy friend\"\"\"\n",
        "query= \"\"\"\n",
        "I am not able to understand the concept taught in class. \\\n",
        "Could you please suggest something? \\\n",
        "I need your help. Give 5 points to work on.\n",
        "\"\"\"\n",
        "messages = chat_template.format_messages(persona = persona, user_input=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XoEgU1O5LlK7",
        "outputId": "59349f95-7305-43cf-97b6-430c46af2d86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful A trustworthy friend.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='\\nI am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\\n', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aP9IQ7q8mYu8",
        "outputId": "3287a0b6-74cb-41b6-9cfe-ee46b57b97ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou are a helpful A trustworthy friend.</s>\\n<|user|>\\nHello, how are you doing?</s>\\n<|assistant|>\\nI'm doing well, thanks!</s>\\n<|user|>\\n\\nI am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\\n</s>\\n<|assistant|>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SOm5ZjnQ24jm",
        "outputId": "a90fbed4-26c1-478f-be29-a96daed538fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a helpful A trustworthy friend.</s>\n",
            "<|user|>\n",
            "Hello, how are you doing?</s>\n",
            "<|assistant|>\n",
            "I'm doing well, thanks!</s>\n",
            "<|user|>\n",
            "\n",
            "I am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\n",
            "</s>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(chat_model._to_chat_prompt(messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tKH257JH_6qi",
        "outputId": "39b3d9b1-b87f-4f74-a8db-d7009f0e4630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! I'd be more than happy to help you. Here are five points to work on as you try to understand the concept:\n",
            "\n",
            "1. Make sure you have a clear understanding of the definitions and key terms related to the concept. Writing them down or creating a mind map might help clarify any confusion.\n",
            "\n",
            "2. Try to relate the concept to real-life examples or situations to help make it more tangible. This can help you see how the concept works in practice.\n",
            "\n",
            "3. Review any class notes or handouts you have on the concept. Highlight or underline any key points or examples.\n",
            "\n",
            "4. Practice working with the concept through exercises or quizzes. This can help reinforce your understanding and identify any areas where you may still be struggling.\n",
            "\n",
            "5. Lastly, don't be afraid to ask your teacher for clarification or additional examples. They are there to help you learn!\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si6NaQVggzQq"
      },
      "source": [
        "**Practice :**\n",
        "Create a prompt template that takes context and a question from the user and answers the question based on the given context.\n",
        "\n",
        "Hint: Keep the context in the system message and the question in the human message.\n",
        "\n",
        "**Context:** Meet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\n",
        "audiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,  Aryan's captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\n",
        "ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled  talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\n",
        "\n",
        "**Question:** What awards did Aryan Kapoor win for his contributions to the entertainment industry, and in which years were they received?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "t4StVsQ7lQS7"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant and know this context ```{context}``\"),\n",
        "        (\"human\", \" pls reply ```{question}``` in points based on the context provided. Strictly don't add extra facts and information ?\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MN6-TQaliTMT"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"What awards did Aryan Kapoor win for his contributions to the entertainment industry,\n",
        "and in which years were they received?\"\"\"\n",
        "context= \"\"\"\n",
        "Meet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\n",
        "audiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the\n",
        " prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his\n",
        "  vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,\n",
        "  Aryan's captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\n",
        "  ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing\n",
        "   his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled\n",
        "   talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\n",
        "\"\"\"\n",
        "messages = chat_template.format_messages(context = context,  question=question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5u6PwxIqjdfn",
        "outputId": "13277021-eeb7-4b6c-b669-0e0f07106d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|system|>\\nYou are a helpful assistant and know this context ```\\nMeet Aryan Kapoor, a rising star in the entertainment industry whose talent knows no bounds. In 2023, Aryan captivated\\naudiences with his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity,\" earning him the\\n prestigious Best Actor award at the National Film Awards. His versatility shone brightly in 2024 when he showcased his\\n  vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon.\" The same year,\\n  Aryan\\'s captivating screen presence garnered him the coveted Filmfare Critics Award for Best Actor. As his star continued to\\n  ascend, Aryan was honored with the International Icon of the Year award at the Global Entertainment Awards in 2025, recognizing\\n   his global impact and widespread admiration. With each role he undertakes, Aryan Kapoor cements his status as an unrivaled\\n   talent in the world of cinema, leaving audiences eagerly anticipating his next masterpiece.\\n``</s>\\n<|user|>\\n pls reply ```What awards did Aryan Kapoor win for his contributions to the entertainment industry,\\nand in which years were they received?``` in points based on the context provided. Strictly don\\'t add extra facts and information ?</s>\\n<|assistant|>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hs-ph2ePAFb5",
        "outputId": "e992a3a5-f34a-4f4a-8691-174a93173931",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. In 2023, Aryan Kapoor won the Best Actor award at the National Film Awards for his mesmerizing performance in the critically acclaimed film \"Echoes of Eternity\".\n",
            "\n",
            "2. In 2024, Aryan Kapoor showcased his vocal prowess as a playback singer in the chart-topping soundtrack of the blockbuster movie \"Infinite Horizon\".\n",
            "\n",
            "3. In the same year, Aryan received the Filmfare Critics Award for Best Actor for his captivating screen presence.\n",
            "\n",
            "4. In 2025, Aryan Kapoor was honored with the International Icon of the Year award at the Global Entertainment Awards for his global impact and widespread admiration.\n",
            "\n",
            "5. These awards acknowledge Aryan Kapoor's exceptional talent as an actor and rising star in the entertainment industry, reflecting his versatility, versatility, and consistent performance in multiple roles.\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BPY4s_hGQv-"
      },
      "source": [
        "#### **Output Parsers**\n",
        "\n",
        "Let's start with defining how we would like the LLM output to look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "x1nyuc_1GO6u",
        "outputId": "d3de627d-27b8-4daa-935c-943688e547d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# An example output format\n",
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qW37-qdYGaOY"
      },
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ppJOtjsEGdL_"
      },
      "outputs": [],
      "source": [
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift or present for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gQDimFP3Gp5a",
        "outputId": "91efa051-a167-4feb-9ff2-4294015cf801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift or present for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Creating prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "print(prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "MYur071rMz6D",
        "outputId": "1af439bb-ae8e-4eef-b358-d7a1e8ff8bf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"gift\": false,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\n",
            "    \"This leaf blower is slightly more expensive than the other leaf blowers out there, but I think itâ€™s worth it for the extra features.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "response = chat_model.invoke(messages)\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cV1eVBvyHhfY",
        "outputId": "d2aa9d5c-26c9-4e3f-ee91-18c9639ab369",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "print(type(response.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3bgzsf7pgP"
      },
      "source": [
        "#### **Parse the LLM output string into a structured data**:\n",
        "\n",
        "Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support [built-in ways to return structured output](https://python.langchain.com/docs/how_to/structured_output/), not all do.\n",
        "\n",
        "Output parsers are classes that help structure language model responses.\n",
        "\n",
        "Below we go over the main type of output parser, the `PydanticOutputParser`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8N7zx3mCoIj"
      },
      "source": [
        "[Structured output parser](https://python.langchain.com/docs/how_to/output_parser_structured/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "EH0hArbY7n4t"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Define your desired data structure\n",
        "class Product_Info(BaseModel):\n",
        "    \"\"\" Product service info.\"\"\"\n",
        "\n",
        "    gift: str = Field(description=\"Was the item purchased\\\n",
        "                             as a gift for someone else? \\\n",
        "                             Answer True if yes,\\\n",
        "                             False if not or unknown.\")\n",
        "    delivery_days: int = Field(description=\"How many days\\\n",
        "                                      did it take for the product\\\n",
        "                                      to arrive? If this \\\n",
        "                                      information is not found,\\\n",
        "                                      output -1.\")\n",
        "    price_value: list = Field(description=\"Extract sentences about the value or \\\n",
        "                                    price, and output them as a \\\n",
        "                                    comma separated Python list.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QzpGk67j24Ti"
      },
      "outputs": [],
      "source": [
        "# Set up a parser + inject instructions into the prompt template\n",
        "parser = PydanticOutputParser(pydantic_object = Product_Info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sJp6QGTF3Ma7"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{text}\\n\",\n",
        "    input_variables=[\"text\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions() + \"\\nOutput the answer as a JSON object.\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "kLPLDAGm9c0F",
        "outputId": "8630530a-6143-49c9-c5c1-ddae1a6a1638",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Product_Info(gift='False', delivery_days=2, price_value=['This leaf blower is pretty amazing. ', 'It has four settings:candle blower, gentle breeze, windy city, and tornado. ', 'I think my wife liked it so much she was speechless. ', \"So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. \", \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features. \"])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# And a query intended to prompt a language model to populate the data structure.\n",
        "prompt_and_model = prompt | llm\n",
        "output = prompt_and_model.invoke({\"text\": customer_review})\n",
        "result = parser.invoke(output)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "u3BgDcCa7tcI",
        "outputId": "4906eeb5-1088-4579-f73d-ff274f67bbb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "2\n",
            "['This leaf blower is pretty amazing. ', 'It has four settings:candle blower, gentle breeze, windy city, and tornado. ', 'I think my wife liked it so much she was speechless. ', \"So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. \", \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features. \"]\n"
          ]
        }
      ],
      "source": [
        "print(result.gift)\n",
        "print(result.delivery_days)\n",
        "print(result.price_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br30bwaPKs0N"
      },
      "source": [
        "#### [**Customizing Conversational Memory**](https://python.langchain.com/docs/how_to/chatbots_memory/)\n",
        "\n",
        "LangChain can helps in building better chatbots, or have\n",
        "an LLM with more effective chats by better managing\n",
        "what it remembers from the conversation you've had so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "TBDcA56ovm_w"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "C1_WmMeZv1HV"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-zGGnVncvtWt"
      },
      "outputs": [],
      "source": [
        "store = {}      # to store chat messages\n",
        "\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    runnable = chain,\n",
        "    get_session_history = get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "PTe4OV8Kwoat",
        "outputId": "6157e60e-628e-4413-fa8f-a1c13317fb15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"\\nHello, James! I'm glad you're here today. My name is [your name] and I'm a helpful AI (Artificial Intelligence) designed to assist you with any questions or requests you may have. Is there anything specific I can help you with today? Simply ask away, and I'll do my best to provide a prompt and accurate response. Thank you for choosing to work with me!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 57, 'total_tokens': 144}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--f10f4b06-bf4d-41b0-b54f-754c49151373-0', usage_metadata={'input_tokens': 57, 'output_tokens': 87, 'total_tokens': 144})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"Hi, my name is James\"},\n",
        "    {\"configurable\": {\"session_id\": \"user1\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ALK5ZboQw53S",
        "outputId": "f423b513-7aca-4468-d390-961ef48fca84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"\\nHello, James! I'm glad you're here today. My name is [your name] and I'm a helpful AI (Artificial Intelligence) designed to assist you with any questions or requests you may have. Is there anything specific I can help you with today? Simply ask away, and I'll do my best to provide a prompt and accurate response. Thank you for choosing to work with me!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 57, 'total_tokens': 144}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--f10f4b06-bf4d-41b0-b54f-754c49151373-0', usage_metadata={'input_tokens': 57, 'output_tokens': 87, 'total_tokens': 144})])}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "HQPxFgMEwz0u",
        "outputId": "decadb12-ccfb-49bc-aed7-02cedb3edfa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"As an AI, I don't have the ability to remember specific information beyond what is provided to me. I was not designed to have a personal memory or capacity to remember names. However, I'll be happy to address you by the name you provided me in our initial exchange, James, as long as you stay consistent with that name throughout our interaction. Is there anything else I can help you with, James?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 176, 'total_tokens': 262}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--eaff9432-cf80-4e71-a271-fbefb39ce636-0', usage_metadata={'input_tokens': 176, 'output_tokens': 86, 'total_tokens': 262})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "chain_with_message_history.invoke(\n",
        "    input = {\"input\": \"Do you remember my name?\"},\n",
        "    config = {\"configurable\": {\"session_id\": \"user1\"}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vsmS0nLgxdxe",
        "outputId": "bd357d3c-16d9-440f-cf82-e54780d68819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"\\nHello, James! I'm glad you're here today. My name is [your name] and I'm a helpful AI (Artificial Intelligence) designed to assist you with any questions or requests you may have. Is there anything specific I can help you with today? Simply ask away, and I'll do my best to provide a prompt and accurate response. Thank you for choosing to work with me!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 57, 'total_tokens': 144}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--f10f4b06-bf4d-41b0-b54f-754c49151373-0', usage_metadata={'input_tokens': 57, 'output_tokens': 87, 'total_tokens': 144}), HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I don't have the ability to remember specific information beyond what is provided to me. I was not designed to have a personal memory or capacity to remember names. However, I'll be happy to address you by the name you provided me in our initial exchange, James, as long as you stay consistent with that name throughout our interaction. Is there anything else I can help you with, James?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 176, 'total_tokens': 262}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--eaff9432-cf80-4e71-a271-fbefb39ce636-0', usage_metadata={'input_tokens': 176, 'output_tokens': 86, 'total_tokens': 262})])}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "gbW0TdOVxef3",
        "outputId": "f2ef1442-915c-483f-d5de-2ebc913ab75e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='As an AI, I unfortunately do not have access to your personal information or name. Please provide me with your name, and I will address you by that name throughout our interaction. Is there anything else I can help you with today, [enter your name here]?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 297, 'total_tokens': 352}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--013709c8-82be-49b3-a237-e404160b5901-0', usage_metadata={'input_tokens': 297, 'output_tokens': 55, 'total_tokens': 352})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "chain_with_message_history.invoke(\n",
        "    input = {\"input\": \"Can you tell me what is my name?\"},\n",
        "    config = {\"configurable\": {\"session_id\": \"user1\"}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Wb2HR0MAvm3j",
        "outputId": "e1bf6114-3bc3-41ab-c3f4-85c9d309dd6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"\\nHello, James! I'm glad you're here today. My name is [your name] and I'm a helpful AI (Artificial Intelligence) designed to assist you with any questions or requests you may have. Is there anything specific I can help you with today? Simply ask away, and I'll do my best to provide a prompt and accurate response. Thank you for choosing to work with me!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 57, 'total_tokens': 144}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--f10f4b06-bf4d-41b0-b54f-754c49151373-0', usage_metadata={'input_tokens': 57, 'output_tokens': 87, 'total_tokens': 144}), HumanMessage(content='Do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I don't have the ability to remember specific information beyond what is provided to me. I was not designed to have a personal memory or capacity to remember names. However, I'll be happy to address you by the name you provided me in our initial exchange, James, as long as you stay consistent with that name throughout our interaction. Is there anything else I can help you with, James?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 176, 'total_tokens': 262}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--eaff9432-cf80-4e71-a271-fbefb39ce636-0', usage_metadata={'input_tokens': 176, 'output_tokens': 86, 'total_tokens': 262}), HumanMessage(content='Can you tell me what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='As an AI, I unfortunately do not have access to your personal information or name. Please provide me with your name, and I will address you by that name throughout our interaction. Is there anything else I can help you with today, [enter your name here]?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 297, 'total_tokens': 352}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--013709c8-82be-49b3-a237-e404160b5901-0', usage_metadata={'input_tokens': 297, 'output_tokens': 55, 'total_tokens': 352})])}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSbCSUOfxXa-"
      },
      "source": [
        "### **II. mistralai/Mistral-7B-Instruct-v0.3**\n",
        "\n",
        "<font color='#990000'> **Note that you need to ask for access before using this model. Go to https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 and click on \"Agree and access repository\".** </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "h_733Aai1RG3"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "_XZllnjBzqw0"
      },
      "outputs": [],
      "source": [
        "question = \"How to learn programing? Give 5 examples. \""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# Remove max_new_tokens from model_kwargs and pass it directly\n",
        "model_kwargs = {\"token\": os.environ[\"HF_TOKEN\"]}\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"conversational\", # Changed task to conversational\n",
        "    temperature=0.5,\n",
        "    max_new_tokens=128\n",
        ")"
      ],
      "metadata": {
        "id": "qp3XpAZwYmNS"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-aqIG9-lyFdF"
      },
      "outputs": [],
      "source": [
        "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# model_kwargs = {\"max_new_tokens\": 128, \"token\": os.environ[\"HF_TOKEN\"]}\n",
        "\n",
        "# llm = HuggingFaceEndpoint(repo_id=repo_id,\n",
        "#                           temperature=0.5,\n",
        "#                           model_kwargs= model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "HBTtXIUh6WZ9"
      },
      "outputs": [],
      "source": [
        "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# llm = HuggingFaceEndpoint(\n",
        "#     repo_id=repo_id,\n",
        "#     temperature=0.5,\n",
        "#     max_new_tokens=128  # Pass it directly, not inside model_kwargs\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "bXJMBgfy0A5Q",
        "outputId": "6a13bd0e-cdd8-49bb-b85f-ee1329fb92c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. **Online Courses**: Websites like Codecademy, freeCodeCamp, and Coursera offer interactive courses in various programming languages such as Python, JavaScript, and Java. These platforms provide a structured learning path, practical exercises, and community support.\n",
            "\n",
            "2. **Books**: Books are a traditional and effective way to learn programming. \"Learn Python the Hard Way\" by Zed Shaw, \"Eloquent JavaScript\" by Marijn Haverbeke, and \"Head First Java\" by Kathy Sierra and Bert Bates are popular choices for beginners.\n",
            "\n",
            "3. **Tutorials and Guides**: Websites like W3Schools, MDN Web Docs, and GeeksforGeeks provide comprehensive tutorials and guides for various programming languages and topics. These resources are often free and can be used at your own pace.\n",
            "\n",
            "4. **Coding Bootcamps**: These are intensive, short-term training programs that teach coding skills. They usually last for a few months and cover multiple programming languages and technologies. Examples include General Assembly, Hack Reactor, and App Academy.\n",
            "\n",
            "5. **Open Source Projects**: Contributing to open source projects is a great way to learn from experienced developers and gain practical experience. Websites like GitHub and GitLab host thousands of open source projects. Start by finding a project that interests you, fork it, and make small contributions. This will help you understand the codebase, learn best practices, and build your portfolio.\n"
          ]
        }
      ],
      "source": [
        "# Get the tokenizer from the repo_id used by the llm\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm.repo_id)\n",
        "\n",
        "# Initialize ChatHuggingFace with both llm and tokenizer\n",
        "chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)\n",
        "\n",
        "# Now use the chat_model to invoke with the HumanMessage\n",
        "response = chat_model.invoke([HumanMessage(content=question)])\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmRI-p9fp0SR"
      },
      "source": [
        "#### **Prompt Template**\n",
        "\n",
        "**Example-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "xiZNTCsGNpPC"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "WnF-Qc_FrTZV"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "SHJWqMg3yxa_"
      },
      "outputs": [],
      "source": [
        "template_s = \"\"\"You are a {style1}.\\\n",
        "Tell me  {count} facts about {event_or_place}.```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "wHucwPZ7yVPF"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(template_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "jH_lqJRbzaHt",
        "outputId": "73960a21-9532-4db5-a460-a841c99b8336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['count', 'event_or_place', 'style1'], input_types={}, partial_variables={}, template='You are a {style1}.Tell me  {count} facts about {event_or_place}.```\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "fDyBeGD8zej9",
        "outputId": "eca81eef-93a7-41e2-c0ef-b1a56f8f688b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['count', 'event_or_place', 'style1']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "QXUiH0sszkUe"
      },
      "outputs": [],
      "source": [
        "user_messages = prompt_template.format_messages(\n",
        "                    style1=\"knowledgeable historian\",\n",
        "                    count=5,\n",
        "                    event_or_place=\"Tajmahal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "qP62G5bz0KBJ",
        "outputId": "8af8a8de-d99a-453d-ec68-73f2a262f08e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='You are a knowledgeable historian.Tell me  5 facts about Tajmahal.```\\n', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "user_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQvXHPho54pT"
      },
      "source": [
        "**<font color='#990000'> Note: Before running the below code cell, go to the following link and click on \"Agree and access repository\" button </font>**\n",
        "\n",
        "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Q5IfurZcyYt8",
        "outputId": "f76ea718-eba5-430b-b0dd-e840adcca236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mistralai/Mistral-7B-Instruct-v0.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "chat_model.model_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the tokenizer from the repo_id used by the llm\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm.repo_id)\n",
        "\n",
        "# Initialize ChatHuggingFace with both llm and tokenizer\n",
        "chat_model = ChatHuggingFace(llm=llm, tokenizer=tokenizer)\n",
        "\n",
        "# Now the chat_model should have a tokenizer and the following line should work\n",
        "chat_model.model_id"
      ],
      "metadata": {
        "id": "BvXF5WBYeHq5",
        "outputId": "9fef78a8-6b1a-4f2e-cd7d-ca674d366c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mistralai/Mistral-7B-Instruct-v0.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ybxEq_9JqywX",
        "outputId": "e7685e29-97f5-45d6-944f-fd649b6adc3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] You are a knowledgeable historian.Tell me  5 facts about Tajmahal.```\\n[/INST]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "chat_model._to_chat_prompt(user_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "SYI0aD8Hsty_",
        "outputId": "cec9972f-6241-4af4-f6af-2e4d8ef975d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. The Taj Mahal is a mausoleum located in Agra, India, built by Mughal Emperor Shah Jahan in memory of his third wife, Mumtaz Mahal. It is widely recognized as the \"jewel of Muslim art in India\" and a universal symbol of love.\n",
            "\n",
            "2. Construction of the Taj Mahal began in 1632 and was completed in 1653, taking approximately 20 years and 20,000 workers. The structure is made of white marble and is adorned with semi-precious stones, inlaid into intricate patterns.\n",
            "\n",
            "3. The Taj Mahal is symmetrical, with a central dome flanked by four minarets. The main chamber houses the cenotaphs of Shah Jahan and Mumtaz Mahal, while the actual graves are located in a crypt beneath the structure.\n",
            "\n",
            "4. The Taj Mahal is set in a large complex that includes a mosque, a guest house, and a reflecting pool. The complex is enclosed by high walls and is entered through a massive gateway.\n",
            "\n",
            "5. The Taj Mahal is not just a monument of love, but also a testament to the Mughal architectural style, which combined elements of Persian, Indian, and Islamic architecture. The Taj Mahal is considered one of the finest examples of Mughal architecture and is listed as a UNESCO World Heritage Site.\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(user_messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wakjVUVZp2YH"
      },
      "source": [
        "**Example-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "jiFDpiyENyOS"
      },
      "outputs": [],
      "source": [
        "messages = [HumanMessage(content=\"How to learn programming? give 2 points\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "6SO2qIrju7rc",
        "outputId": "9a840d36-c79e-4fba-d61e-3d2942f403a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] How to learn programming? give 2 points[/INST]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "chat_model._to_chat_prompt(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "jEhKW2msODT2",
        "outputId": "a842430d-365f-4930-ab6d-9b6ff53dabd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. Start with the basics: Begin with a beginner-friendly programming language like Python or JavaScript. These languages have simple syntax and are widely used, making them great for beginners. You can find numerous online resources, tutorials, and courses to help you get started.\n",
            "\n",
            "2. Practice consistently: Programming is a skill that requires regular practice to master. Set aside time each day to work on coding projects, solve problems, and complete exercises. Websites like Codecademy, LeetCode, and HackerRank offer a variety of coding challenges that can help you improve your skills. Additionally, consider working on personal projects or contributing to open-source projects to gain real-world experience.\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaaC7-gu_KOK"
      },
      "source": [
        "### **III.** **[Llama2](https://ai.meta.com/llama/)** ***(Optional)***\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        ">For using this model you have to click `Download models` link available in [this](https://ai.meta.com/llama/) reference which re-direct to a **form for request**. It may take 1 hour to 2 days to get the **approval** for usage of this model through HuggingFace. You will get an email for the same.\n",
        "\n",
        ">Once the request is approved, connect to **GPU runtime** for below steps. Also, you need to provide your HF api key/access token.\n",
        "\n",
        "Trying Llama2-2-7b model:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "CQPKoaTRiPPh",
        "outputId": "5e2d543d-4030-4dec-ed7b-fd46a10a0dc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; torch._dynamo.reset()\"\n",
        "!rm -rf ~/.cache/torch"
      ],
      "metadata": {
        "id": "QSzd8Hdnioz4"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "Y3USJfxHi88_",
        "outputId": "d168effa-97a4-446e-f808-43f1b1f35c3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# !pip uninstall -y transformers accelerate langchain xformers bitsandbytes torch"
      ],
      "metadata": {
        "id": "FYoGuEprcCt9"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# !pip install -q torch==2.1.0"
      ],
      "metadata": {
        "id": "2hX0__WIcNre"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "HbzbpWIK_J4T"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q transformers accelerate langchain xformers bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "zW69WOWSzwkG"
      },
      "outputs": [],
      "source": [
        "# Enter your HuggingFace access token when prompted\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "from google.colab import userdata\n",
        "pass_token = userdata.get('rajiv-ranjan-redhat-cds-miniproject-1')\n",
        "\n",
        "# pass_token = getpass(\"Enter your HuggingFace access token: \")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = pass_token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = pass_token\n",
        "\n",
        "del pass_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw7oXd2pAE1f"
      },
      "source": [
        "## Initializing the Hugging Face Pipeline\n",
        "\n",
        "The first thing we need to do is initialize a `text-generation` pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:\n",
        "\n",
        "* A LLM, in this case it will be `meta-llama/Llama-2-7b-chat-hf`.\n",
        "\n",
        "* The respective tokenizer for the model.\n",
        "\n",
        "We'll explain these as we get to them, let's begin with our model.\n",
        "\n",
        "We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "--40_VMGJBEz"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "h49z-C1RJC9D",
        "outputId": "6b7fe48c-4bb5-4e1f-ccac-60625b71f3bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "BzK3uhhYbTvr"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "lsWptt-rYXqu",
        "outputId": "4ccb666c-e6ad-4a4c-b188-52abbdc85372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.1+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "WpuAxDJEbXtR",
        "outputId": "51f0296d-53ae-4a7c-fbee-dc7aa8a8b83c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.52.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqH62ls48J9k"
      },
      "source": [
        "<font color='#990000'> **Note: Before running the below code cell, go to the following link and execute the following steps.** </font>\n",
        "\n",
        "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "**Step-1:** You need to share contact information with Meta to access this model\n",
        "\n",
        "**Step-2:** Under **'LLAMA 2 COMMUNITY LICENSE AGREEMENT'** section, click on **'Expand to review and access'** button.\n",
        "\n",
        "**Step-3:** Then fill up the form and finally check-in the check box for accepting the terms of the license.\n",
        "\n",
        "**Step-4:** Click on **'Submit'** button.\n",
        "\n",
        "**Step-5:** After submitting you need to wait till the owner has given you the access. **You will receive one email notification.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "T2Cp6jnomS0S",
        "outputId": "45e57c36-a4b7-4850-a9d9-65c58fce4423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7169b1337815445ea8afe1e3d7a2b43f",
            "775435ee38324c99ba6e3b030701bd49",
            "9b4a3f0be4724509a9f5f33b572067e8",
            "221c4fd3d4d14d3da5c81da3a75809ec",
            "f0255f9f8e9c4fa6b4b22fad227e1e23",
            "ae1515f534234d189f5e925978d6ac16",
            "33786824f4df4f7bb3e353558008f7c3",
            "836723b0f3624186988dd6d9cc67b5d1",
            "5553c2fd8d344229bf20c7af50be5d30",
            "574c508fea564f2eb9a65a7b77a92ddc",
            "f202c7a77df84e1983cf9e30832e8265"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7169b1337815445ea8afe1e3d7a2b43f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1000,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "8n7MjpX4nM-r",
        "outputId": "12c021f0-670c-4d3b-a7b1-dc95ab96dc64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How to learn programming?\n",
            "\n",
            "Learning programming can be a challenging but rewarding experience. Here are some steps you can take to get started:\n",
            "\n",
            "1. Choose a programming language: With so many programming languages to choose from, it's important to pick one that interests you and has good resources available. Some popular languages for beginners include Python, JavaScript, and HTML/CSS.\n",
            "2. Learn the basics: Once you've chosen a language, start by learning the basics. This includes understanding syntax, data types, variables, loops, and control structures.\n",
            "3. Practice, practice, practice: The best way to learn programming is by writing code. Start with simple exercises and projects, and gradually work your way up to more complex tasks.\n",
            "4. Use online resources: There are many online resources available to help you learn programming, including tutorials, videos, and coding challenges. Some popular platforms include Codecademy, FreeCodeCamp, and Coursera.\n",
            "5. Join a community: Joining a community of other programmers can be a great way to learn and get support. Look for online forums or local meetups in your area.\n",
            "6. Take online courses: Online courses are a great way to learn programming, and many platforms offer courses on a wide range of topics.\n",
            "7. Read books: There are many great books available to learn programming, including \"Code Complete\" by Steve McConnell, \"Clean Code\" by Robert C. Martin, and \"The Pragmatic Programmer\" by Andrew Hunt and David Thomas.\n",
            "8. Work on projects: As you learn, start working on small projects to apply what you've learned. This will help you to reinforce your understanding and to gain practical experience.\n",
            "9. Learn by doing: Learning programming is not just about reading and watching, it's about doing. Start coding as soon as possible, and keep practicing.\n",
            "10. Be patient: Learning programming takes time and effort, so be patient with yourself and don't get discouraged if you don't understand something at first.\n",
            "11. Learn problem-solving skills: Programming is all about solving problems, so it's important to learn how to approach and solve problems in a logical and methodical way.\n",
            "12. Learn algorithms and data structures: As you progress in your learning, it's important to understand algorithms and data structures, which are the building blocks of programming.\n",
            "13. Learn a second language: Once you have a good grasp of one language, it's worth learning a second language to expand your horizons and open up more job opportunities.\n",
            "14. Keep learning: Programming is a constantly evolving field, so it's important to stay up-to-date with the latest technologies and trends.\n",
            "15. Find a mentor: Having a mentor who is experienced in programming can be a great way to learn and get support.\n",
            "\n",
            "Remember, learning programming takes time and effort, but with persistence and dedication, you can become a proficient programmer.\n"
          ]
        }
      ],
      "source": [
        "res = pipeline(\"How to learn programming?\")\n",
        "print(res[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzqImT9RAdAg"
      },
      "source": [
        "#### **Now implementing with LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "hLkLetCxWF-t"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain\n",
        "!pip -q install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "h71jByoBAffB",
        "outputId": "827f7001-8445-4eee-f2ed-4851058b0df4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-92-2980594009.py:3: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0.7})\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0.7})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "2tr-YGpvnr0X",
        "outputId": "cf8291ce-f304-48d8-ffba-933396acb8b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How to learn programming?\n",
            "\n",
            "Learning programming can be a challenging task, but with the right approach and resources, it can also be a rewarding and exciting experience. Here are some steps you can take to learn programming:\n",
            "\n",
            "1. Start with the basics: Begin by learning the basic concepts of programming such as data types, variables, loops, and control structures. Understand the syntax and grammar of the programming language you are learning.\n",
            "2. Choose a programming language: There are many programming languages to choose from, and each has its own strengths and weaknesses. Choose a language that interests you and has good resources available. Some popular programming languages for beginners include Python, JavaScript, HTML/CSS, and Java.\n",
            "3. Practice coding: Practice coding by writing small programs or exercises to solve specific problems. Start with simple programs and gradually move on to more complex ones.\n",
            "4. Use online resources: There are many online resources available to learn programming, including video tutorials, online courses, and coding communities. Some popular online resources include Codecademy, FreeCodeCamp, and GitHub.\n",
            "5. Join a community: Join a community of programmers to learn from others, get feedback on your code, and stay motivated. Participate in online forums, attend meetups, or join a coding group.\n",
            "6. Work on projects: Once you have a good understanding of the basics, start working on small projects to apply your knowledge and gain practical experience. Start with small projects such as building a personal website, creating a simple game, or developing a tool for a specific task.\n",
            "7. Learn by doing: Programming is a hands-on skill, and the best way to learn is by doing. Don't be afraid to try new things and experiment with different approaches.\n",
            "8. Learn from others: Learn from other programmers by reading their code, attending workshops or conferences, or participating in online forums.\n",
            "9. Take online courses: There are many online courses available that can help you learn programming, including introductory courses, specialized courses, and advanced courses.\n",
            "10. Read books: There are many great books available that can help you learn programming, including \"Code Complete\" by Steve McConnell, \"Clean Code\" by Robert C. Martin, and \"The Pragmatic Programmer\" by Andrew Hunt and David Thomas.\n",
            "11. Join a bootcamp: Join a bootcamp to learn programming in a structured and intensive environment.\n",
            "12. Get a mentor: Find a mentor who is experienced in programming and can guide you in your learning journey.\n",
            "13. Learn the basics of data structures and algorithms: Understanding data structures and algorithms is crucial for any programmer. Learn the basics of data structures such as arrays, linked lists, stacks, and queues, and algorithms such as sorting, searching, and graph traversal.\n",
            "14. Learn a second language: Learning a second programming language can help you broaden your horizons and expand your job opportunities.\n",
            "15. Keep learning: Programming is a constantly evolving field, and there is always more to learn. Keep learning new technologies, frameworks, and languages to stay up-to-date and advance your career.\n",
            "\n",
            "Remember, learning programming takes time and effort, but with persistence and dedication, you can become a proficient programmer.\n"
          ]
        }
      ],
      "source": [
        "print(llm.invoke(\"How to learn programming?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prWXot8wcKyS"
      },
      "source": [
        "#### **Prompt Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "7DTl7I1HaOJK"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "mqU1EAehaOJP"
      },
      "outputs": [],
      "source": [
        "template_s = \"\"\"Reply  the answer \\\n",
        "like  {style1}. \\\n",
        "text: ```{text1}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "YK6fMe1paOJP"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(template_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "R5AlexzxaOJP",
        "outputId": "368bdb7b-3a9c-4b1a-e29d-12443a2d3b30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style1', 'text1'], input_types={}, partial_variables={}, template='Reply  the answer like  {style1}. text: ```{text1}```\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "bJa3nBfbaOJQ",
        "outputId": "a4642fa0-914b-4add-aa7e-9782adfb2a07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['style1', 'text1']"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "dYlQrFsnaOJQ"
      },
      "outputs": [],
      "source": [
        "style = \"\"\"trustworthy friend\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "D9UtT3reaOJQ"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "I am not able to understand the concept taught in class. \\\n",
        "Could you please suggest something? \\\n",
        "I need your help. Give 5 points to work on.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "AUB4HHQTaOJQ"
      },
      "outputs": [],
      "source": [
        "user_messages = prompt_template.format_messages(\n",
        "                    style1=style,\n",
        "                    text1=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "6r6WC6OZaOJQ",
        "outputId": "f366c046-f71e-487c-f9a8-4175dcfc2ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Reply  the answer like  trustworthy friend. text: ```\\nI am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\\n```\\n' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "print(user_messages[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "w-zPBqhKaOJQ"
      },
      "outputs": [],
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "llm_response = llm.invoke(user_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "2sMDbkrmoIs_",
        "outputId": "368cd8e7-915f-4a81-9bf5-e81ed9ba507b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Reply  the answer like  trustworthy friend. text: ```\n",
            "I am not able to understand the concept taught in class. Could you please suggest something? I need your help. Give 5 points to work on.\n",
            "```\n",
            "\n",
            "Friendly Reply:  Of course, I'm here to help! ðŸ¤— Here are 5 points you can work on to better understand the concept taught in class:\n",
            "\n",
            "1. Review the basics: Make sure you have a solid grasp of the fundamental concepts and terminology related to the topic.\n",
            "2. Watch video explanations: There are plenty of videos online that explain complex concepts in an easy-to-understand manner.\n",
            "3. Practice with examples: Try working through examples related to the topic to help solidify your understanding.\n",
            "4. Ask your teacher or classmates for help: Don't be afraid to reach out to your teacher or classmates for additional support and clarification.\n",
            "5. Take breaks and come back to it: Sometimes, taking a break and coming back to the material later can help you approach it with fresh eyes and a clearer mind.\n",
            "\n",
            "Remember, learning takes time and effort, but with persistence and dedication, you'll get there! ðŸ’ª Good luck!\n"
          ]
        }
      ],
      "source": [
        "print(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErjQyyi4nR2n"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title Which of the following prompt techniques in LangChain allows flexible templated prompts that are suitable for better describing the role and content? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"Both\" #@param [\"\", \"PromptTemplate\", \"ChatPromptTemplate\", \"Both\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good, But Not Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "outputId": "3a6ec57c-0b74-4e31-85c9-159d529c8633",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 6001\n",
            "Date of submission:  23 Jun 2025\n",
            "Time of submission:  17:59:34\n",
            "View your submissions: https://learn-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TyeMFKXaqpS2"
      },
      "execution_count": 111,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7169b1337815445ea8afe1e3d7a2b43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_775435ee38324c99ba6e3b030701bd49",
              "IPY_MODEL_9b4a3f0be4724509a9f5f33b572067e8",
              "IPY_MODEL_221c4fd3d4d14d3da5c81da3a75809ec"
            ],
            "layout": "IPY_MODEL_f0255f9f8e9c4fa6b4b22fad227e1e23"
          }
        },
        "775435ee38324c99ba6e3b030701bd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae1515f534234d189f5e925978d6ac16",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_33786824f4df4f7bb3e353558008f7c3",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "9b4a3f0be4724509a9f5f33b572067e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_836723b0f3624186988dd6d9cc67b5d1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5553c2fd8d344229bf20c7af50be5d30",
            "value": 2
          }
        },
        "221c4fd3d4d14d3da5c81da3a75809ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_574c508fea564f2eb9a65a7b77a92ddc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f202c7a77df84e1983cf9e30832e8265",
            "value": "â€‡2/2â€‡[01:22&lt;00:00,â€‡37.68s/it]"
          }
        },
        "f0255f9f8e9c4fa6b4b22fad227e1e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae1515f534234d189f5e925978d6ac16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33786824f4df4f7bb3e353558008f7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "836723b0f3624186988dd6d9cc67b5d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5553c2fd8d344229bf20c7af50be5d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "574c508fea564f2eb9a65a7b77a92ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f202c7a77df84e1983cf9e30832e8265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}