{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jUxscojnnv0r"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 7 Kafka Producer: Apache Kafka Streaming"]},{"cell_type":"markdown","metadata":{"id":"ImAbiiU2nv0u"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"dtaHO9W4nv0v"},"source":["At the end of the experiment, you will be able to\n","\n","* understand what is Kafka and its components\n","* perform real-time data analytics with Kafka"]},{"cell_type":"markdown","metadata":{"id":"iCvtDBFenv0v"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"9Fe8re5Pnv0w"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"584kfh_hUVRy"},"source":["Stream processing refers to the processing of data in motion, or in other words, computing on data directly as it is produced or received. The majority of data are born as continuous streams: sensor events, user activity on a website, financial trades, and so on – all these data are created as a series of events over time.\n","\n","Before stream processing, this data was often stored in a database, a file system, or other forms of mass storage. Applications would query the data or compute over the data as needed. Stream Processing turns this paradigm around: the application logic, analytics, and queries exist continuously, and data flows through them continuously.\n","\n","Some of the stream processing frameworks are Apache Flink, Apache Storm, Apache Kafka, and Spark streaming as shown in the figure below.  \n","\n","<img src= \"https://cdn.iisc.talentsprint.com/CDS/Images/Modern_Stream_Processing_Frameworks.jpg\" width= 550 px/>\n","\n","Here we will consider Apache Kafka for streaming.\n","\n","Apache Kafka is an open-source software platform developed by the Apache Software Foundation written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.\n","\n","Kafka was originally developed by LinkedIn and was subsequently open-sourced in early 2011.\n","\n","Kafka is a distributed publish-subscribe messaging system that is designed to be fast, scalable, and durable. Kafka maintains messages in topics. Producers write data to topics and consumers read from topics. Kafka is a distributed system, topics are partitioned and replicated across multiple nodes.\n","\n","It is based on the commit log, and it allows users to subscribe to it and publish data to any number of systems or real-time applications. Example applications include managing passenger and driver matching at Uber, providing real-time analytics and predictive maintenance for British Gas smart home, and performing numerous real-time services across all of LinkedIn.\n","\n","To know more about the use cases of Kafka click [here](https://kafka.apache.org/documentation.html#uses)."]},{"cell_type":"markdown","metadata":{"id":"oxC7feShUVRz"},"source":["### Components of Kafka cluster"]},{"cell_type":"markdown","metadata":{"id":"-iuRTkSmUVRz"},"source":["The components of Kafka are as follows:\n","\n","* **Log:**\n","Write-ahead log, commit log, transaction log; Each partition in Apache Kafka is a log - a time-ordered, append-only sequence of data, from where data is removed only when a given retention period has been exceeded. Records are appended to the end of the log and can be read in order. The log can also be rewound and records can be skipped\n","over for consumers to read from any point in the partition.\n","\n","* **Record or Message:**\n","Data sent to and from the broker is called a record, a key-value pair. The record contains the topic name and partition number. The Kafka broker keeps records inside topic partitions.\n","\n","* **Broker:**\n","The brokers in a Kafka cluster handle the process of receiving, storing and forwarding the records to the interested consumers.\n","\n","* **Topics:**\n","Records are grouped into categories called topics. A Topic is a category/feed name to which records are stored and published. Example: LogMessage or StockMessage. If we wish to send a record we send it to a specific topic and if we want to read a record we read it from a specific topic.\n","\n","* **Retention period:**\n","Records published to the cluster will stay in the cluster until a configurable retention period has passed. Kafka retains all records for a set amount of time or until a configurable size is reached. The consumption time is not impacted by the size of the log.\n","\n","* **Producer, Producer API:**\n","The processes that publish records/messages into a topic are called producers and are using the producer API.\n","\n","* **Consumer, Consumer API:**\n","The processes that consume records/messages from a topic are called consumers and are using the consumer API.\n","\n","* **Partition:**\n","Topics are divided into one or more partitions, which can be replicated between nodes. Partitions are the unit of parallelism in Kafka. Partitions allow records in a topic to be distributed to multiple brokers. A topic can have any number of partitions that we can specify.\n","\n","<center>\n","<figure>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Kafka_offset.png\" width= 450 px/>\n","</figure>\n","</center>\n","\n","\n","* **Offset:**\n","Kafka topics are divided into several partitions, which contain records in an unchangeable sequence. Each record in a partition is assigned and identified by its unique offset.\n","\n","* **Consumer group:**\n","A consumer group includes the set of consumers that are subscribing to a specific topic. Kafka consumers are usually a part of a consumer group. Each consumer in the group is assigned a set of partitions, from which they are able to consume messages. Each consumer in the group\n","will receive records from different subsets of the partitions in the topic.\n","\n","* **ZooKeeper:**\n","Zookeeper is a stand-alone, centralized service, acting across nodes to relieve Kafka from administrative duties. Zookeeper is responsible for controller elections, the configuration of topics, handling access control lists and cluster memberships.\n","\n","* **Instance (“As in a CloudKarafka instance”):**\n","When a CloudKarafka plan is created, we get CloudKarafka instance or an instance of Apache Kafka. It could be a dedicated instance, an Apache Kafka broker, or a shared instance, which gives you five dedicated topics on a shared plan.\n"]},{"cell_type":"markdown","metadata":{"id":"OihZ9gFT4WzP"},"source":["### Kafka Architecture"]},{"cell_type":"markdown","metadata":{"id":"Jy-bMK-2vgdC"},"source":["\n","<img src= \"https://cdn.iisc.talentsprint.com/CDS/Images/KafkaCluster.png\" width= 500 px/>\n","\n","Kafka stores messages that come from arbitrarily many processes called **producers**. The data can be partitioned into different **partitions** within different **topics**. Within a partition, messages are strictly ordered by their **offsets** (the position of a message within a partition) and indexed and stored together with a timestamp. Other processes called **consumers** can read messages from partitions.\n","\n","Kafka runs on a cluster of one or more servers (called **brokers**), and the partitions of all topics are distributed across the cluster nodes. Additionally, partitions are replicated to multiple brokers. This architecture allows Kafka to deliver massive streams of messages in a fault-tolerant fashion."]},{"cell_type":"markdown","metadata":{"id":"fMyKHLWn85L6"},"source":["**Running Producer and Consumer from two separate notebooks**\n","\n","The use of two Colab notebooks is necessary as both producer and consumer files need to be run **simultaneously** and we cannot run two code cells together within the same colab notebook. So while the consumer file is running in another notebook we can run the producer (given in this notebook) file and send messages. The messages sent by the producer will be available at the consumer side. Then we can perform operations on these messages for example print the message, count the number of words in it, compute the rolling mean if numerical data provided, or trigger something if a particular message is received.\n","\n","The steps for the same are given below:\n","\n","* Go to Kafka Consumer notebook and run `consumer.py` file first\n","* While consumer file is running, run `producer.py` file from Kafka Producer notebook\n","* When both files are running, type your message in Producer file and send\n","* The message will be received at the consumer side and output of the operations will be displayed\n","* Stop the Producer cell first and then the corresponding Consumer cell"]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"M6_AST_07_Kafka_Streaming_Producer_B\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://cds-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1MseB3YVQ7lV"},"source":["### Install Confluent kafka"]},{"cell_type":"code","metadata":{"id":"uw6luxJxaR2t"},"source":["# Confluent (Open Source) is a developer-optimized distribution of Apache Kafka.\n","# Confluent is a more complete distribution of Apache Kafka. It expands Kafka’s integration capabilities,\n","# adding tools to optimize and manage Kafka clusters and methods to ensure the streams are secure.\n","# It makes Kafka easier to build and easier to operate\n","\n","!pip install confluent_kafka"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"87xlk-ydYANy"},"source":["### Connect to Kafka cluster"]},{"cell_type":"markdown","metadata":{"id":"3XBDe2cxYANz"},"source":["CloudKarafka provides Apache Kafka as a service and offers tools to simplify the usage of it.\n","\n","To know more about CloudKarafka click [here](https://www.cloudkarafka.com/docs/productoverview.html).\n","\n","**CloudKarafka login:** Login to [Cloudkarafka](https://www.cloudkarafka.com) and create an instance\n","\n","For detailed instructions on the account and instance creation, please refer to this [document](https://cdn.iisc.talentsprint.com/CDS/CloudKarafka.pdf).\n","\n","**Connect the cluster:**\n","\n","* Create an instance and get credentials\n","* Create two topics (one for each example) and note down the topic names"]},{"cell_type":"markdown","metadata":{"id":"xoryOW8zz8vq"},"source":["Specify your `BROKERS`, `USERNAME`, `PASSWORD`, and `TOPIC` in the below script files."]},{"cell_type":"markdown","metadata":{"id":"VMO_ueoU3VwY"},"source":["### Example 1: Send and receive messages"]},{"cell_type":"markdown","metadata":{"id":"1DJkNkch5v1c"},"source":["Here we create two files one is `producer1.py` and another one is `consumer1.py`(in Consumer notebook). Producer will send messages to a topic and consumer will read these messages in real-time from that particular topic and displays the message along with its word count and an alert message if the number of words exceeds 6."]},{"cell_type":"markdown","metadata":{"id":"0WOc_3FQSeZf"},"source":["#### Write Producer file"]},{"cell_type":"markdown","metadata":{"id":"Pe9bHDY8V2Ge"},"source":["Here the producer will send messages to the specified `topic`."]},{"cell_type":"code","metadata":{"id":"-lHkW4aWEiuF"},"source":["%%writefile producer1.py\n","\n","import sys\n","import os\n","\n","from confluent_kafka import Producer\n","\n","# Specify BROKERS, USERNAME, PASSWORD and TOPIC\n","brokers = \"\"\n","username = \"\"\n","password = \"\"\n","topic = \"\"\n","\n","# Set the path for the user-defined modules so that they can be directly imported into the python program\n","os.environ['CLOUDKARAFKA_BROKERS']= brokers\n","os.environ['CLOUDKARAFKA_USERNAME']= username\n","os.environ['CLOUDKARAFKA_PASSWORD']= password\n","os.environ['CLOUDKARAFKA_TOPIC']= topic\n","\n","if __name__ == '__main__':\n","    topic = os.environ['CLOUDKARAFKA_TOPIC'].split(\",\")[0]\n","\n","    # Consumer configuration\n","    conf = {\n","        'bootstrap.servers': os.environ['CLOUDKARAFKA_BROKERS'],                # Specify kafka servers\n","        'session.timeout.ms': 6000,                                             # The producer sends periodic heartbeats to indicate its liveness to the broker\n","        'default.topic.config': {'auto.offset.reset': 'smallest'},              # if there is no offset info, the offset will be set to the smallest value available\n","        'security.protocol': 'SASL_SSL',                                        # protocol used to communicate with brokers\n","\t      'sasl.mechanisms': 'SCRAM-SHA-256',                       # SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, OAUTHBEARER\n","        'sasl.username': os.environ['CLOUDKARAFKA_USERNAME'],\n","        'sasl.password': os.environ['CLOUDKARAFKA_PASSWORD']\n","    }\n","\n","    p = Producer(**conf)\n","\n","    def delivery_callback(err, msg):\n","        if err:\n","            sys.stderr.write('%% Message failed delivery: %s\\n' % err)\n","        else:\n","            sys.stderr.write('%% Message delivered to %s [%d]\\n' %(msg.topic(), msg.partition()))\n","    print(\"\\nEnter text: \")\n","    # Take input data continuously\n","    for line in sys.stdin:\n","        try:\n","            # send data to specified topic\n","            p.produce(topic, line.rstrip(), callback=delivery_callback)\n","        except BufferError as e:\n","            sys.stderr.write('%% Local producer queue is full (%d messages awaiting delivery): try again\\n' %len(p))\n","        p.poll(0)\n","        print(\"\\nEnter text or interupt the execution to stop.\")\n","\n","    sys.stderr.write('%% Waiting for %d deliveries\\n' % len(p))\n","    p.flush()            # makes all buffered records immediately available to send"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_gOm4EA3uK5"},"source":["#### Run Producer file"]},{"cell_type":"markdown","metadata":{"id":"nyjb8u7m3uK6"},"source":["Before running the producer file, make sure that the corresponding consumer file `consumer1.py` is running in [Consumer notebook](https://drive.google.com/file/d/1w665jmNtt4zT-1UxKi04GIb6BpsjqqfR/view?usp=drive_link).\n","\n","The producer will keep on running and allow us to send messages. The output will be shown on the consumer side.\n","\n","<font color='blue'>Before executing the below cell ensure that you created the CloudKarafka account and specified the credentials.</font>"]},{"cell_type":"code","metadata":{"id":"Zh-zwEuw61XM"},"source":["!python producer1.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NftTA06UXtlF"},"source":["For next example **create a new topic** on CloudKarafka and use its topic name. To create a topic, please refer to step 11 in this [document](https://cdn.iisc.talentsprint.com/CDS/CloudKarafka.pdf)."]},{"cell_type":"markdown","metadata":{"id":"Na97-y5N1kSm"},"source":["### Example 2: Compute the rolling mean of the last three insertions"]},{"cell_type":"markdown","metadata":{"id":"PAP5mcrmUwxG"},"source":["Here we create two files one is `producer2.py` and other one is `consumer2.py`(in Consumer notebook). Producer will send data to a topic and consumer will read these records in real-time from that particular topic and displays the rolling mean of the last three insertions. Only the added numbers will be displayed for the first two insertions."]},{"cell_type":"markdown","metadata":{"id":"bS6rbvP536cU"},"source":["#### Write Producer file"]},{"cell_type":"markdown","metadata":{"id":"V_hBqISN36cW"},"source":["Here the producer will send messages to the specified `topic`."]},{"cell_type":"code","metadata":{"id":"dK4jpidFfj4n"},"source":["%%writefile producer2.py\n","\n","import sys\n","import os\n","\n","from confluent_kafka import Producer\n","\n","# Specify BROKERS, USERNAME, PASSWORD and new TOPIC\n","brokers = \"\"\n","username = \"\"\n","password = \"\"\n","topic = \"\"\n","\n","# Set the path for the user-defined modules so that they can be directly imported into the python program\n","os.environ['CLOUDKARAFKA_BROKERS']= brokers\n","os.environ['CLOUDKARAFKA_USERNAME']= username\n","os.environ['CLOUDKARAFKA_PASSWORD']= password\n","os.environ['CLOUDKARAFKA_TOPIC']= topic\n","\n","if __name__ == '__main__':\n","    topic = os.environ['CLOUDKARAFKA_TOPIC'].split(\",\")[0]\n","\n","    # Consumer configuration\n","    conf = {\n","        'bootstrap.servers': os.environ['CLOUDKARAFKA_BROKERS'],                # Specify kafka servers\n","        'session.timeout.ms': 6000,                                             # The producer sends periodic heartbeats to indicate its liveness to the broker\n","        'default.topic.config': {'auto.offset.reset': 'smallest'},\n","        'security.protocol': 'SASL_SSL',\n","\t      'sasl.mechanisms': 'SCRAM-SHA-256',\n","        'sasl.username': os.environ['CLOUDKARAFKA_USERNAME'],\n","        'sasl.password': os.environ['CLOUDKARAFKA_PASSWORD']\n","    }\n","\n","    # YOUR CODE HERE to create 'p' and instantiate Producer with above configuration 'conf'\n","\n","    def delivery_callback(err, msg):\n","        # YOUR CODE HERE\n","\n","    print(\"\\nEnter number: \")\n","    # Take input data continuously\n","    for line in sys.stdin:\n","        try:\n","            # send data to specified topic\n","            p.produce(topic, line, callback=delivery_callback)\n","        except BufferError as e:\n","            sys.stderr.write('%% Local producer queue is full (%d messages awaiting delivery): try again\\n' %len(p))\n","        p.poll(0)\n","        print(\"\\nEnter a number or interupt the execution to stop.\")\n","\n","    sys.stderr.write('%% Waiting for %d deliveries\\n' % len(p))\n","    # YOUR CODE HERE for flush                     # makes all buffered records immediately available to send"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPLtUEC6SiEo"},"source":["#### Run Producer file"]},{"cell_type":"markdown","metadata":{"id":"V_kHIE4dUT0H"},"source":["Before running the producer file, please make sure that the corresponding consumer file `consumer2.py` is running in [Consumer notebook](https://drive.google.com/file/d/1w665jmNtt4zT-1UxKi04GIb6BpsjqqfR/view?usp=drive_link).\n","\n","The producer will keep on running and allow us to send messages. The output will be shown on the consumer side.\n","\n","<font color='blue'>Before executing the below cell ensure that you created the CloudKarafka account and specified the credentials.</font>"]},{"cell_type":"code","metadata":{"id":"b7689U76fpn3"},"source":["# YOUR CODE HERE to run producer2.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"3XNrZ6Wd4IHl"},"source":["# @title Select the incorrect statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\", \"The Producer API allows an application to publish a stream of records to one or more Kafka topics\", \"The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them\", \"Kafka comes with a command line client that will take input from a file and send it out as a collection of all the messages to the Kafka cluster\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}