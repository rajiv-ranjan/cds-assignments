{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"NCtEIcCVFhO8"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 4: ETL concepts and pipeline"]},{"cell_type":"markdown","metadata":{"id":"nH6llaZpFhPC"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"qxQatabcFhPD"},"source":["At the end of the experiment, you will be able to:\n","\n","* use Sparkâ€™s built-in and external data sources to read, refine, and write data in different file formats as part of the extract, transform, and load (ETL) tasks\n","* perform complex data exploration and analysis using Spark SQL"]},{"cell_type":"markdown","metadata":{"id":"GAEv9h69FhPE"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"nFzjN683FhPE"},"source":["ETL (Extract, Transform, and Load) is the procedure of migrating data from one system to another.\n","* Data **extraction** is the process of retrieving data out of homogeneous or heterogeneous sources for further data processing and data storage.\n","* During data **transformation**, the data is cleaned and incorrect or inaccurate records are modified or deleted.\n","* Finally, the processed data is **loaded** (or stored) into a target system such as a data warehouse or NoSQL database or RDBMS.\n","\n","Data engineers use Spark because it provides a simple way to parallelize computations and hides all the complexity of distribution and fault tolerance. This leaves them free to focus on using high-level DataFrame-based APIs and domain-specific language queries to do ETL, reading and combining data from multiple sources.\n","\n","Here we will consider tabular data to do ETL operations. Starting from data extraction, we will perform various transformations and try to gain some insights from it and then load it to a NoSQL database or store it in different file formats."]},{"cell_type":"markdown","metadata":{"id":"jptEWxkKFhPD"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"1b0GRwC0FhPE"},"source":["The dataset chosen for this assignment is [Productivity Prediction of Garment Employees](https://archive.ics.uci.edu/ml/datasets/Productivity+Prediction+of+Garment+Employees). The dataset is made up of 1197 records and 15 columns. It includes important attributes of the garment manufacturing process and the productivity of the employees. The dataset contains records of three months (Jan to Mar 2015) with 2 distinct departments, 12 unique team numbers and 5 different quarters. Some of the features are listed below:\n","\n","* date: Date in MM-DD-YYYY\n","* day: Day of the Week\n","* quarter: A portion of the month. A month was divided into four quarters\n","* department: Associated department with the instance\n","* team: Associated team number with the instance\n","* no_of_workers: Number of workers in each team\n","* no_of_style_change: Number of changes in the style of a particular product\n","* targeted_productivity: Targeted productivity set by the Authority for each team for each day.\n","* smv: Standard Minute Value, it is the allocated time for a task\n","* wip: Work in progress. Includes the number of unfinished items for products\n","* overtime: Represents the amount of overtime by each team in minutes\n","* incentive: Represents the amount of financial incentive (in BDT) that enables or motivates a particular course of action.\n","* idletime: The amount of time when the production was interrupted due to several reasons\n","* idlemen: The number of workers who were idle due to production interruption\n","* actual_productivity: The actual % of productivity that was delivered by the workers. It ranges from 0-1.\n","\n","Date, quarter, department, and day are object datatypes and the rest are int or float types.\n","\n","To know more about the dataset click [here](https://archive.ics.uci.edu/ml/datasets/Productivity+Prediction+of+Garment+Employees)."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"M7_AST_04_ETL_Operations_PySpark_B\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/garments_worker_productivity.csv\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://learn-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dHY77Q8DFhPF"},"source":["### Install Pyspark"]},{"cell_type":"code","metadata":{"id":"KPf_EpH-FhPF"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cq_ToATpFhPF"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"FmMx7T9eFhPG"},"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.feature import MinMaxScaler\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bO5nb-O0FhPG"},"source":["### Start a Spark Session"]},{"cell_type":"markdown","metadata":{"id":"pRSLW_ZqFhPG"},"source":["Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0. Instead of having various context, everything is now encapsulated in a **Spark session**."]},{"cell_type":"code","metadata":{"id":"sY9Ht3RFFhPG"},"source":["# Start spark session\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('ETL').getOrCreate()\n","spark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gOYuB7SBFhPG"},"source":["### Tabular Data Analytics"]},{"cell_type":"markdown","metadata":{"id":"MBlN05cNFhPH"},"source":["#### Extract data into PySpark\n","To load the dataset we will use the read.csv module.  The inferSchema parameter provided will enable Spark to automatically determine the data type for each column."]},{"cell_type":"code","metadata":{"id":"cHiMKSPZFhPH"},"source":["df = spark.read.csv('garments_worker_productivity.csv', header=True, inferSchema= True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ni5SQb1uFhPH"},"source":["#### Transforming Data"]},{"cell_type":"markdown","metadata":{"id":"8hOkkyXNFhPI"},"source":["* Display first few rows of the data"]},{"cell_type":"code","metadata":{"id":"PqiEA5SqFhPI"},"source":["df.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eyu_0_uHFhPI"},"source":["In the above output, the wip (work in progress) column contains null values. We need to check for other columns as well.\n","* Display total number of rows"]},{"cell_type":"code","metadata":{"id":"6-49cC7GFhPI"},"source":["df.count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZKWUDmyyFhPJ"},"source":["* Check for Null values in each column"]},{"cell_type":"code","metadata":{"id":"PYQZZSAZFhPJ"},"source":["df.select([(count(when(isnan(c) | col(c).isNull(), c))/1197).alias(c) for c in df.columns]).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wocbg9GvFhPK"},"source":["The wip column contains around 42% null values so we can drop that column.\n","* Drop the wip column having Null values"]},{"cell_type":"code","metadata":{"id":"VdYpNTTAFhPK"},"source":["df1 = df.drop('wip')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ig3e4w9HFhPK"},"source":["# Recheck for null values\n","# YOUR CODE HERE using df1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5M_8GG9FhPL"},"source":["Let's see the distinct values in department column.\n","* Display distinct `department` from dataframe"]},{"cell_type":"code","metadata":{"id":"PhXnYf4OFhPL"},"source":["# Display count of distinct 'department'\n","df1.select('department').distinct().count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSXZe3pwFhPL"},"source":["# Display distinct values for 'department' column\n","df1.select('department').distinct().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I68uWOsiFhPL"},"source":["There is ambiguity in distinct department values and it needs to be taken care of.\n","* Transform department column"]},{"cell_type":"code","metadata":{"id":"yZdsPCaAFhPL"},"source":["# Removing trailing spaces from both sides using department column\n","df2 = df1.withColumn('department', trim(col('department')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRyZMZlSFhPM"},"source":["# Display distinct values for 'department' column\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_195IjXEFhPM"},"source":["# Replacing department value from 'sweing' to 'sewing'\n","df3 = df2.withColumn('department', regexp_replace(col('department'), 'sweing', 'sewing'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T2b72zb6FhPM"},"source":["# Display distinct values for 'department' column\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5YPfkb-yFhPM"},"source":["* Check for duplicate values in data"]},{"cell_type":"code","metadata":{"id":"kJHpOnaBFhPM"},"source":["cols = df3.columns\n","if df3.count() > df3.dropDuplicates(cols).count():\n","    print('Data has duplicates')\n","else:\n","  print('Data has no duplicates')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-91dTPIBFhPN"},"source":["If data has duplicate values, run the below cell by uncommenting it"]},{"cell_type":"code","metadata":{"id":"eS5t2r0_FhPN"},"source":["# df3 = df3.dropDuplicates(df3.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFFNpfG-FhPN"},"source":["After removing duplicates, let's take a look at the datatypes of our columns.\n","* Display data types of dataframe columns"]},{"cell_type":"code","metadata":{"id":"fdaA7KR4FhPN"},"source":["# Print the data types\n","df3.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CvLXXC_BFhPN"},"source":["Here, the 'date' column has a string datatype. In order to perform analytics involving year and month, we need to convert it into timestamp datatype.\n","* Transform the `date` column from string type to Spark `timestamp` data type"]},{"cell_type":"code","metadata":{"id":"X8PVJt_bFhPO"},"source":["df4 = df3.withColumn(\"date\", to_timestamp(col(\"date\"), \"M/d/yyyy\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"19alU-8kFhPO"},"source":["# YOUR CODE HERE to display datatypes of df4 columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Clxysl2cFhPO"},"source":["Now we can use the `year()` SQL Spark function on the Timestamp column data type `date`.\n","* Display how many distinct years of data is in the dataset"]},{"cell_type":"code","metadata":{"id":"Zqe9JtGpFhPO"},"source":["df4.select(year('date')).distinct().orderBy(year('date')).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRbxYZF1FhPO"},"source":["We see that all the records are from the year 2015.\n","\n","Similar to year() we can use the `month()` SQL Spark function on the Timestamp datatype column `date`.\n","* Display how many distinct months of data is in the dataset"]},{"cell_type":"code","metadata":{"id":"ISePnPs0FhPR"},"source":["df4.select(month('date')).distinct().orderBy(month('date')).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rS8SPyPGFhPS"},"source":["We see that the data was collected in the months January to March."]},{"cell_type":"markdown","metadata":{"id":"C_ZMvuN9FhPS"},"source":["* Check in which month the productivity was maximum"]},{"cell_type":"code","metadata":{"id":"F5fc05-0FhPS"},"source":["df_d = df4.groupby(month('date')).avg().select(['month(date)', 'avg(actual_productivity)'])\n","df_d.show()\n","sns.barplot(x = df_d.toPandas()['month(date)'], y= df_d.toPandas()['avg(actual_productivity)'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nSgxRTs2FhPS"},"source":["From the above plot, it can be seen that in January the productivity was little higher than remaining two.\n","\n","Let's gain few more insights from the data\n","* Display the incentives paid to different teams"]},{"cell_type":"code","metadata":{"id":"1GM_CVgZFhPS"},"source":["df_i = df4.groupby('team').avg().select(['team', 'avg(incentive)'])\n","df_i.show()\n","sns.barplot(x = df_i.toPandas()['team'], y= df_i.toPandas()['avg(incentive)'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9IRv_tDkFhPS"},"source":["So on average team 9 received the highest incentive.\n","* Display number of workers in each Team"]},{"cell_type":"code","metadata":{"id":"9_3wTe13FhPS"},"source":["df_w = df4.groupby('team').sum().select(['team', 'sum(no_of_workers)'])\n","df_w.show()\n","# YOUR CODE HERE to plot the barplot"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9EHlIhavFhPT"},"source":["From the above plot, it can be seen that team 6 and 12 have less number of workers compared to other teams.\n","\n","Let's find out how much it costs for the first quarter of the first month.\n","* Display the `incentive` paid for the first quarter of the first month"]},{"cell_type":"code","metadata":{"id":"KIKPLKx1FhPT"},"source":["df_q = df4.select(month(\"date\"), \"quarter\", \"incentive\").where((col('month(date)') == 1) & (col(\"quarter\") == \"Quarter1\"))\n","df_q.show(5)\n","df_q.groupby('quarter').sum().select('sum(incentive)').show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1oXWp_fFhPT"},"source":["Before fitting to a model, the outlier removal and feature scaling of data are important.\n","* Check for outliers"]},{"cell_type":"code","metadata":{"id":"N35RLI5PFhPT"},"source":["df4.toPandas().boxplot()\n","plt.xticks(rotation= 90)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4arjqqo6FhPT"},"source":["* Handling outliers\n","\n","Here instead of removing the outliers we will change their values to upper bound and lower bound depending on whether the value is higher than upper bound or lower than lower bound respectively."]},{"cell_type":"code","metadata":{"id":"NkOMFKlvFhPT"},"source":["df5 = df4\n","outlier_cols = [\"targeted_productivity\", \"smv\", \"over_time\", \"incentive\", \"idle_time\", \"idle_men\", \"no_of_style_change\", \"actual_productivity\"] # Columns with outliers\n","def handle_outliers(df, colm):\n","    df = df.toPandas()\n","    q1 = df.describe()[colm].loc[\"25%\"]\n","    q3 = df.describe()[colm].loc[\"75%\"]\n","    iqr = q3 - q1\n","    lower_bound = q1 - (1.5 * iqr)\n","    upper_bound = q3 + (1.5 * iqr)\n","    for i in range(len(df)):\n","      if df.loc[i,colm] > upper_bound:\n","        df.loc[i,colm]= upper_bound\n","      if df.loc[i,colm] < lower_bound:\n","        df.loc[i,colm]= lower_bound\n","    return spark.createDataFrame(df)\n","\n","for colm in outlier_cols:\n","    df5 = handle_outliers(df5, colm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWlcasCMFhPT"},"source":["# Recheck for outliers\n","# YOUR CODE HERE using df5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DnDUZeBPFhPU"},"source":["* Scaling features"]},{"cell_type":"code","metadata":{"id":"xpZy1k0GFhPU"},"source":["# Display the statistics of dataframe\n","df5.toPandas().describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZaHW1C7FhPU"},"source":["# Specify columns that need to be scaled\n","columns = [\"smv\", \"over_time\", \"incentive\", \"no_of_workers\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8pgGe4yFhPU"},"source":["print(\"Before Scaling :\")\n","# YOUR CODE HERE to display first five rows of df5\n","df6 = df5\n","\n","from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.sql.functions import udf, round\n","from pyspark.sql.types import DoubleType\n","\n","# UDF for converting column type from vector to double type\n","unlist = udf(lambda x: float(list(x)[0]), DoubleType())\n","spark.udf.register(\"unlist\", unlist)\n","\n","# Iterating over columns to be scaled\n","for i in columns:\n","    # VectorAssembler Transformation - Converting column to vector type\n","    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n","\n","    # MinMaxScaler Transformation\n","    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n","\n","    # Pipeline of VectorAssembler and MinMaxScaler\n","    pipeline = Pipeline(stages=[assembler, scaler])\n","\n","    # Fitting pipeline on dataframe\n","    df6 = pipeline.fit(df6).transform(df6).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n","\n","print(\"After Scaling :\")\n","# YOUR CODE HERE to display first five rows of df6"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XrNKGgMLFhPU"},"source":["After outlier removal, few records got deleted. Let's create a new `id` column that will contain a unique value for each record."]},{"cell_type":"code","metadata":{"id":"Vws1d6rPFhPU"},"source":["df7 = df6.withColumn(\"id\", monotonically_increasing_id()+1)\n","df7.select('id').show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AxtWPtpBFhPU"},"source":["We can also perform SQL queries on spark dataframe using the `spark.sql()` function. But for that, we first need to register the dataframe as a table in the spark catalog. We can do this using the `createOrReplaceTempView()` spark dataframe method. It takes the name of the temporary table we'd like to register as argument. As this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark dataframe."]},{"cell_type":"code","metadata":{"id":"IQuN6nz5FhPV"},"source":["df7.createOrReplaceTempView('df_table')\n","spark.sql(\"select date, department, team, smv, over_time, incentive from df_table where department='sewing'\").show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NuxGK5lMFhPV"},"source":["#### Load Data"]},{"cell_type":"markdown","metadata":{"id":"WZa9WGtjFhPV"},"source":["Once we have extracted and transformed our data, we might want to load it into the destination or store it somewhere. We will load it into the MongoDB database."]},{"cell_type":"markdown","metadata":{"id":"Cw_To9V-FhPW"},"source":["Data in MongoDB is represented and stored using JSON-style documents. In PyMongo we use dictionaries to represent documents."]},{"cell_type":"code","metadata":{"id":"YQtw9vFnFhPW"},"source":["data = df7.toPandas()\n","documents = []\n","for i in range(len(data)):\n","        doc = data.iloc[i,:].to_dict()\n","        for keys in doc:\n","          if keys != 'date':\n","            if type(doc[keys]) not in [str]:\n","                doc[keys] = float(doc[keys])\n","        documents.append(doc)\n","documents[0:1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gBdnqWBmFhPW"},"source":["If you would like to perform the data insertion step then please **create your own account** on MongoDB Atlas as given in the reference [here](https://cdn.iisc.talentsprint.com/CDS/DB_Connect_Docs/Assignment_MongoDB_Connect.pdf) and change the credentials and run the below code by uncommenting it."]},{"cell_type":"code","metadata":{"id":"y4JC5Zg5FhPX"},"source":["### new_document = coll.insert_many(documents)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ayplTl--FhPX"},"source":["Also, we can store the data in other formats like json, csv, and parquet and read it back whenever required.\n","\n","**Store the dataframe as a `json file`**"]},{"cell_type":"code","metadata":{"id":"jGbjYqmDFhPX"},"source":["df7.write.format(\"json\").mode(\"overwrite\").save('transformed_json_data.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9hzX8LdjFhPX"},"source":["**Read data from `json` to spark dataframe**"]},{"cell_type":"code","metadata":{"id":"p2--YPdhFhPY"},"source":["df_json = spark.read.format(\"json\").load('transformed_json_data.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJVFRRoQFhPY"},"source":["df_json.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3_5wx0aWFhPY"},"source":["**Store the dataframe as a `csv file`**"]},{"cell_type":"code","metadata":{"id":"mpABXzByFhPY"},"source":["df7.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"transformed_csv_data.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7mO6h-HpFhPY"},"source":["**Read data from `csv` to spark dataframe**"]},{"cell_type":"code","metadata":{"id":"-oDKXltXFhPY"},"source":["df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").load('transformed_csv_data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sW53Urb8FhPY"},"source":["# YOUR CODE HERE to display first five rows of df_csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"is1lmuQuFhPY"},"source":["**Use Parquet files to store data**"]},{"cell_type":"markdown","metadata":{"id":"3NfAZB2cFhPZ"},"source":["Parquet uses snappy compression to compress the data. If the DataFrame is written as Parquet, the schema is preserved as part of the Parquet metadata.\n","\n","To know more about parquet file format click [here](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module5/ellicium_com_blog_parquet_file_format_structure_teevel_2C_20the_stored_20in_20the_20footer_20section.pdf)."]},{"cell_type":"code","metadata":{"id":"vQ-u4QOMFhPZ"},"source":["df7.write.format(\"parquet\").mode(\"overwrite\").save(\"transformed_parquet_data\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hSnBkk5KFhPZ"},"source":["**Read data from Parquet file**\n","\n","We don't have to specify the schema here since it's stored as part of the Parquet metadata."]},{"cell_type":"code","metadata":{"id":"lqsho9lhFhPZ"},"source":["df_parquet = spark.read.format(\"parquet\").load(\"transformed_parquet_data\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1MQ19ypFhPZ"},"source":["# YOUR CODE HERE to display first five rows of df_parquet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"VgSwVENIPcM6"},"source":["# @title Considering the scaled Spark dataframe (df7), which of the following teams has received the highest average incentive among those who worked on Monday? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\",\"1\",\"6\",\"11\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}