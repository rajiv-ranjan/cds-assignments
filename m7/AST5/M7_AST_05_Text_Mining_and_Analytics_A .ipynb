{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"LM_GMDQ-J1uN"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 5: ETL pipeline for Text Mining and Analytics"]},{"cell_type":"markdown","metadata":{"id":"pAZ01Z5tJ1uV"},"source":["At the end of the experiment, you will be able to:\n","\n","* perform text mining and analytics using Spark SQL functions\n","* use Spark’s built-in and external data sources to write data in different file formats as part of the extract, transform, and load (ETL) tasks\n"]},{"cell_type":"markdown","metadata":{"id":"J93wmCVyJ1uW"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"MyHY_53qJ1uX"},"source":["The basic terminology related to text analytics are\n","\n","* **Text**: a sequence of words and punctuation\n","* **Corpus**: a large body of text\n","* **Frequency distribution**: the frequency of words in a text object\n","* **Collocation**: a sequence of words that occur together unusually often\n","* **Bigrams**: word pairs. High frequent bigrams are collocations\n","* **Text normalization**: the process of transforming text into a single canonical form, e.g., converting text to lowercase, removing punctuations and stop words."]},{"cell_type":"markdown","metadata":{"id":"UexUWZb2J1uX"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"F2dNzmmEJ1uX"},"source":["Text analytics is the process of deriving information from text. It usually involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging, information extraction, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods.\n","\n","Here we will consider `milton-paradise.txt` text file from Gutenberg corpus to do text mining and analytics. Starting from data extraction, we will perform various transformations on text including tokenization, the number of words counting, POS tagging, chunking and then store it in different file formats."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"M7_AST_05_Text_Mining_and_Analytics_A\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://learn-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GsODE6yLJ1uY"},"source":["### Install Pyspark"]},{"cell_type":"code","metadata":{"id":"PbVweILTJ1uY"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gjdtWmUaJ1uY"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"eTb45PY962Qp"},"source":["from pyspark.sql import SparkSession\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","import string\n","from pyspark.ml.feature import NGram\n","from pyspark.ml import Pipeline\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f8ZWzA-VJ1uZ"},"source":["### Start a Spark Session"]},{"cell_type":"markdown","metadata":{"id":"RKKkyz0IJ1uZ"},"source":["Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0. Instead of having various context, everything is now encapsulated in a Spark session."]},{"cell_type":"code","metadata":{"id":"pAeBc9XpJ1uZ"},"source":["# Start spark session\n","spark = SparkSession.builder.appName('ETL text data').getOrCreate()\n","spark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_w1rS1nRJ1ua"},"source":["### Text Analytics"]},{"cell_type":"markdown","metadata":{"id":"QUO-MOywJ1ua"},"source":["#### Get the text data\n","\n","The raw text is from the Gutenberg corpus from the nltk package. Get file ids in Gutenberg corpus."]},{"cell_type":"code","metadata":{"id":"r8JRZaZPJ1ua"},"source":["import nltk\n","nltk.download('gutenberg')\n","\n","# Download dependencies for sent_tokenize()\n","nltk.download('punkt_tab')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bkEWZrVWJ1ua"},"source":["from nltk.corpus import gutenberg\n","gutenberg_fileids = gutenberg.fileids()\n","# YOUR CODE HERE to display gutenberg_fileids"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0_vRcgahJ1ua"},"source":["The file id is `milton-paradise.txt`. Use the nltk.sent_tokenize() function to split text into sentences."]},{"cell_type":"code","metadata":{"id":"XtvBSiuFJ1ub"},"source":["milton_paradise = gutenberg.raw('milton-paradise.txt')\n","\n","pdf = pd.DataFrame({'sentences': nltk.sent_tokenize(milton_paradise)})\n","# YOUR CODE HERE to create spark dataframe 'd' from pdf\n","d.show(1, truncate= False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1wiCzYxgJ1ub"},"source":["From above it can be seen that empty spaces are present in the data."]},{"cell_type":"markdown","metadata":{"id":"EW2NfTKfJ1ub"},"source":["#### Transform Data\n","\n","* Remove trailing spaces"]},{"cell_type":"code","metadata":{"id":"iearnIOYJ1ub"},"source":["# Transform data\n","# YOUR CODE HERE to change d1                                                        # replace all underscores with one space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hVrcNODJ1ub"},"source":["d1.show(5, truncate= False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrO3vZ67J1uc"},"source":["# Check for empty lines\n","d1.where(col(\"sentences\")==\"\").count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUTbYzXSJ1uc"},"source":["##### Word Tokenization\n","\n","It is the process of breaking down a paragraph, a sentence or a complete text corpus into an array of words."]},{"cell_type":"code","metadata":{"id":"5TaiZDtkJ1uc"},"source":["from nltk.tokenize import word_tokenize\n","\n","word_udf = udf(lambda x: word_tokenize(x), ArrayType(StringType()))\n","d2 = d1.withColumn(\"words\", word_udf(\"sentences\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VX4130tAJ1uc"},"source":["# YOUR CODE HERE to display first five rows of d2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F4N9SIngJ1uc"},"source":["From above it can be seen that data has punctuations in it.\n","\n","* **Remove punctuation and stopwords**"]},{"cell_type":"code","metadata":{"id":"DxR9eooPJ1uc"},"source":["# Download stopwords\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rRHaCN0J1ud"},"source":["from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","# YOUR CODE HERE to display stop_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSSCh2QeJ1ud"},"source":["# YOUR CODE HERE to display punctuation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gKHU3-bJ1ud"},"source":["# Transform data\n","punct_udf = udf(lambda x: [w for w in x if not w.lower() in punctuation if not w.lower() in stop_words])\n","# YOUR CODE HERE to create d3 by applying punct_udf function on words column of d2 and naming it as words column only\n","d3.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7CN5QwlJ1ud"},"source":["# Convert dataframe column to arraytype for further processing\n","\n","array_udf = udf(lambda x: x, ArrayType(StringType()))\n","d4 = d3.withColumn(\"words\", array_udf(\"words\"))\n","# YOUR CODE HERE to display first five rows of d4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0HgsKRUDJ1ud"},"source":["##### Ngrams and collocations\n","\n","Collocation is a sequence of words that occur together unusually often.\n","Bigrams: word pairs. High frequent bigrams are collocations.\n","\n","Let's see how we transform texts to 2-grams, 3-grams, and 4-grams collocations."]},{"cell_type":"code","metadata":{"id":"oSeoJeoMJ1ue"},"source":["ngrams = [NGram(n=n, inputCol='words', outputCol=str(n)+'-grams') for n in [2,3,4]]\n","\n","# build pipeline model and transform data\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxhhYXXrJ1ue"},"source":["# display result\n","texts_ngrams.select('2-grams').show(6, truncate=False)\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BozJZvZVJ1ue"},"source":["* Add the number of words column"]},{"cell_type":"code","metadata":{"id":"gnW-5PvwJ1ue"},"source":["# Transform data\n","len_udf = udf(lambda x: len(x), IntegerType())\n","\n","# YOUR CODE HERE to create d5 by applying len_udf function on 'words' column of d4 and naming it as 'no_of_words' column"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byxmFkQxJ1ue"},"source":["# YOUR CODE HERE to display first five rows of d5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iw8-o6gMJ1ue"},"source":["##### **POS (part-of-speech) tagging**\n","\n","It is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having a form (word, tag)). The tag is a part-of-speech tag and signifies whether the word is a noun, adjective, verb, and so on.\n","\n","To know more about POS tagging click [here](https://medium.com/greyatom/learning-pos-tagging-chunking-in-nlp-85f7f811a8cb)."]},{"cell_type":"code","metadata":{"id":"U8JhY1RqJ1uf"},"source":["# Download dependencies for pos_tag()\n","nltk.download('averaged_perceptron_tagger_eng')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2M-3UpFRJ1uf"},"source":["## define schema for returned result from the udf function\n","## the returned result is a list of tuples\n","schema = ArrayType(StructType([\n","            StructField('f1', StringType()),\n","            StructField('f2', StringType())    ]))\n","\n","sent_to_tag_words_udf = udf(lambda x: nltk.pos_tag(x), schema)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PkckMu1MJ1uf"},"source":["# Transform data\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1EYWgCKWJ1uf"},"source":["##### **Frequency Distribution Plot**\n","\n","It gives us information about the number of times a word has occurred within a sentence."]},{"cell_type":"code","metadata":{"id":"NWhKkH9-J1uf"},"source":["from nltk.probability import FreqDist\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36AvWojEJ1uf"},"source":["From the above plot it can be seen that in the first row, the word 'Man' has occurred twice."]},{"cell_type":"markdown","metadata":{"id":"IwWhqb88J1uf"},"source":["##### **Chunking**\n","Chunking is the process of grouping similar words together based on the nature of the word. It is the process of segmenting and labeling multitokens. Let's see how to do a noun phrase chunking on the tagged words data frame from the previous step.\n","\n","First we need to define a udf function that chunks noun phrases from a list of pos-tagged words."]},{"cell_type":"code","metadata":{"id":"g08SQc6XJ1ug"},"source":["# define a udf function to chunk noun phrases from pos-tagged words\n","grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n","chunk_parser = nltk.RegexpParser(grammar)\n","chunk_parser_udf = udf(lambda x: str(chunk_parser.parse(x)), StringType())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PrAbDbOGJ1ug"},"source":["# Transform data\n","# YOUR CODE HERE to create d7 by applying chunk_parser_udf on 'tagged_words' column of d6 and naming it as 'NP_chunk' column"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0bVIAQOJ1uh"},"source":["d7.select('NP_chunk').show(1, truncate= False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5l8mSsydJ1uh"},"source":["#### Load data"]},{"cell_type":"markdown","metadata":{"id":"8wG_GKthJ1uh"},"source":["**Use Parquet file to store data**"]},{"cell_type":"code","metadata":{"id":"Jwz7yHbcJ1uh"},"source":["d7.write.format(\"parquet\").mode(\"overwrite\").save(\"transformed_text_parquet_data\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vn8a8aVKJ1uh"},"source":["**Read data from Parquet file**"]},{"cell_type":"code","metadata":{"id":"zU67l4gUJ1uh"},"source":["df_text_parquet = spark.read.format(\"parquet\").load(\"transformed_text_parquet_data\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yonfCe5wJ1ui"},"source":["# YOUR CODE HERE to display first five rows of df_text_parquet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IsSrVl6bJ1ui"},"source":["**Store the data as a `json file`**"]},{"cell_type":"code","metadata":{"id":"B5Jobed2J1ui"},"source":["d7.write.format(\"json\").mode(\"overwrite\").save('transformed_text_json_data.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IxhbRkgOJ1ui"},"source":["**Read data from `json` to spark dataframe**"]},{"cell_type":"code","metadata":{"id":"IXldw5HBJ1ui"},"source":["# YOUR CODE HERE to create 'df_text_json'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcb-LBjAJ1ui"},"source":["# YOUR CODE HERE to display first five rows of df_text_json"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"VgSwVENIPcM6"},"source":["# @title Considering the 16th row of transformed spark dataframe (d7), how many words occur more than once? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\",\"1\",\"2\",\"3\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}