{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"5gco6DBlYANg"},"source":["# Advanced Certification Program in Computational Data Science\n","## A Program by IISc and TalentSprint\n","### Additional Notebook (ungraded) on PySpark ML\n"]},{"cell_type":"markdown","metadata":{"id":"c3WtQbQ6YANo"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"5e7GNBKzYANp"},"source":["At the end of the experiment, you will be able to\n","\n","* understand the concept of machine learning using PySpark\n","* Explore and visualize California housing dataset\n","* understand code implementation for performing machine learning using PySpark"]},{"cell_type":"markdown","metadata":{"id":"T0Ww4neyYANq"},"source":["### Introduction"]},{"cell_type":"markdown","source":["### Machine Learning using PySpark"],"metadata":{"id":"E5rRP3nka1HH"}},{"cell_type":"markdown","source":["PySpark MLlib is a machine-learning library. It is like a wrapper over PySpark Core to do data analysis using machine-learning algorithms. It works on distributed systems and is scalable. It can be used for classification, clustering, linear regression, and other machine-learning algorithms in PySpark MLlib."],"metadata":{"id":"5B_BXJwHa3Ra"}},{"cell_type":"markdown","source":["* It has a number advantages such as it is faster than previous approaches like MapReduce.\n","* It has multiple functions that it offers such as running distributed SQL.\n","\n","To know more about a pyspark's ML pipeline click [here](https://spark.apache.org/docs/2.3.1/api/python/pyspark.ml.html#module-pyspark.ml.classification)"],"metadata":{"id":"GhbanVfWd7Uv"}},{"cell_type":"markdown","source":["\n","**Problem Statement:** Predicting House Prices using California Housing Dataset"],"metadata":{"id":"jLyDqEafOmYg"}},{"cell_type":"markdown","source":["In this section, we'll make use of the California Housing data set. Note, of course, that this is actually 'small', but, the purpose of this notebook is meant to give you an idea of how we can use PySpark to build a machine learning model."],"metadata":{"id":"3zbRzn_2WIoB"}},{"cell_type":"markdown","source":["**Dataset Description** : The California Housing data set appeared in a 1997 paper titled Sparse Spatial Autoregressions, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.\n","\n","The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n","\n","These spatial data contain 20,640 observations on housing prices with 9 economic variables:\n","\n","`Longitude`:refers to the angular distance of a geographic place north or south of the earth’s equator for each block group\n","\n","`Latitude` :refers to the angular distance of a geographic place east or west of the earth’s equator for each block group\n","\n","`Housing Median Age`:is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values\n","\n","`Total Rooms`:is the total number of rooms in the houses per block group\n","\n","`Total Bedrooms`:is the total number of bedrooms in the houses per block group\n","\n","`Population`:is the number of inhabitants of a block group\n","\n","`Households`:refers to units of houses and their occupants per block group\n","\n","`Median Income`:is used to register the median income of people that belong to a block group\n","\n","`Median House Value`:is the dependent variable and refers to the median house value per block group\n","\n","\n","The Median house value is the dependent variable and will be assigned the role of the target variable in our ML model."],"metadata":{"id":"5nZn6MKYWgiy"}},{"cell_type":"code","source":["#@title Run this cell to download the dataset\n","from IPython import get_ipython\n","ipython = get_ipython()\n","ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/cal_housing.data\")"],"metadata":{"cellView":"form","id":"nOj2rcqJWBYN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Importing the required libraries and packages"],"metadata":{"id":"tqvn3NfldIEv"}},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"id":"O68_-GPiXoG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","from pyspark import SparkConf, SparkContext\n","from pyspark.sql import SparkSession, SQLContext\n","\n","from pyspark.sql.types import StructType\n","from pyspark.sql.types import StructField\n","from pyspark.sql.types import FloatType\n","\n","import pyspark.sql.functions as F\n","from pyspark.sql.functions import udf, col\n","\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.mllib.evaluation import RegressionMetrics\n","\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n","from pyspark.ml.feature import VectorAssembler, StandardScaler\n","from pyspark.ml.evaluation import RegressionEvaluator\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"metadata":{"id":"BNbIkL9DXtKo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualization\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","pd.set_option('display.max_columns', 200)\n","pd.set_option('display.max_colwidth', 400)\n","\n","from matplotlib import rcParams\n","sns.set(context='notebook', style='whitegrid', rc={'figure.figsize': (18,4)})\n","rcParams['figure.figsize'] = 18,4\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'"],"metadata":{"id":"LTce0dhxX55K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting random seed\n","rnd_seed=23\n","np.random.seed=rnd_seed\n","np.random.set_state=rnd_seed"],"metadata":{"id":"RS3bziWhX67h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Creating the Spark Session"],"metadata":{"id":"jegkXn3xX_hc"}},{"cell_type":"code","source":["spark = SparkSession.builder.master(\"local[2]\").appName(\"Linear-Regression-California-Housing\").getOrCreate()"],"metadata":{"id":"WubepuVEX9EE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spark"],"metadata":{"id":"7UKlmmf7YFLj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creating Spark Context"],"metadata":{"id":"1wszx0QCSTxa"}},{"cell_type":"code","source":["sc = spark.sparkContext\n","sc"],"metadata":{"id":"7Kv9LhAFYF2c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creating SQL Context"],"metadata":{"id":"v6l38V69SUe-"}},{"cell_type":"code","source":["sqlContext = SQLContext(spark.sparkContext)\n","sqlContext"],"metadata":{"id":"0ppSUWhNYK7g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Load The Data From the File"],"metadata":{"id":"b57grHvvYPLQ"}},{"cell_type":"code","source":["HOUSING_DATA = '/content/cal_housing.data'"],"metadata":{"id":"Kaun7WFJYOy8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Specifying the schema when loading data into a DataFrame will give better performance than schema inference."],"metadata":{"id":"jSrwug1lXq5s"}},{"cell_type":"code","source":["# Define the schema, corresponding to a line in the csv data file.\n","schema = StructType([\n","    StructField(\"long\", FloatType(), nullable=True),\n","    StructField(\"lat\", FloatType(), nullable=True),\n","    StructField(\"medage\", FloatType(), nullable=True),\n","    StructField(\"totrooms\", FloatType(), nullable=True),\n","    StructField(\"totbdrms\", FloatType(), nullable=True),\n","    StructField(\"pop\", FloatType(), nullable=True),\n","    StructField(\"houshlds\", FloatType(), nullable=True),\n","    StructField(\"medinc\", FloatType(), nullable=True),\n","    StructField(\"medhv\", FloatType(), nullable=True)]\n",")"],"metadata":{"id":"aLQDkA4-YYll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load housing data\n","housing_df = spark.read.csv(path=HOUSING_DATA, schema=schema).cache()"],"metadata":{"id":"LRqtjRYuYbod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inspect first five rows\n","housing_df.take(5)"],"metadata":{"id":"5pqBm42nYd4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display first five rows\n","housing_df.show(5)"],"metadata":{"id":"zmXg02AnYgNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show the dataframe columns\n","housing_df.columns"],"metadata":{"id":"-OpkyOitYjd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show the schema of the dataframe\n","housing_df.printSchema()"],"metadata":{"id":"zLfr1YhyYmBP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Exploration"],"metadata":{"id":"CwYmcgSKXF_J"}},{"cell_type":"code","source":["# Run a sample selection\n","housing_df.select('pop','totbdrms').show(10)"],"metadata":{"id":"qrQ7_tk3YxTt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Distribution of the median age of the people living in the area"],"metadata":{"id":"khdWEHL8ZCOZ"}},{"cell_type":"code","source":["# Group by housing median age and see the distribution\n","result_df = housing_df.groupBy(\"medage\").count().sort(\"medage\", ascending=False)"],"metadata":{"id":"iQRixh37ZIPF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_df.show(10)"],"metadata":{"id":"NuUu_qf6ZKWe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_df.toPandas().plot.bar(x='medage',figsize=(14, 6))"],"metadata":{"id":"F4dqFsBbZNLW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Most of the residents are either in their youth or middle age group."],"metadata":{"id":"7kWbLAAeZQlV"}},{"cell_type":"markdown","source":["#### Summary Statistics\n","Spark DataFrames include some built-in functions for statistical processing. The describe() function performs summary statistics calculations on all numeric columns and returns them as a DataFrame."],"metadata":{"id":"YRZxvrYXZazY"}},{"cell_type":"code","source":["(housing_df.describe().select(\n","                    \"summary\",\n","                    F.round(\"medage\", 4).alias(\"medage\"),\n","                    F.round(\"totrooms\", 4).alias(\"totrooms\"),\n","                    F.round(\"totbdrms\", 4).alias(\"totbdrms\"),\n","                    F.round(\"pop\", 4).alias(\"pop\"),\n","                    F.round(\"houshlds\", 4).alias(\"houshlds\"),\n","                    F.round(\"medinc\", 4).alias(\"medinc\"),\n","                    F.round(\"medhv\", 4).alias(\"medhv\"))\n","                    .show())"],"metadata":{"id":"yvtyaiIKZlL9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look at the minimum and maximum values of all the (numerical) attributes. We see that multiple attributes have a wide range of values: we will need to normalize the dataset."],"metadata":{"id":"Orq7db7MZn_z"}},{"cell_type":"markdown","source":["### Data Preprocessing\n","\n","* Standardize the data, as we have seen that the range of minimum and maximum values is quite big.\n","\n","* There are possibly some additional attributes that we could add, such as a feature that registers the number of bedrooms per room or the rooms per household.\n","\n","* The dependent variable is large in value; To make it easier to work with it, we will slightly adjust the values."],"metadata":{"id":"0G3QsvbZZqj5"}},{"cell_type":"markdown","source":["#### Preprocessing The Target Values\n","\n","First, let's start with the medianHouseValue, the dependent variable. To facilitate our working with the target values, we will express the house values in units of 100,000. That means that a target such as 452600.000000 should become 4.526."],"metadata":{"id":"mGclV3HdaP7A"}},{"cell_type":"code","source":["# Adjust the values of `medianHouseValue`\n","housing_df = housing_df.withColumn(\"medhv\", col(\"medhv\")/100000)"],"metadata":{"id":"om6Ht0rwae1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show the first 2 lines of `df`\n","housing_df.show(2)"],"metadata":{"id":"51BUNk1zahZI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can clearly see that the values have been adjusted correctly when we look at the result of the show() method."],"metadata":{"id":"WHOGyeksaj9V"}},{"cell_type":"markdown","source":["### Feature Engineering"],"metadata":{"id":"VHitBip4anG5"}},{"cell_type":"markdown","source":["Now that we have adjusted the values in medianHouseValue, we will now add the following columns to the data set:\n","\n","*   Rooms per household which refers to the number of rooms in households per block group;\n","\n","*   Population per household, which basically gives us an indication of how many people live in households per block group;\n","*   Bedrooms per room which will give us an idea about how many rooms are bedrooms per block group;\n","\n","As we are working with DataFrames, it is best to use the select() method to select the columns that we are going to work with, namely totalRooms, households, and population. Additionally, we need to indicate that we are working with columns by adding the col() function to our code. Otherwise, we won't be able to do element-wise operations like the division step ahead.\n","\n","\n"],"metadata":{"id":"5ZgBUAcBaoV8"}},{"cell_type":"code","source":["housing_df.columns"],"metadata":{"id":"wmjW3cl3c3sT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add the new columns to `df`\n","housing_df = (housing_df.withColumn(\"rms_per_hh\", F.round(col(\"totrooms\")/col(\"houshlds\"), 2))\n","                       .withColumn(\"pop_per_hh\", F.round(col(\"pop\")/col(\"houshlds\"), 2))\n","                       .withColumn(\"bdrms_per_rm\", F.round(col(\"totbdrms\")/col(\"totrooms\"), 2)))"],"metadata":{"id":"FJiaqvwZc6ps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inspect the result\n","housing_df.show(5)"],"metadata":{"id":"6h0C8KJyc9RL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that, for the first row, there are about 6.98 rooms per household, the households in the block group consist of about 2.5 people and the amount of bedrooms is quite low with 0.14.\n","\n","We do not want to necessarily standardize our target values so we should ensure to to isolate those in our data set. Also let us leave out variables that we do not want to consider in our analysis, such as longitude, latitude, housingMedianAge and totalRooms.\n","\n","In this case, we will use the select() method and passing the column names in the order that is more appropriate. In this case, the target variable medianHouseValue is put first, so that it will not be affected by the standardization."],"metadata":{"id":"jd9lF6XqdCPO"}},{"cell_type":"code","source":["# Re-order and select columns\n","housing_df = housing_df.select(\"medhv\",\n","                              \"totbdrms\",\n","                              \"pop\",\n","                              \"houshlds\",\n","                              \"medinc\",\n","                              \"rms_per_hh\",\n","                              \"pop_per_hh\",\n","                              \"bdrms_per_rm\")"],"metadata":{"id":"nE117HG0dVIi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Feature Extraction\n","\n","Now that the data is re-ordered, we are ready to normalize the data. We will choose the features to be normalized."],"metadata":{"id":"YuWiZwoedX-E"}},{"cell_type":"code","source":["featureCols = [\"totbdrms\", \"pop\", \"houshlds\", \"medinc\", \"rms_per_hh\", \"pop_per_hh\", \"bdrms_per_rm\"]"],"metadata":{"id":"W0Br4YTOdd9y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Use a VectorAssembler to put features into a feature vector column**"],"metadata":{"id":"NgMFGtcKdgRW"}},{"cell_type":"code","source":["# Put features into a feature vector column\n","assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")"],"metadata":{"id":"UZxE_YVedmF0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["assembled_df = assembler.transform(housing_df)"],"metadata":{"id":"D8jhDdLQdoA0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["assembled_df.show(10, truncate=False)"],"metadata":{"id":"tWQ5nZGsdqIl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All the features have transformed into a Dense Vector.\n","\n"],"metadata":{"id":"zamnqcAmdx6A"}},{"cell_type":"markdown","source":["#### Standardization\n","\n","Next, we can finally scale the data using StandardScaler. The input columns are the features, and the output column with the rescaled values that will be included in the scaled_df will be named \"features_scaled\"."],"metadata":{"id":"lOd7w4eSd0ex"}},{"cell_type":"code","source":["# Initialize the `standardScaler`\n","standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")"],"metadata":{"id":"l5G28Ivod8z8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the DataFrame to the scaler\n","scaled_df = standardScaler.fit(assembled_df).transform(assembled_df)"],"metadata":{"id":"tCCiEOhBd9jT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inspect the result\n","scaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)"],"metadata":{"id":"VggKmWUad_3g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Building A Machine Learning Model With Spark ML"],"metadata":{"id":"BxrW-IOseFNx"}},{"cell_type":"markdown","source":["With all the preprocessing done, it's finally time to start building our Linear Regression model! First, split the data into training and test sets using the randomSplit() method:"],"metadata":{"id":"YNAYeRqaeKUA"}},{"cell_type":"code","source":["# Split the data into train and test sets\n","train_data, test_data = scaled_df.randomSplit([.8,.2], seed=rnd_seed)"],"metadata":{"id":"WP3iLRxfeMQP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We pass in a list with two numbers that represent the size that we want training and test sets to have including a seed.\n","\n","Note that the argument elasticNetParam corresponds to  α  or the vertical intercept and that the regParam or the regularization paramater corresponds to  λ ."],"metadata":{"id":"hUKlWdmweO91"}},{"cell_type":"code","source":["train_data.columns"],"metadata":{"id":"cNWQusu8eRti"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Create an ElasticNet model**\n","\n","ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both."],"metadata":{"id":"5WR8wFaNeUYj"}},{"cell_type":"code","source":["# Initialize `lr`\n","lr = (LinearRegression(featuresCol='features_scaled', labelCol=\"medhv\", predictionCol='predmedhv',\n","                               maxIter=10, regParam=0.3, elasticNetParam=0.8, standardization=False))"],"metadata":{"id":"uiQP6iase8yC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the data to the model\n","linearModel = lr.fit(train_data)"],"metadata":{"id":"eWBVOqNee_JM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluating the Model\n","\n","We can now generate predictions for our test data by using the transform() method to predict the labels for our test_data. Then, we can use RDD operations to extract the predictions as well as the true labels from the DataFrame."],"metadata":{"id":"IU8ldByRfCRT"}},{"cell_type":"markdown","source":["#### Inspect the Model Co-efficients"],"metadata":{"id":"w4tbh_wLfVPc"}},{"cell_type":"code","source":["# Coefficients for the model\n","linearModel.coefficients"],"metadata":{"id":"MGun7VXjfZXz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["featureCols"],"metadata":{"id":"pG0J_Q4nfcev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Intercept for the model\n","linearModel.intercept"],"metadata":{"id":"A_gAhFS3ffkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coeff_df = pd.DataFrame({\"Feature\": [\"Intercept\"] + featureCols, \"Co-efficients\": np.insert(linearModel.coefficients.toArray(), 0, linearModel.intercept)})\n","coeff_df = coeff_df[[\"Feature\", \"Co-efficients\"]]"],"metadata":{"id":"Vlc1rXtlfiDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coeff_df"],"metadata":{"id":"wX_uGUMUfj-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Generating Predictions"],"metadata":{"id":"8FzalDN5fY-N"}},{"cell_type":"code","source":["# Generate predictions\n","predictions = linearModel.transform(test_data)"],"metadata":{"id":"5QNIiz4jfqSn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the predictions and the \"known\" correct labels\n","predandlabels = predictions.select(\"predmedhv\", \"medhv\")"],"metadata":{"id":"VAGJ0SyxfsXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predandlabels.show()"],"metadata":{"id":"XnAVE5DGfuNf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Inspect the Metrics\n","\n","We will now inspect the metrics using the LinearRegressionModel.summary attribute, to pull up the rootMeanSquaredError and the r2 score."],"metadata":{"id":"Ht6AMM-QfxDs"}},{"cell_type":"code","source":["# Get the RMSE\n","print(\"RMSE: {0}\".format(linearModel.summary.rootMeanSquaredError))"],"metadata":{"id":"urKx_AU6gCD8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"MAE: {0}\".format(linearModel.summary.meanAbsoluteError))"],"metadata":{"id":"meQFEK4dgEco"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the R2\n","print(\"R2: {0}\".format(linearModel.summary.r2))"],"metadata":{"id":"-fpE6kzIgGzw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Using the RegressionEvaluator from pyspark.ml package**"],"metadata":{"id":"mnkCADy9gSZP"}},{"cell_type":"code","source":["evaluator = RegressionEvaluator(predictionCol=\"predmedhv\", labelCol='medhv', metricName='rmse')\n","print(\"RMSE: {0}\".format(evaluator.evaluate(predandlabels)))"],"metadata":{"id":"52v3uAhagYTw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluator = RegressionEvaluator(predictionCol=\"predmedhv\", labelCol='medhv', metricName='mae')\n","print(\"MAE: {0}\".format(evaluator.evaluate(predandlabels)))"],"metadata":{"id":"I4nHYuvLgbTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluator = RegressionEvaluator(predictionCol=\"predmedhv\", labelCol='medhv', metricName='r2')\n","print(\"R2: {0}\".format(evaluator.evaluate(predandlabels)))"],"metadata":{"id":"F4Zc3OFRgdSZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Using the RegressionMetrics from pyspark.mllib package**\n","\n"],"metadata":{"id":"hw1Ozhfzgf09"}},{"cell_type":"code","source":["# mllib is old that is why the methods are available in rdd\n","metrics = RegressionMetrics(predandlabels.rdd)"],"metadata":{"id":"I_uz0MwLgj8T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"RMSE: {0}\".format(metrics.rootMeanSquaredError))"],"metadata":{"id":"8TZiMzu1gmk0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"MAE: {0}\".format(metrics.meanAbsoluteError))"],"metadata":{"id":"tX-28sEjgoo_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"R2: {0}\".format(metrics.r2))"],"metadata":{"id":"cQyOYD6ggq7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Improvements to the model are still needed! One should play around with the parameters that we passed to the model and the variables that we included in the original DataFrame."],"metadata":{"id":"L3WLlyJjgqo1"}},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"Tooxdg2CgxrO"},"execution_count":null,"outputs":[]}]}