{"cells":[{"cell_type":"markdown","source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Additional Notebook (Ungraded): Hadoop MapReduce"],"metadata":{"id":"FZvvlfY3LIJ4"}},{"cell_type":"markdown","metadata":{"id":"cs0B3ScgUVRv"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"Sq5piVQRUVRw"},"source":["At the end of the experiment, you will be able to\n","\n","* understand what is Hadoop and its components\n","* perform various Hadoop-HDFS shell commands\n","* perform MapReduce operation on data"]},{"cell_type":"markdown","metadata":{"id":"8QqNbemVUVRx"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"LmQjymFwUVRy"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"584kfh_hUVRy"},"source":["Hadoop is an open-source framework that allows the storage and processing of big data in a distributed environment across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage.\n","\n","Apache Software Foundation is the developer of Hadoop, and its co-founders are Doug Cutting and Mike Cafarella.\n","Its co-founder Doug Cutting named it on his son’s toy elephant. In October 2003 the first paper release was Google File System. In January 2006, MapReduce development started on the Apache Nutch which consisted of around 6000 lines of code for it and around 5000 lines of code for HDFS. In April 2006 Hadoop 0.1.0 was released.\n","\n","Apache Hadoop is a collection of open-source software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the MapReduce programming model. Hadoop was originally designed for computer clusters built from commodity hardware, which is still common use. It has since also found use on clusters of higher-end hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework."]},{"cell_type":"markdown","metadata":{"id":"oxC7feShUVRz"},"source":["### Components of Hadoop"]},{"cell_type":"markdown","metadata":{"id":"-iuRTkSmUVRz"},"source":["The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part which is a MapReduce programming model. Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers the packaged code into nodes to process the data in parallel. This approach takes advantage of data locality, where nodes manipulate the data they have access to and allows the dataset to be processed faster and more efficiently.\n","\n","The components of Hadoop are as follows:\n","\n","* Storage unit - Hadoop HDFS(Hadoop Distributed File System)\n","* Processing unit - Hadoop MapReduce\n","* Resource management unit - Hadoop YARN(Yet Another Resource Framework)"]},{"cell_type":"markdown","metadata":{"id":"qjtfvKzyUVR0"},"source":["#### Hadoop HDFS"]},{"cell_type":"markdown","metadata":{"id":"h851H3rZUVR0"},"source":["Hadoop File System was developed using distributed file system design. It is run on commodity hardware. Unlike other distributed systems, HDFS is highly fault tolerant and designed using low-cost hardware.\n","\n","It holds very large amount of data and provides easier access. To store such huge data, the files are stored across multiple machines. These files are stored in a redundant fashion to rescue the system from possible data losses in case of failure. HDFS also makes applications available to parallel processing.\n","\n","**Features of HDFS**\n","\n","* Suitable for the distributed storage and processing\n","* Hadoop provides a command interface to interact with HDFS\n","* The built-in servers of namenode and datanode help users to easily check the status of the cluster\n","* Streaming access to file system data\n","* HDFS provides file permissions and authentication"]},{"cell_type":"markdown","metadata":{"id":"UwSaTLqCUVR1"},"source":["**Name Node**\n","\n","HDFS consists of only one Name Node that is called the Master Node. The master node can track files, manage the file system and has the metadata of all of the stored data within it. In particular, the name node contains the details of the number of blocks, locations of the data node that the data is stored in, where the replications are stored, and other details. The name node has direct contact with the client.\n","\n","**Data Node**\n","\n","A Data Node stores data in it as blocks. This is also known as the slave node and it stores the actual data into HDFS which is responsible for the client to read and write. These are slave daemons. Every Data node sends a Heartbeat message to the Name node every 3 seconds and conveys that it is alive. In this way when Name Node does not receive a heartbeat from a data node for 2 minutes, it will take that data node as dead and start the process of block replications on some other Data node.\n","<figure>\n","<img src= 'https://cdn.iisc.talentsprint.com/CDS/Images/Datanode.png' width= 500 px/>\n","<figure/>"]},{"cell_type":"markdown","metadata":{"id":"s1vSaqimUVR1"},"source":["#### Hadoop MapReduce"]},{"cell_type":"markdown","metadata":{"id":"FiLl3ARqUVR2"},"source":["It is the processing unit of Hadoop. In map reduce approach, the processing is done at the slave nodes, and the final result is sent to the master node.\n","<figure>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/MapReduce.jpg\" />\n","<figure/>\n","    \n","A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then used as input to the reduce tasks. At the reduce phase, the aggregation takes place, and the final output is obtained."]},{"cell_type":"markdown","metadata":{"id":"NL5-qfG5UVR2"},"source":["#### Hadoop YARN"]},{"cell_type":"markdown","metadata":{"id":"a6JpbdyVUVR2"},"source":["Hadoop YARN stands for Yet Another Resource Negotiator and is the resource management unit of Hadoop, available as a component of Hadoop version 2. It is a file system that is built on top of HDFS and responsible for managing cluster resources to prevent overloading. It performs job scheduling to make sure that the jobs are scheduled in the right place\n","\n","Whenever a client machine wants to do a query or fetch some code this job request goes to the resource manager (Hadoop Yarn), which is responsible for resource allocation and management.\n","\n","<figure>\n","<img src='https://cdn.iisc.talentsprint.com/CDS/Images/yarn_architecture.gif' />\n","<figure/>\n","    \n","In the node section, each of the nodes has its node managers. These node managers manage the nodes and monitor the resource usage in the node. The containers contain a collection of physical resources, which could be RAM, CPU, or hard drives. Whenever a job request comes in, the application master requests the container from the node manager. Once the node manager gets the resource, it goes back to the Resource Manager."]},{"cell_type":"code","source":["#@title Download Dataset\n","!wget -qq https://cdn.iisc.talentsprint.com/CDSE_experiments_data/ecommerce_uci.csv\n","!wget -qq http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz\n","!tar -xzvf 20news-18828.tar.gz\n","print(\"Downloaded dataset successfully\")"],"metadata":{"cellView":"form","id":"mW4vNq7F99qL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ksnosRXLUVR3"},"source":["**Note: Hadoop installation in this assignment is performed on top of the linux file system available in Google Colab, therefore it is strictly recommended to execute it only on Google Colab and it may not work in any local environment.**"]},{"cell_type":"markdown","metadata":{"id":"N7efophkUVR3"},"source":["### Installing Hadoop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZXHB-549JFo"},"outputs":[],"source":["# Downloading the hadoop zip file\n","!wget -qq https://archive.apache.org/dist/hadoop/common/hadoop-3.3.3/hadoop-3.3.3.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MTEB_MWmUVR4"},"outputs":[],"source":["# Unzipping the hadoop zip file\n","!tar -xzvf hadoop-3.3.3.tar.gz"]},{"cell_type":"markdown","metadata":{"id":"BOi7LsclUVR4"},"source":["Since Colaboratory is built on top of Ubuntu. All the Ubuntu files are available in the Colab file section (left panel of Colab notebook).\n","\n","Copying the Hadoop file to `user/local` directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArYtJk2NUVR4"},"outputs":[],"source":["# copy  hadoop file to user/local\n","!cp -r hadoop-3.3.3/ /usr/local/"]},{"cell_type":"markdown","metadata":{"id":"T08k-EKlUVR5"},"source":["### Configuring Hadoop’s Java Home\n","\n","Hadoop requires that you set the path to Java, either as an environment variable or in the Hadoop configuration file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xcEQoZyUVR5"},"outputs":[],"source":["#To find the default Java path\n","!readlink -f /usr/bin/java | sed \"s:bin/java::\""]},{"cell_type":"markdown","metadata":{"id":"Fs9ahPcsUVR5"},"source":["set the java environmental variable using os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adULhxAQUVR5"},"outputs":[],"source":["import os\n","os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64/'"]},{"cell_type":"markdown","metadata":{"id":"bD7_PFeJUVR6"},"source":["### Running Hadoop\n","\n","From the `user/local` directory where we copied the hadoop file, run the hadoop command."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10Wh-A20UVR6"},"outputs":[],"source":["#Running Hadoop\n","!/usr/local/hadoop-3.3.3/bin/hadoop"]},{"cell_type":"markdown","metadata":{"id":"8lVgHNRUUVR6"},"source":["### Perform various Hadoop-HDFS shell commands"]},{"cell_type":"markdown","metadata":{"id":"lyZz21HWUVR6"},"source":["#### Check the version of Hadoop\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PsgpytKSUVR6"},"outputs":[],"source":["!/usr/local/hadoop-3.3.3/bin/hadoop version"]},{"cell_type":"markdown","metadata":{"id":"W9WRdhX1UVR7"},"source":["#### List all the files/directories for the given hdfs destination path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i48aj_dvUVR7"},"outputs":[],"source":["!/usr/local/hadoop-3.3.3/bin/hdfs dfs -ls /"]},{"cell_type":"markdown","metadata":{"id":"uK8E1Ba8UVR7"},"source":["#### Display free space at given hdfs destination"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uB0YMr_RUVR7"},"outputs":[],"source":["!/usr/local/hadoop-3.3.3/bin/hdfs dfs -df /"]},{"cell_type":"markdown","metadata":{"id":"itbjrL1GUVR7"},"source":["#### HDFS Command to create the directory in HDFS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzMSBrOZUVR8"},"outputs":[],"source":["!/usr/local/hadoop-3.3.3/bin/hdfs dfs -mkdir /hadoop"]},{"cell_type":"markdown","metadata":{"id":"cFWx-EioUVR8"},"source":["#### HDFS command to remove the entire directory and all of its content from HDFS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFWGwnTxUVR8"},"outputs":[],"source":["!/usr/local/hadoop-3.3.3/bin/hdfs dfs -rm -r /hadoop"]},{"cell_type":"markdown","metadata":{"id":"VT4a3qtIUVR8"},"source":["### Perform MapReduce operation"]},{"cell_type":"markdown","metadata":{"id":"yF-8QqF2UVR9"},"source":["We will use Ecommerce sales dataset from the UCI Machine Learning Repository  containing real-life transaction data from a UK retailer.\n","\n","Here we will perform MapReduce operation to calculate total sales for each country."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UGY5aUsUVR9"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv('ecommerce_uci.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"48jYWnq4UVR9"},"source":["### Hadoop Streaming"]},{"cell_type":"markdown","metadata":{"id":"79qOIiqvUVR9"},"source":["Here we will use HadoopStreaming for helping us pass data between our Map and Reduce code via STDIN (standard input) and STDOUT (standard output). We will simply use python’s sys.stdin to read input data and print output to sys.stdout. That’s all we need to do and HadoopStreaming will take care of everything else."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0rkPO5PUVR-"},"outputs":[],"source":["!find / -name 'hadoop-streaming*.jar'"]},{"cell_type":"markdown","metadata":{"id":"sXio5SB4UVR-"},"source":["### Write the mapper and reducer files"]},{"cell_type":"markdown","metadata":{"id":"ptmEyO-FUVR-"},"source":["Mapper will\n","\n","* read the data\n","* convert it into a proper format\n","* print output as key-value pair i.e, Country Name, Sales."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2r64LeO5UVR-"},"outputs":[],"source":["%%writefile mapper.py\n","import sys\n","line_number = 1\n","# csv is file is passed as command line input\n","for line in sys.stdin:\n","    line = line.strip()           # remove trailing spaces\n","    InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Total_Sales, Country = line.split(',')\n","    #print(InvoiceNo)\n","    if line_number>=2:\n","        print('{0}\\t{1}'.format(Country.replace(' ','_'),Total_Sales))\n","    line_number += 1\n"]},{"cell_type":"markdown","metadata":{"id":"tCKahzQeUVR_"},"source":["Reducer will\n","\n","* read input from the mapper\n","* check for existing country key in the dictionary\n","* add the total to an existing value\n","* print all key-value pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oznWSarwUVR_"},"outputs":[],"source":["%%writefile reducer.py\n","from operator import itemgetter\n","import sys\n","word2count = {}\n","# csv is file is passed as command line input\n","for line in sys.stdin:\n","    line = line.strip()\n","    try:\n","        country_name, sales = line.split('\\t')\n","        sales = float(sales)\n","        # get() method takes maximum of two parameters: Value(0) to be returned if the key is not found\n","        word2count[country_name] = word2count.get(country_name, 0) + sales\n","    except ValueError:\n","        pass\n","# itemgetter(1) will specify that we need to sort based on dictionary values\n","sorted_word2count = sorted(word2count.items(), key=itemgetter(1), reverse=True)\n","\n","for word, count in sorted_word2count:\n","    print('{0}\\t{1}'.format(word, count))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83AT5N4bUVR_"},"outputs":[],"source":["# Restoring the access permissions for user including read, write and execute\n","!chmod u+rwx /content/mapper.py\n","!chmod u+rwx /content/reducer.py"]},{"cell_type":"markdown","metadata":{"id":"rRQSi8FRUVSA"},"source":["#### Running the Python Code on Hadoop cluster\n","\n","Now that everything is prepared, we can finally run our Python MapReduce job on the Hadoop cluster. As mentioned earlier, we use Hadoop streaming for passing data between our Map and Reduce code via STDIN (standard input) and STDOUT (standard output)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bk0oQOSuUVSA"},"outputs":[],"source":["!/usr/local/hadoop-3.3.3/bin/hadoop jar /usr/local/hadoop-3.3.3/share/hadoop/tools/lib/hadoop-streaming-3.3.3.jar -input /content/ecommerce_uci.csv -output /content/output1 -file /content/mapper.py  -file /content/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rl09opsHUVSA"},"outputs":[],"source":["# Locating output directory\n","!ls /content/output1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YG81DDGUVSB"},"outputs":[],"source":["# Display output\n","!cat /content/output1/part-00000"]},{"cell_type":"markdown","metadata":{"id":"R6zKx-E9UVSB"},"source":["Now we will conssider 20-newsgroups dataset. This dataset is a collection of newsgroup documents. There are 20 files that contain all of the documents, one document per newsgroup. In this dataset, duplicate messages have been removed and the original messages only contain \"From\" and \"Subject\" headers (18828 messages total).\n","\n","Each newsgroup file in the bundle represents a single newsgroup. Each message in a file is the text of some newsgroup document that was posted to that newsgroup.\n","\n","Some of the newsgroups are as follows:\n","\n","* comp.graphics\n","* comp.os.ms-windows.misc\n","* comp.sys.ibm.pc.hardware\n","* rec.motorcycles\n","* rec.sport.baseball\n","* rec.sport.hockey sci.crypt\n","* sci.electronics\n","* sci.med\n","* misc.forsale talk.politics.misc\n","* talk.politics.guns\n","* talk.politics.mideast talk.religion.misc\n","* alt.atheism\n","* soc.religion.christian\n","\n","Here we will be using `alt.atheism` newsgroup and perform MapReduce operation to calculate the count of words in it.\n","\n","To know more about the dataset click [here](http://qwone.com/~jason/20Newsgroups/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOBcqyrAWE3e"},"outputs":[],"source":["!cat /content/20news-18828/alt.atheism/49960"]},{"cell_type":"markdown","metadata":{"id":"Py8aEO5qUVSB"},"source":["#### Write the Mapper file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0uY8YGpgUVSC"},"outputs":[],"source":["%%writefile mapper_news.py\n","import sys\n","import io\n","import re\n","import nltk\n","nltk.download('stopwords',quiet=True)\n","from nltk.corpus import stopwords\n","import string\n","# list possible punctuations\n","punctuations = string.punctuation\n","\n","# configure english stopwords\n","stop_words = set(stopwords.words('english'))\n","# convert text to lines of string\n","input_stream = io.TextIOWrapper(sys.stdin.buffer, encoding='latin1')\n","for line in input_stream:\n","  line = line.strip()                      # remove trailing spaces\n","  line = re.sub(r'[^\\w\\s]', '',line)       # replace apostrophe with empty space\n","  line = line.lower()                      # convert line to lowercase\n","  for x in line:\n","    if x in punctuations:\n","      line=line.replace(x, \" \")           # replace punctuations with space\n","\n","  words=line.split()                      # split line into words\n","  for word in words:\n","    if word not in stop_words:\n","      print('%s\\t%s' % (word, 1))"]},{"cell_type":"markdown","metadata":{"id":"yOW2FFmBUVSC"},"source":["#### Write the Reducer file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Divwzc5UVSC"},"outputs":[],"source":["%%writefile reducer_news.py\n","from operator import itemgetter\n","import sys\n","\n","current_word = None\n","current_count = 0\n","word = None\n","\n","# input comes from STDIN\n","for line in sys.stdin:\n","    # remove leading and trailing whitespace\n","    line = line.strip()\n","    line=line.lower()\n","\n","    # parse the input we got from mapper.py\n","    word, count = line.split('\\t', 1)\n","    try:\n","      count = int(count)\n","    except ValueError:\n","      #count was not a number, so silently\n","      #ignore/discard this line\n","      continue\n","\n","    # this IF-switch only works because Hadoop sorts map output\n","    # by key (here: word) before it is passed to the reducer\n","    if current_word == word:\n","        current_count += count\n","    else:\n","        if current_word:\n","            # write result to STDOUT\n","            print ('%s\\t%s' % (current_word, current_count))\n","        current_count = count\n","        current_word = word\n","\n","# do not forget to output the last word if needed!\n","if current_word == word:\n","    print( '%s\\t%s' % (current_word, current_count))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-_2vycIUVSC"},"outputs":[],"source":["# Restoring the access permissions for user including read, write and execute\n","!chmod u+rwx /content/mapper_news.py\n","!chmod u+rwx /content/reducer_news.py"]},{"cell_type":"markdown","metadata":{"id":"H9WdMNPzUVSC"},"source":["#### Running the Python Code on Hadoop cluster\n","\n","Now that everything is prepared, we can finally run our Python MapReduce job on the Hadoop cluster. As mentioned earlier, we use Hadoop streaming for passing data between our Map and Reduce code via STDIN (standard input) and STDOUT (standard output)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGZJ_eO8UVSD"},"outputs":[],"source":["!/usr/local/hadoop-3.3.3/bin/hadoop jar /usr/local/hadoop-3.3.3/share/hadoop/tools/lib/hadoop-streaming-3.3.3.jar -input /content/20news-18828/alt.atheism/49960 -output /content/output_news -file /content/mapper_news.py  -file /content/reducer_news.py  -mapper 'python mapper_news.py'  -reducer 'python reducer_news.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1sYpEKIUVSD"},"outputs":[],"source":["# Locating output directory\n","!ls /content/output_news"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFmDdIJ2UVSD"},"outputs":[],"source":["# Display output\n","!cat /content/output_news/part-00000"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}