{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Additional Notebook (ungraded): Stock Price prediction using ARIMA"],"metadata":{"id":"fEkE24Ti-axj"}},{"cell_type":"markdown","source":["## Learning Objectives"],"metadata":{"id":"7R3Vv8Sv-p4M"}},{"cell_type":"markdown","source":["At the end of the experiment, you will be able to :\n","\n","* Understand and preprocess time series data by applying log transformation, exponential decay, and time shift techniques to achieve stationarity.\n","\n","* Analyze time series dependencies using Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) to determine optimal ARIMA model parameters.\n","\n","* Build and train an ARIMA model to forecast the demand(passenger traffic) in Airplanes by selecting appropriate values for AR (p), I (d), and MA (q) based on statistical analysis. The data is classified in date/time and the passengers travelling per month\n","\n","* Evaluate and visualize predictions by applying inverse transformations to recover original scale data and comparing actual vs. forecasted values on a single overlapping plot.\n"],"metadata":{"id":"mPjoaecdJSF2"}},{"cell_type":"markdown","source":["## Information"],"metadata":{"id":"xv7Yb7XgBCIe"}},{"cell_type":"markdown","source":["## Dataset\n","\n","The Air Passengers dataset is a well-known time series dataset that records the number of passengers traveling by air over time. It consists of monthly observations from January 1949 to December 1960, covering a total period of 12 years (144 months).\n","\n","**Attributes Information:**\n","\n","1. Month – Represents the time index in the format YYYY-MM (e.g., 1949-01).\n","2. Passengers – Represents the total number of passengers (in thousands) who traveled in that particular month.\n","\n","**Usage in Forecasting:**\n","\n","- This dataset is used to analyze historical trends in air travel and build a predictive model to forecast future passenger numbers.\n","- It exhibits characteristics such as seasonality (yearly patterns) and trend (gradual increase over time), making it ideal for time series forecasting using ARIMA.\n","- Preprocessing steps like log transformation and differencing are applied to make the data stationary before fitting the ARIMA model."],"metadata":{"id":"KO-hXYDeBI7p"}},{"cell_type":"markdown","source":["## Problem Statement"],"metadata":{"id":"6vazqAC-CiT2"}},{"cell_type":"markdown","source":["Implement ARIMA model to forecast the passenger traffic in Airplanes by using the time series Air Passengers dataset"],"metadata":{"id":"zmlFOLdqCnDU"}},{"cell_type":"markdown","source":["**contents**\n","* Import Libraries\n","* Read Data\n","* Data Transformation to achieve Stationarity\n","* Log Scale Transformation\n","* Exponential Decay Transformation\n","* Time Shift Transformation\n","* Plotting ACF & PACF\n","* Building Models\n","* Prediction & Reverse transformations"],"metadata":{"id":"i1NU330RJw_d"}},{"cell_type":"code","source":["!pip -qq install pmdarima"],"metadata":{"id":"DnApsTPKDKq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Download the Dataset\n","! wget https://cdn.exec.talentsprint.com/static/cds/content/Air_Passengers.csv\n","\n","print(\"The datset was downloaded\")"],"metadata":{"cellView":"form","id":"G0jC0F1bey0W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import required Packages"],"metadata":{"id":"M4O8309ODQib"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JarhbqWNJChJ"},"outputs":[],"source":["from datetime import datetime\n","import numpy as np             #for numerical computations like log,exp,sqrt etc\n","import pandas as pd           #for reading & storing data, pre-processing\n","import matplotlib.pylab as plt #for visualization\n","#for making sure matplotlib plots are generated in Jupyter notebook itself\n","%matplotlib inline\n","from statsmodels.tsa.stattools import adfuller\n","from statsmodels.tsa.stattools import acf, pacf\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","#from statsmodels.tsa.arima_model import ARIMA\n","from statsmodels.tsa.arima.model import ARIMA\n","from matplotlib.pylab import rcParams\n","rcParams['figure.figsize'] = 10, 6"]},{"cell_type":"markdown","source":["### Load the data and analyze"],"metadata":{"id":"KMsY8oNPDaRR"}},{"cell_type":"code","source":["df = pd.read_csv('/content/Air_Passengers.csv')\n","df.head()"],"metadata":{"id":"Homb55tgKBYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = df\n","#Parse strings to datetime type\n","dataset['Month'] = pd.to_datetime(dataset['Month'],infer_datetime_format=True) #convert from string to datetime\n","indexedDataset = dataset.set_index(['Month'])\n","indexedDataset.head(5)"],"metadata":{"id":"wHIuGmWHKMw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## plot graph\n","plt.xlabel('Date')\n","plt.ylabel('Number of air passengers')\n","plt.plot(indexedDataset)"],"metadata":{"id":"hOh7I1t7KVHi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Computing Rolling Mean and Standard Deviation for Stationarity Analysis"],"metadata":{"id":"yac1Fw9rD95m"}},{"cell_type":"code","source":["#Determine rolling statistics\n","rolmean = indexedDataset.rolling(window=12).mean() #window size 12 denotes 12 months, giving rolling mean at yearly level\n","rolstd = indexedDataset.rolling(window=12).std()\n","print(rolmean,rolstd)"],"metadata":{"id":"eKmdfPcSKadJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot rolling statistics\n","orig = plt.plot(indexedDataset, color='blue', label='Original')\n","mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n","std = plt.plot(rolstd, color='black', label='Rolling Std')\n","plt.legend(loc='best')\n","plt.title('Rolling Mean & Standard Deviation')\n","plt.show(block=False)"],"metadata":{"id":"cXovT_DMKhNq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Augmented Dickey–Fuller test"],"metadata":{"id":"auLzLTp9EOEy"}},{"cell_type":"code","source":["#Perform Augmented Dickey–Fuller test:\n","print('Results of Dickey Fuller Test:')\n","dftest = adfuller(indexedDataset['#Passengers'], autolag='AIC')\n","\n","dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n","for key,value in dftest[4].items():\n","    dfoutput['Critical Value (%s)'%key] = value\n","\n","print(dfoutput)"],"metadata":{"id":"PgojMCCJKm7h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the above result,\n","1. Test Statistic (1.221233): This value is greater than all the critical values (-3.48, -2.88, -2.57), meaning the null hypothesis (H₀: the series is non-stationary) cannot be rejected.\n","2. p-value (0.996129): A high p-value (typically > 0.05) suggests that the time series is not stationary, meaning it still exhibits trends or seasonality.\n","3. Lags Used (14): The model used 14 lag values to estimate stationarity.\n","4. Number of Observations Used (124): The test was performed on 124 data points after accounting for the lag values.\n","\n","Since the **p-value is very high (0.996129)**, and the test statistic is above all critical values, the series is still **non-stationary** and requires further transformations (like differencing) to achieve stationarity before applying ARIMA."],"metadata":{"id":"SERNKwpiJHWY"}},{"cell_type":"markdown","source":["### Log Scale Transformation for Trend Estimation"],"metadata":{"id":"eLHqY3hwEehc"}},{"cell_type":"code","source":["#Estimating trend\n","indexedDataset_logScale = np.log(indexedDataset)\n","plt.plot(indexedDataset_logScale)"],"metadata":{"id":"AOxsS4gSKsSh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Moving Average Transformation for Stationarity"],"metadata":{"id":"0ph2OjQyFJrl"}},{"cell_type":"code","source":["#The below transformation is required to make series stationary\n","movingAverage = indexedDataset_logScale.rolling(window=12).mean()\n","movingSTD = indexedDataset_logScale.rolling(window=12).std()\n","plt.plot(indexedDataset_logScale)\n","plt.plot(movingAverage, color='red')"],"metadata":{"id":"1aGlk-jxKxds"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Subtracting Moving Average to Detrend the Time Series"],"metadata":{"id":"k5eJPFClFcdA"}},{"cell_type":"code","source":["datasetLogScaleMinusMovingAverage = indexedDataset_logScale - movingAverage\n","datasetLogScaleMinusMovingAverage.head(12)\n","\n","#Remove NAN values\n","datasetLogScaleMinusMovingAverage.dropna(inplace=True)\n","datasetLogScaleMinusMovingAverage.head(10)"],"metadata":{"id":"oAPXIoWkK3JS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing Stationarity using Rolling Statistics and Dickey-Fuller Test"],"metadata":{"id":"36wpvU2rFuCR"}},{"cell_type":"code","source":["def test_stationarity(timeseries):\n","\n","    #Determine rolling statistics\n","    movingAverage = timeseries.rolling(window=12).mean()\n","    movingSTD = timeseries.rolling(window=12).std()\n","\n","    #Plot rolling statistics\n","    orig = plt.plot(timeseries, color='blue', label='Original')\n","    mean = plt.plot(movingAverage, color='red', label='Rolling Mean')\n","    std = plt.plot(movingSTD, color='black', label='Rolling Std')\n","    plt.legend(loc='best')\n","    plt.title('Rolling Mean & Standard Deviation')\n","    plt.show(block=False)\n","\n","    #Perform Dickey–Fuller test:\n","    print('Results of Dickey Fuller Test:')\n","    dftest = adfuller(timeseries['#Passengers'], autolag='AIC')\n","    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n","    for key,value in dftest[4].items():\n","        dfoutput['Critical Value (%s)'%key] = value\n","    print(dfoutput)\n","\n","test_stationarity(datasetLogScaleMinusMovingAverage)"],"metadata":{"id":"to7-ZxnpK9wc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The above result shows\n","- Test Statistic: The test statistic is -3.158932. This value is compared to the critical values at different significance levels (1%, 5%, 10%).\n","- p-value: The p-value is 0.022488, which is less than the previous p-value (0.996129) and less than the common significance level of 0.05. This means we reject the null hypothesis (which suggests the series has a unit root or is non-stationary) and conclude that the series is stationary.\n","\n","If this result is compared to the previous result where the test statistic was higher (1.221233) and the p-value was 0.996129 which is greater than 0.05, that would indicate non-stationarity.\n","\n","In the current result, since the test statistic is more negative and the p-value is below 0.05, it indicates that the dataset after subtracting the moving average is stationary.\n","\n","Hence, the transformation (log scaling and subtracting the moving average) has successfully made the dataset stationary."],"metadata":{"id":"u--TJwXbLlWB"}},{"cell_type":"markdown","source":["**Exponential Weighted Moving Average (EWMA) transformation**\n","\n","The following code cell applies an Exponential Weighted Moving Average (EWMA) transformation to the time series data to smooth out fluctuations and highlight trends.\n","\n","The ewm() function is used with a half-life of 12, meaning past observations lose influence at an exponential rate, with each point retaining half of its weight after 12 periods (months).\n","\n","Unlike a simple moving average, EWMA assigns greater weight to more recent observations, making it more responsive to recent changes in the dataset.\n","- The min_periods=0 ensures that all available data points are considered without introducing NaN values.\n","- The adjust=True parameter maintains the default behavior where the EWMA is normalized based on the weights.\n","- Finally, the original log-transformed dataset is plotted in blue, while the smoothed exponential decay trend is plotted in red to visualize the difference."],"metadata":{"id":"8SMsk510GcvA"}},{"cell_type":"code","source":["exponentialDecayWeightedAverage = indexedDataset_logScale.ewm(halflife=12, min_periods=0, adjust=True).mean()\n","plt.plot(indexedDataset_logScale)\n","plt.plot(exponentialDecayWeightedAverage, color='red')"],"metadata":{"id":"ZeOkVuJFLFhb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following code cell removes the trend from the log-transformed time series by subtracting the Exponential Weighted Moving Average (EWMA) from the original log-scaled data, resulting in a detrended series **(datasetLogScaleMinusExponentialMovingAverage)**."],"metadata":{"id":"f8TUxDybHf3O"}},{"cell_type":"code","source":["datasetLogScaleMinusExponentialMovingAverage = indexedDataset_logScale - exponentialDecayWeightedAverage\n","test_stationarity(datasetLogScaleMinusExponentialMovingAverage)"],"metadata":{"id":"gX_0P55DLM1p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the above result, the following are observed.\n","- **Test Statistic: -3.541470**, which is more negative than the previous result (-3.158932). So, it suggests stronger evidence for stationarity after subtracting the exponential moving average.\n","- **p-value: 0.006984**, which is even smaller than the previous p-value of 0.022488. This confirms stronger evidence to reject the null hypothesis and suggests the series is stationary."],"metadata":{"id":"tVRYle7MONJ_"}},{"cell_type":"markdown","source":["The following code cell implements the operation **datasetLogDiffShifting = indexedDataset_logScale - indexedDataset_logScale.shift()**\n","\n","This is computing the first difference of the log-transformed dataset.\n","\n","- shift(): This function shifts the values of the time series by one period. Essentially, each value in the series is replaced with the previous value from the series (i.e., it shifts the data by 1).\n","- Subtraction: By subtracting the shifted series from the original series, the difference between consecutive log-transformed values is calculated. This is done to remove trends and make the series stationary."],"metadata":{"id":"7K_ZrmvpPjco"}},{"cell_type":"code","source":["datasetLogDiffShifting = indexedDataset_logScale - indexedDataset_logScale.shift()\n","plt.plot(datasetLogDiffShifting)"],"metadata":{"id":"lrz7uX0hLS-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasetLogDiffShifting.dropna(inplace=True)\n","test_stationarity(datasetLogDiffShifting)"],"metadata":{"id":"9vfohv4-LYu6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Time Series Decomposition of Log-Scaled Dataset: Trend, Seasonality, and Residuals"],"metadata":{"id":"wEJQlGFNQjSO"}},{"cell_type":"code","source":["decomposition = seasonal_decompose(indexedDataset_logScale)\n","\n","trend = decomposition.trend\n","seasonal = decomposition.seasonal\n","residual = decomposition.resid\n","\n","plt.subplot(411)\n","plt.plot(indexedDataset_logScale, label='Original')\n","plt.legend(loc='best')\n","\n","plt.subplot(412)\n","plt.plot(trend, label='Trend')\n","plt.legend(loc='best')\n","\n","plt.subplot(411)\n","plt.plot(seasonal, label='Seasonality')\n","plt.legend(loc='best')\n","\n","plt.subplot(411)\n","plt.plot(residual, label='Residuals')\n","plt.legend(loc='best')\n","\n","plt.tight_layout()"],"metadata":{"id":"pOe95NMILfhM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ACF and PACF Plots for Differenced Log-Scaled Dataset"],"metadata":{"id":"Gfj07L4JQvm3"}},{"cell_type":"code","source":["#ACF & PACF plots\n","\n","lag_acf = acf(datasetLogDiffShifting, nlags=20)\n","lag_pacf = pacf(datasetLogDiffShifting, nlags=20, method='ols')\n","\n","plot_acf(lag_acf)\n","plot_pacf(lag_pacf)"],"metadata":{"id":"rbInBtcANHqM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"hiiuT_XnQ9MX"}},{"cell_type":"markdown","source":["From the above plots, we can get the following.\n","- Moving Average (q value) = 2 (from acf graph)\n","- Auto-regressive (p value) = 1 (from pacf graph)\n","- Differentiation (d value) = 2 (from the window, period value)\n","\n","We have now removed the ets (error, trend, seasonality)"],"metadata":{"id":"M6dQb6i-Q9N3"}},{"cell_type":"code","source":["# Differencing to make it stationary\n","df_log_diff = indexedDataset_logScale.diff().dropna()"],"metadata":{"id":"FjLr2HRaQDg6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ARIMA Model Implementation:\n","**AR Model Fitting and Visualization on Differenced Log-Scaled Dataset**"],"metadata":{"id":"BpC946eaRdnk"}},{"cell_type":"code","source":["#AR Model\n","#making order=(2,1,0) gives RSS=1.9972\n","model = ARIMA(df_log_diff, order=(2,1,0))\n","results_AR = model.fit()\n","plt.plot(datasetLogDiffShifting)\n","plt.plot(results_AR.fittedvalues, color='red')\n","plt.title('RSS: %.4f'%sum((results_AR.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\n","print('Plotting AR model')"],"metadata":{"id":"A9B_v3ImONzp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the following code,\n","\n","The order of the ARIMA model is changed from (2,1,0) to (2,1,2).\n","\n","**Order (2,1,0):** The previous model has 2 lags for the AR component, 1 differencing step for stationarity, and no MA component (0).\n","\n","**Order (2,1,2):** The following model still has 2 lags for the AR component and 1 differencing step, but now it includes a Moving Average component with 2 lags (2).\n","\n","**Why to change the order?**\n","\n","- Inclusion of the MA component: By adding the MA component (order=2), we are accounting for the residual error (or noise) in the time series, which could help capture more intricate patterns not captured by the AR component alone.\n","- Model performance: The change in the order helps to improve the model's fit and reduce residuals (RSS).\n","- By adding the MA component, the model might better account for autocorrelations in the residuals, improving accuracy."],"metadata":{"id":"5YMHx64dSeOp"}},{"cell_type":"code","source":["# AR+I+MA = ARIMA model\n","model = ARIMA(df_log_diff, order=(2,1,2))\n","results_ARIMA = model.fit()\n","plt.plot(datasetLogDiffShifting)\n","plt.plot(results_ARIMA.fittedvalues, color='red')\n","plt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\n","print('Plotting ARIMA model')"],"metadata":{"id":"lBoVR0maRTex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n","print(predictions_ARIMA_diff.head())"],"metadata":{"id":"o0EiHKWFSBmL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Convert to cumulative sum\n","predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n","print(predictions_ARIMA_diff_cumsum)"],"metadata":{"id":"1LWsMjYySHNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions_ARIMA_log = pd.Series(indexedDataset_logScale['#Passengers'].iloc[0], index=indexedDataset_logScale.index)\n","predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\n","predictions_ARIMA_log.head()"],"metadata":{"id":"13EuY_tXSMDI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inverse of log is exp.\n","predictions_ARIMA = np.exp(predictions_ARIMA_log)\n","print(len(predictions_ARIMA))\n","plt.plot(indexedDataset)\n","#plt.plot(indexedDataset_logScale)\n","plt.plot(predictions_ARIMA)\n","#plt.plot(predictions_ARIMA_log)"],"metadata":{"id":"X31hqVJFSP0R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["indexedDataset_logScale"],"metadata":{"id":"gtVafmDkSbUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Forecast the next 120 steps\n","forecast_steps = 120\n","forecast_results_log = results_ARIMA.forecast(steps=forecast_steps)\n","\n","# Convert forecast results back from log scale\n","forecast_results = np.exp(forecast_results_log)\n","print(forecast_results)\n","\n","# Calculate the confidence intervals in log scale\n","forecast_conf_int_log = results_ARIMA.get_forecast(steps=forecast_steps).conf_int()\n","\n","# Convert confidence intervals back from log scale\n","forecast_conf_int = np.exp(forecast_conf_int_log)\n","print(forecast_conf_int)\n","# Get confidence intervals and convert from log scale\n","#forecast_conf_int = np.exp(forecast_results_log.conf_int())\n","\n","# Ensure forecast starts smoothly from last 'predictions_ARIMA' value\n","last_date = predictions_ARIMA.index[-1]  # Last available date\n","forecast_index = pd.date_range(start=last_date, periods=forecast_steps+1, freq='MS')[1:]  # Generate future dates\n"],"metadata":{"id":"FYP2nQm-4_CF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Compute Trend Factor for Scaling Forecast"],"metadata":{"id":"v6MfzGiP2Hv4"}},{"cell_type":"code","source":["# **Compute Trend Factor for Scaling Forecast**\n","recent_trend_factor = predictions_ARIMA.iloc[-12:].pct_change().mean()  # Average monthly growth rate\n","trend_multiplier = (1 + recent_trend_factor) ** np.arange(1, forecast_steps + 1)  # Apply growth over steps\n","\n","# Scale the forecasted values using the computed trend multiplier\n","forecast_results = forecast_results * trend_multiplier * (predictions_ARIMA.iloc[-1] / forecast_results.iloc[0])\n","\n","# Extract confidence interval bounds\n","lower_bound = forecast_conf_int.iloc[:, 0] * trend_multiplier * (predictions_ARIMA.iloc[-1] / forecast_conf_int.iloc[:, 0].iloc[0])\n","upper_bound = forecast_conf_int.iloc[:, 1] * trend_multiplier * (predictions_ARIMA.iloc[-1] / forecast_conf_int.iloc[:, 1].iloc[0])"],"metadata":{"id":"qTMquIvd1_aM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot ARIMA Forecast for next 120 steps with Confidence Interval"],"metadata":{"id":"1pu92Zw_2eTg"}},{"cell_type":"code","source":["# **Plot the data**\n","plt.figure(figsize=(12, 6))\n","\n","# Original data (blue)\n","plt.plot(indexedDataset, label='Original Data', color='blue')\n","\n","# Fitted predictions (orange)\n","plt.plot(predictions_ARIMA, color='orange', label='Fitted Curve')\n","\n","# Forecasted values (red)\n","plt.plot(forecast_index, forecast_results, color='red', linestyle='dashed', label='Forecast (Next 120 Steps)')\n","\n","# Confidence intervals (shaded region)\n","plt.fill_between(forecast_index, lower_bound, upper_bound, color='gray', alpha=0.7, label='Confidence Interval')\n","\n","# Labels and legend\n","plt.title('ARIMA Forecast with Confidence Interval')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Z1K56RCT2SWQ"},"execution_count":null,"outputs":[]}]}